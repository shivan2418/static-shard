{"version":3,"sources":["../../src/cli/index.ts","../../src/cli/commands/build.ts","../../src/cli/utils/parsers.ts","../../src/cli/utils/schema.ts","../../src/cli/utils/chunker.ts","../../src/cli/utils/indexer.ts","../../src/cli/utils/codegen.ts","../../src/cli/commands/inspect.ts"],"sourcesContent":["#!/usr/bin/env node\n\n/**\n * static-shard CLI\n */\n\nimport { Command } from \"commander\";\nimport { build } from \"./commands/build.js\";\nimport { inspect } from \"./commands/inspect.js\";\n\nconst program = new Command();\n\nprogram\n  .name(\"static-shard\")\n  .description(\"Query large static datasets efficiently by splitting them into chunks\")\n  .version(\"0.1.0\");\n\nprogram\n  .command(\"build\")\n  .description(\"Build chunks and client from a data file\")\n  .argument(\"<input>\", \"Input data file (JSON, NDJSON, or CSV)\")\n  .requiredOption(\"-o, --output <dir>\", \"Output directory\")\n  .option(\"-s, --chunk-size <size>\", \"Target chunk size (e.g., 5mb)\", \"5mb\")\n  .option(\"-c, --chunk-by <field>\", \"Field to sort and chunk by\")\n  .option(\"-i, --index <fields>\", \"Comma-separated fields to index\")\n  .option(\"-f, --format <format>\", \"Input format (json, ndjson, csv)\")\n  .action(async (input, options) => {\n    try {\n      await build(input, {\n        output: options.output,\n        chunkSize: options.chunkSize,\n        chunkBy: options.chunkBy,\n        index: options.index,\n        format: options.format,\n      });\n    } catch (error) {\n      console.error(\"Error:\", (error as Error).message);\n      process.exit(1);\n    }\n  });\n\nprogram\n  .command(\"inspect\")\n  .description(\"Analyze a data file and suggest chunking strategy\")\n  .argument(\"<input>\", \"Input data file\")\n  .option(\"-n, --sample <count>\", \"Number of records to sample\", \"1000\")\n  .option(\"-f, --format <format>\", \"Input format (json, ndjson, csv)\")\n  .action(async (input, options) => {\n    try {\n      await inspect(input, {\n        sample: parseInt(options.sample, 10),\n        format: options.format,\n      });\n    } catch (error) {\n      console.error(\"Error:\", (error as Error).message);\n      process.exit(1);\n    }\n  });\n\nprogram.parse();\n","/**\n * Build command - process data file and generate output\n */\n\nimport * as fs from \"node:fs\";\nimport * as path from \"node:path\";\nimport type { BuildConfig, BuildOptions, Manifest } from \"../../types/index.js\";\nimport { parseFile, parseSize } from \"../utils/parsers.js\";\nimport { inferSchema, suggestChunkField } from \"../utils/schema.js\";\nimport { chunkRecords, generateChunkMeta, serializeChunk } from \"../utils/chunker.js\";\nimport { buildInvertedIndices } from \"../utils/indexer.js\";\nimport { generateClient } from \"../utils/codegen.js\";\n\nconst VERSION = \"1.0.0\";\n\nexport interface BuildResult {\n  manifest: Manifest;\n  outputDir: string;\n  chunkCount: number;\n  totalRecords: number;\n}\n\nexport async function build(\n  inputFile: string,\n  options: BuildOptions\n): Promise<BuildResult> {\n  const startTime = Date.now();\n\n  // Validate input file exists\n  if (!fs.existsSync(inputFile)) {\n    throw new Error(`Input file not found: ${inputFile}`);\n  }\n\n  console.log(`Reading ${inputFile}...`);\n\n  // Parse input file\n  const { records, format } = await parseFile(inputFile, options.format);\n  console.log(`Parsed ${records.length} records (format: ${format})`);\n\n  if (records.length === 0) {\n    throw new Error(\"No records found in input file\");\n  }\n\n  // Infer schema\n  console.log(\"Inferring schema...\");\n  const schema = inferSchema(records);\n  console.log(`Found ${schema.fields.length} fields`);\n\n  if (schema.primaryField) {\n    console.log(`Detected primary field: ${schema.primaryField}`);\n  }\n\n  // Determine chunk field\n  const chunkBy = options.chunkBy || suggestChunkField(schema, records);\n  if (chunkBy) {\n    console.log(`Chunking by field: ${chunkBy}`);\n  }\n\n  // Parse chunk size\n  const targetChunkSize = parseSize(options.chunkSize);\n  console.log(`Target chunk size: ${formatBytes(targetChunkSize)}`);\n\n  // Parse indexed fields\n  const indexedFields = options.index\n    ? options.index.split(\",\").map((f) => f.trim())\n    : schema.fields.filter((f) => f.indexed).map((f) => f.name);\n\n  if (indexedFields.length > 0) {\n    console.log(`Indexing fields: ${indexedFields.join(\", \")}`);\n  }\n\n  // Update schema with indexed fields\n  for (const field of schema.fields) {\n    field.indexed = indexedFields.includes(field.name);\n  }\n\n  // Chunk the data\n  console.log(\"Chunking data...\");\n  const chunks = chunkRecords(records, schema, {\n    targetSize: targetChunkSize,\n    chunkBy,\n  });\n  console.log(`Created ${chunks.length} chunks`);\n\n  // Build indices\n  console.log(\"Building indices...\");\n  const indices = buildInvertedIndices(chunks, schema, indexedFields);\n  console.log(`Built indices for ${Object.keys(indices).length} fields`);\n\n  // Create output directory\n  const outputDir = path.resolve(options.output);\n  const chunksDir = path.join(outputDir, \"chunks\");\n  await fs.promises.mkdir(chunksDir, { recursive: true });\n\n  // Write chunks\n  console.log(\"Writing chunks...\");\n  const chunkMetas = [];\n  for (const chunk of chunks) {\n    const chunkPath = path.join(chunksDir, `${chunk.id}.json`);\n    await fs.promises.writeFile(chunkPath, serializeChunk(chunk));\n    chunkMetas.push(generateChunkMeta(chunk, schema, \"chunks\"));\n  }\n\n  // Build config\n  const config: BuildConfig = {\n    chunkSize: targetChunkSize,\n    chunkBy: chunkBy || null,\n    indexedFields,\n  };\n\n  // Build manifest\n  const manifest: Manifest = {\n    version: VERSION,\n    generatedAt: new Date().toISOString(),\n    schema,\n    chunks: chunkMetas,\n    indices,\n    totalRecords: records.length,\n    config,\n  };\n\n  // Write manifest\n  const manifestPath = path.join(outputDir, \"manifest.json\");\n  await fs.promises.writeFile(manifestPath, JSON.stringify(manifest, null, 2));\n  console.log(`Wrote manifest to ${manifestPath}`);\n\n  // Generate client\n  console.log(\"Generating client...\");\n  const clientCode = generateClient(schema, manifest);\n  const clientPath = path.join(outputDir, \"client.ts\");\n  await fs.promises.writeFile(clientPath, clientCode);\n  console.log(`Wrote client to ${clientPath}`);\n\n  const elapsed = ((Date.now() - startTime) / 1000).toFixed(2);\n  console.log(`\\nBuild completed in ${elapsed}s`);\n  console.log(`Output: ${outputDir}`);\n  console.log(`  - ${chunks.length} chunks`);\n  console.log(`  - ${records.length} total records`);\n  console.log(`  - manifest.json`);\n  console.log(`  - client.ts`);\n\n  return {\n    manifest,\n    outputDir,\n    chunkCount: chunks.length,\n    totalRecords: records.length,\n  };\n}\n\nfunction formatBytes(bytes: number): string {\n  if (bytes < 1024) return `${bytes} B`;\n  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)} KB`;\n  if (bytes < 1024 * 1024 * 1024) return `${(bytes / (1024 * 1024)).toFixed(1)} MB`;\n  return `${(bytes / (1024 * 1024 * 1024)).toFixed(1)} GB`;\n}\n","/**\n * Data file parsers for JSON, NDJSON, and CSV formats\n */\n\nimport * as fs from \"node:fs\";\nimport * as readline from \"node:readline\";\nimport Papa from \"papaparse\";\nimport type { DataFormat, DataRecord } from \"../../types/index.js\";\n\n/**\n * Detect file format from extension or content\n */\nexport function detectFormat(filePath: string): DataFormat {\n  const ext = filePath.toLowerCase().split(\".\").pop();\n\n  if (ext === \"csv\") return \"csv\";\n  if (ext === \"ndjson\" || ext === \"jsonl\") return \"ndjson\";\n  if (ext === \"json\") {\n    // Peek at file to distinguish JSON array from NDJSON\n    const fd = fs.openSync(filePath, \"r\");\n    const buffer = Buffer.alloc(1024);\n    fs.readSync(fd, buffer, 0, 1024, 0);\n    fs.closeSync(fd);\n\n    const content = buffer.toString(\"utf-8\").trim();\n    if (content.startsWith(\"[\")) return \"json\";\n    if (content.startsWith(\"{\")) return \"ndjson\";\n  }\n\n  return \"json\"; // default\n}\n\n/**\n * Parse a JSON array file\n * For large files, reads entirely into memory (JSON parsing requires this)\n */\nexport async function parseJsonArray(filePath: string): Promise<DataRecord[]> {\n  const content = await fs.promises.readFile(filePath, \"utf-8\");\n  const data = JSON.parse(content);\n\n  if (!Array.isArray(data)) {\n    throw new Error(\"JSON file must contain an array of objects\");\n  }\n\n  return data as DataRecord[];\n}\n\n/**\n * Parse an NDJSON file (newline-delimited JSON)\n * Streams line-by-line for memory efficiency\n */\nexport async function parseNdjson(\n  filePath: string,\n  onRecord?: (record: DataRecord, index: number) => void\n): Promise<DataRecord[]> {\n  const records: DataRecord[] = [];\n  let index = 0;\n\n  const fileStream = fs.createReadStream(filePath, { encoding: \"utf-8\" });\n  const rl = readline.createInterface({\n    input: fileStream,\n    crlfDelay: Infinity,\n  });\n\n  for await (const line of rl) {\n    const trimmed = line.trim();\n    if (!trimmed) continue;\n\n    try {\n      const record = JSON.parse(trimmed) as DataRecord;\n      records.push(record);\n      onRecord?.(record, index);\n      index++;\n    } catch (e) {\n      throw new Error(`Invalid JSON at line ${index + 1}: ${trimmed.slice(0, 50)}...`);\n    }\n  }\n\n  return records;\n}\n\n/**\n * Parse a CSV file\n * Uses papaparse for robust CSV handling\n */\nexport async function parseCsv(\n  filePath: string,\n  onRecord?: (record: DataRecord, index: number) => void\n): Promise<DataRecord[]> {\n  return new Promise((resolve, reject) => {\n    const records: DataRecord[] = [];\n    let index = 0;\n\n    const fileStream = fs.createReadStream(filePath, { encoding: \"utf-8\" });\n\n    Papa.parse(fileStream, {\n      header: true,\n      dynamicTyping: true,\n      skipEmptyLines: true,\n      step: (result) => {\n        if (result.errors.length > 0) {\n          reject(new Error(`CSV parse error: ${result.errors[0].message}`));\n          return;\n        }\n        const record = result.data as DataRecord;\n        records.push(record);\n        onRecord?.(record, index);\n        index++;\n      },\n      complete: () => resolve(records),\n      error: (error) => reject(error),\n    });\n  });\n}\n\n/**\n * Parse a data file in any supported format\n */\nexport async function parseFile(\n  filePath: string,\n  format?: DataFormat,\n  onRecord?: (record: DataRecord, index: number) => void\n): Promise<{ records: DataRecord[]; format: DataFormat }> {\n  const detectedFormat = format || detectFormat(filePath);\n\n  let records: DataRecord[];\n\n  switch (detectedFormat) {\n    case \"json\":\n      records = await parseJsonArray(filePath);\n      // Call onRecord for each if provided\n      if (onRecord) {\n        records.forEach((r, i) => onRecord(r, i));\n      }\n      break;\n    case \"ndjson\":\n      records = await parseNdjson(filePath, onRecord);\n      break;\n    case \"csv\":\n      records = await parseCsv(filePath, onRecord);\n      break;\n    default:\n      throw new Error(`Unsupported format: ${detectedFormat}`);\n  }\n\n  return { records, format: detectedFormat };\n}\n\n/**\n * Get file size in bytes\n */\nexport function getFileSize(filePath: string): number {\n  const stats = fs.statSync(filePath);\n  return stats.size;\n}\n\n/**\n * Parse size string (e.g., \"5mb\", \"1gb\") to bytes\n */\nexport function parseSize(sizeStr: string): number {\n  const match = sizeStr.toLowerCase().match(/^(\\d+(?:\\.\\d+)?)\\s*(b|kb|mb|gb)?$/);\n  if (!match) {\n    throw new Error(`Invalid size format: ${sizeStr}. Use format like \"5mb\" or \"1gb\"`);\n  }\n\n  const value = parseFloat(match[1]);\n  const unit = match[2] || \"b\";\n\n  const multipliers: Record<string, number> = {\n    b: 1,\n    kb: 1024,\n    mb: 1024 * 1024,\n    gb: 1024 * 1024 * 1024,\n  };\n\n  return Math.floor(value * multipliers[unit]);\n}\n","/**\n * Schema inference from data records\n */\n\nimport type { DataRecord, FieldSchema, FieldStats, FieldType, Schema } from \"../../types/index.js\";\n\n/**\n * Infer the type of a value\n */\nfunction inferType(value: unknown): FieldType {\n  if (value === null || value === undefined) {\n    return \"null\";\n  }\n\n  const type = typeof value;\n\n  if (type === \"string\") {\n    // Check if it's a date string\n    if (isDateString(value as string)) {\n      return \"date\";\n    }\n    return \"string\";\n  }\n\n  if (type === \"number\") {\n    return \"number\";\n  }\n\n  if (type === \"boolean\") {\n    return \"boolean\";\n  }\n\n  // Arrays and objects treated as string (JSON serialized)\n  return \"string\";\n}\n\n/**\n * Check if a string looks like a date\n */\nfunction isDateString(value: string): boolean {\n  // ISO 8601 date patterns\n  const isoPattern = /^\\d{4}-\\d{2}-\\d{2}(T\\d{2}:\\d{2}:\\d{2}(\\.\\d{3})?(Z|[+-]\\d{2}:?\\d{2})?)?$/;\n  if (isoPattern.test(value)) {\n    const date = new Date(value);\n    return !isNaN(date.getTime());\n  }\n  return false;\n}\n\n/**\n * Merge two field types, returning the more general type\n */\nfunction mergeTypes(type1: FieldType, type2: FieldType): FieldType {\n  if (type1 === type2) return type1;\n  if (type1 === \"null\") return type2;\n  if (type2 === \"null\") return type1;\n\n  // If types differ, fall back to string\n  return \"string\";\n}\n\n/**\n * Collect statistics for a field across all records\n */\nclass FieldStatsCollector {\n  private values = new Set<string>();\n  private min: string | number | undefined;\n  private max: string | number | undefined;\n  private nullCount = 0;\n  private count = 0;\n  private type: FieldType = \"null\";\n  private sampleValues: (string | number | boolean | null)[] = [];\n\n  add(value: unknown): void {\n    this.count++;\n\n    if (value === null || value === undefined) {\n      this.nullCount++;\n      return;\n    }\n\n    const valueType = inferType(value);\n    this.type = mergeTypes(this.type, valueType);\n\n    // Track unique values for cardinality (limit to avoid memory issues)\n    if (this.values.size < 10000) {\n      this.values.add(String(value));\n    }\n\n    // Collect sample values\n    if (this.sampleValues.length < 5 && !this.sampleValues.includes(value as string | number | boolean | null)) {\n      this.sampleValues.push(value as string | number | boolean | null);\n    }\n\n    // Track min/max for numbers and dates\n    if (typeof value === \"number\") {\n      if (this.min === undefined || value < (this.min as number)) {\n        this.min = value;\n      }\n      if (this.max === undefined || value > (this.max as number)) {\n        this.max = value;\n      }\n    } else if (typeof value === \"string\" && (this.type === \"date\" || this.type === \"string\")) {\n      if (this.min === undefined || value < this.min) {\n        this.min = value;\n      }\n      if (this.max === undefined || value > this.max) {\n        this.max = value;\n      }\n    }\n  }\n\n  getStats(): FieldStats {\n    return {\n      min: this.min,\n      max: this.max,\n      cardinality: this.values.size,\n      nullCount: this.nullCount,\n      sampleValues: this.sampleValues,\n    };\n  }\n\n  getType(): FieldType {\n    return this.type;\n  }\n\n  isNullable(): boolean {\n    return this.nullCount > 0;\n  }\n}\n\n/**\n * Infer schema from a set of records\n */\nexport function inferSchema(records: DataRecord[]): Schema {\n  if (records.length === 0) {\n    return { fields: [], primaryField: null };\n  }\n\n  // Collect all field names\n  const fieldNames = new Set<string>();\n  for (const record of records) {\n    for (const key of Object.keys(record)) {\n      fieldNames.add(key);\n    }\n  }\n\n  // Collect stats for each field\n  const collectors = new Map<string, FieldStatsCollector>();\n  for (const name of fieldNames) {\n    collectors.set(name, new FieldStatsCollector());\n  }\n\n  for (const record of records) {\n    for (const name of fieldNames) {\n      const collector = collectors.get(name)!;\n      collector.add(record[name]);\n    }\n  }\n\n  // Build field schemas\n  const fields: FieldSchema[] = [];\n  let primaryField: string | null = null;\n\n  for (const name of fieldNames) {\n    const collector = collectors.get(name)!;\n    const stats = collector.getStats();\n    const type = collector.getType();\n\n    // Determine if field should be indexed\n    // Index low-cardinality fields (good for filtering)\n    const cardinality = stats.cardinality;\n    const isLowCardinality = cardinality <= 1000 && cardinality < records.length * 0.5;\n    const indexed = isLowCardinality;\n\n    // Detect potential primary key\n    // High cardinality, not nullable, unique values\n    if (\n      primaryField === null &&\n      stats.cardinality === records.length &&\n      !collector.isNullable() &&\n      (name.toLowerCase().includes(\"id\") || name.toLowerCase() === \"key\")\n    ) {\n      primaryField = name;\n    }\n\n    fields.push({\n      name,\n      type,\n      nullable: collector.isNullable(),\n      indexed,\n      stats,\n    });\n  }\n\n  // Sort fields: primary key first, then alphabetically\n  fields.sort((a, b) => {\n    if (a.name === primaryField) return -1;\n    if (b.name === primaryField) return 1;\n    return a.name.localeCompare(b.name);\n  });\n\n  return { fields, primaryField };\n}\n\n/**\n * Suggest the best field to chunk by\n */\nexport function suggestChunkField(schema: Schema, records: DataRecord[]): string | null {\n  // If there's a primary field, chunk by it\n  if (schema.primaryField) {\n    return schema.primaryField;\n  }\n\n  // Look for a good chunking candidate:\n  // - High cardinality (spreads data across chunks)\n  // - Numeric or date (supports range queries)\n  // - Not nullable\n  let bestField: string | null = null;\n  let bestScore = 0;\n\n  for (const field of schema.fields) {\n    if (field.nullable) continue;\n\n    let score = 0;\n\n    // Prefer numeric/date fields\n    if (field.type === \"number\" || field.type === \"date\") {\n      score += 50;\n    }\n\n    // Prefer higher cardinality (but not too high)\n    const cardinalityRatio = field.stats.cardinality / records.length;\n    if (cardinalityRatio > 0.1 && cardinalityRatio <= 1) {\n      score += cardinalityRatio * 30;\n    }\n\n    // Prefer fields with \"id\", \"key\", \"date\", \"time\" in name\n    const nameLower = field.name.toLowerCase();\n    if (nameLower.includes(\"id\")) score += 20;\n    if (nameLower.includes(\"date\") || nameLower.includes(\"time\")) score += 15;\n    if (nameLower.includes(\"created\") || nameLower.includes(\"updated\")) score += 10;\n\n    if (score > bestScore) {\n      bestScore = score;\n      bestField = field.name;\n    }\n  }\n\n  return bestField;\n}\n\n/**\n * Get fields that should be indexed based on cardinality\n */\nexport function getIndexableFields(schema: Schema): string[] {\n  return schema.fields\n    .filter((f) => f.indexed)\n    .map((f) => f.name);\n}\n","/**\n * Data chunking logic for splitting records into smaller files\n */\n\nimport type { ChunkMeta, DataRecord, Schema } from \"../../types/index.js\";\n\nexport interface Chunk {\n  id: string;\n  records: DataRecord[];\n  byteSize: number;\n}\n\nexport interface ChunkOptions {\n  targetSize: number; // target bytes per chunk\n  chunkBy?: string; // field to sort/chunk by\n}\n\n/**\n * Estimate JSON byte size of a record\n */\nfunction estimateRecordSize(record: DataRecord): number {\n  return JSON.stringify(record).length;\n}\n\n/**\n * Compare two values for sorting\n */\nfunction compareValues(a: unknown, b: unknown): number {\n  if (a === b) return 0;\n  if (a === null || a === undefined) return -1;\n  if (b === null || b === undefined) return 1;\n\n  if (typeof a === \"number\" && typeof b === \"number\") {\n    return a - b;\n  }\n\n  return String(a).localeCompare(String(b));\n}\n\n/**\n * Split records into chunks based on target size\n */\nexport function chunkRecords(\n  records: DataRecord[],\n  schema: Schema,\n  options: ChunkOptions\n): Chunk[] {\n  const { targetSize, chunkBy } = options;\n\n  // Sort records if chunkBy field is specified\n  let sortedRecords = records;\n  if (chunkBy) {\n    sortedRecords = [...records].sort((a, b) =>\n      compareValues(a[chunkBy], b[chunkBy])\n    );\n  }\n\n  const chunks: Chunk[] = [];\n  let currentChunk: DataRecord[] = [];\n  let currentSize = 0;\n  let chunkIndex = 0;\n\n  // Account for JSON array overhead: [ and ] plus commas\n  const arrayOverhead = 2; // []\n  const commaOverhead = 1; // ,\n\n  for (const record of sortedRecords) {\n    const recordSize = estimateRecordSize(record) + commaOverhead;\n\n    // If adding this record would exceed target size and we have records,\n    // start a new chunk\n    if (currentSize + recordSize > targetSize && currentChunk.length > 0) {\n      chunks.push({\n        id: String(chunkIndex),\n        records: currentChunk,\n        byteSize: currentSize + arrayOverhead,\n      });\n      chunkIndex++;\n      currentChunk = [];\n      currentSize = 0;\n    }\n\n    currentChunk.push(record);\n    currentSize += recordSize;\n  }\n\n  // Don't forget the last chunk\n  if (currentChunk.length > 0) {\n    chunks.push({\n      id: String(chunkIndex),\n      records: currentChunk,\n      byteSize: currentSize + arrayOverhead,\n    });\n  }\n\n  return chunks;\n}\n\n/**\n * Calculate field ranges for a chunk (min/max per field)\n */\nexport function calculateFieldRanges(\n  records: DataRecord[],\n  schema: Schema\n): ChunkMeta[\"fieldRanges\"] {\n  const ranges: ChunkMeta[\"fieldRanges\"] = {};\n\n  for (const field of schema.fields) {\n    const values = records\n      .map((r) => r[field.name])\n      .filter((v) => v !== null && v !== undefined);\n\n    if (values.length === 0) {\n      continue;\n    }\n\n    // For numbers and dates, calculate actual min/max\n    if (field.type === \"number\" || field.type === \"date\" || field.type === \"string\") {\n      const sorted = [...values].sort(compareValues);\n      ranges[field.name] = {\n        min: sorted[0],\n        max: sorted[sorted.length - 1],\n      };\n    }\n  }\n\n  return ranges;\n}\n\n/**\n * Generate chunk metadata\n */\nexport function generateChunkMeta(\n  chunk: Chunk,\n  schema: Schema,\n  basePath: string\n): ChunkMeta {\n  return {\n    id: chunk.id,\n    path: `${basePath}/${chunk.id}.json`,\n    count: chunk.records.length,\n    byteSize: chunk.byteSize,\n    fieldRanges: calculateFieldRanges(chunk.records, schema),\n  };\n}\n\n/**\n * Serialize a chunk to JSON string\n */\nexport function serializeChunk(chunk: Chunk): string {\n  return JSON.stringify(chunk.records);\n}\n\n/**\n * Calculate optimal chunk count based on data size and target chunk size\n */\nexport function calculateChunkCount(\n  totalRecords: number,\n  totalSize: number,\n  targetChunkSize: number\n): number {\n  const estimatedChunks = Math.ceil(totalSize / targetChunkSize);\n  // At least 1 chunk, and reasonable upper bound\n  return Math.max(1, Math.min(estimatedChunks, totalRecords));\n}\n","/**\n * Index generation for efficient query pruning\n */\n\nimport type { Chunk } from \"./chunker.js\";\nimport type { Schema } from \"../../types/index.js\";\n\n/**\n * Inverted index: field -> value -> chunk IDs\n */\nexport type InvertedIndex = Record<string, Record<string, string[]>>;\n\n/**\n * Build inverted indices for indexed fields\n * Maps each unique value to the chunk IDs that contain it\n */\nexport function buildInvertedIndices(\n  chunks: Chunk[],\n  schema: Schema,\n  indexedFields?: string[]\n): InvertedIndex {\n  const indices: InvertedIndex = {};\n\n  // Get fields to index\n  const fieldsToIndex = indexedFields\n    ? schema.fields.filter((f) => indexedFields.includes(f.name))\n    : schema.fields.filter((f) => f.indexed);\n\n  for (const field of fieldsToIndex) {\n    const fieldIndex: Record<string, string[]> = {};\n\n    for (const chunk of chunks) {\n      const valuesInChunk = new Set<string>();\n\n      for (const record of chunk.records) {\n        const value = record[field.name];\n        if (value !== null && value !== undefined) {\n          valuesInChunk.add(String(value));\n        }\n      }\n\n      // Add chunk ID to each value's list\n      for (const value of valuesInChunk) {\n        if (!fieldIndex[value]) {\n          fieldIndex[value] = [];\n        }\n        fieldIndex[value].push(chunk.id);\n      }\n    }\n\n    // Only include index if it would be useful\n    // (not too many unique values, which would make the index huge)\n    const uniqueValues = Object.keys(fieldIndex).length;\n    if (uniqueValues <= 10000) {\n      indices[field.name] = fieldIndex;\n    }\n  }\n\n  return indices;\n}\n\n/**\n * Find chunk IDs that might contain records matching a field value\n */\nexport function findChunksForValue(\n  indices: InvertedIndex,\n  fieldName: string,\n  value: unknown\n): string[] | null {\n  const fieldIndex = indices[fieldName];\n  if (!fieldIndex) {\n    // Field not indexed, return null to indicate all chunks must be searched\n    return null;\n  }\n\n  const stringValue = String(value);\n  return fieldIndex[stringValue] || [];\n}\n\n/**\n * Find chunk IDs that match multiple field conditions (AND logic)\n */\nexport function findChunksForConditions(\n  indices: InvertedIndex,\n  conditions: Record<string, unknown>\n): string[] | null {\n  let resultChunks: Set<string> | null = null;\n\n  for (const [field, value] of Object.entries(conditions)) {\n    const chunks = findChunksForValue(indices, field, value);\n\n    if (chunks === null) {\n      // This field isn't indexed, can't narrow down\n      continue;\n    }\n\n    if (resultChunks === null) {\n      resultChunks = new Set(chunks);\n    } else {\n      // Intersect with existing results\n      resultChunks = new Set([...resultChunks].filter((c) => chunks.includes(c)));\n    }\n\n    // Early exit if no chunks match\n    if (resultChunks.size === 0) {\n      return [];\n    }\n  }\n\n  return resultChunks ? [...resultChunks] : null;\n}\n\n/**\n * Estimate index size in bytes\n */\nexport function estimateIndexSize(indices: InvertedIndex): number {\n  return JSON.stringify(indices).length;\n}\n","/**\n * TypeScript client code generator\n */\n\nimport type { FieldSchema, Manifest, Schema } from \"../../types/index.js\";\n\n/**\n * Convert field type to TypeScript type\n */\nfunction fieldTypeToTs(field: FieldSchema): string {\n  let tsType: string;\n\n  switch (field.type) {\n    case \"string\":\n    case \"date\":\n      tsType = \"string\";\n      break;\n    case \"number\":\n      tsType = \"number\";\n      break;\n    case \"boolean\":\n      tsType = \"boolean\";\n      break;\n    case \"null\":\n      tsType = \"null\";\n      break;\n    default:\n      tsType = \"unknown\";\n  }\n\n  if (field.nullable) {\n    tsType += \" | null\";\n  }\n\n  return tsType;\n}\n\n/**\n * Generate TypeScript interface for records\n */\nfunction generateRecordInterface(schema: Schema): string {\n  const fields = schema.fields\n    .map((f) => `  ${f.name}: ${fieldTypeToTs(f)};`)\n    .join(\"\\n\");\n\n  return `export interface Record {\n${fields}\n}`;\n}\n\n/**\n * Generate field names type\n */\nfunction generateFieldNamesType(schema: Schema): string {\n  const names = schema.fields.map((f) => `\"${f.name}\"`).join(\" | \");\n  return `export type FieldName = ${names};`;\n}\n\n/**\n * Generate where clause types\n */\nfunction generateWhereTypes(schema: Schema): string {\n  const stringFields = schema.fields\n    .filter((f) => f.type === \"string\" || f.type === \"date\")\n    .map((f) => f.name);\n\n  const numericFields = schema.fields\n    .filter((f) => f.type === \"number\")\n    .map((f) => f.name);\n\n  return `\nexport type StringOperators = {\n  eq?: string;\n  neq?: string;\n  contains?: string;\n  startsWith?: string;\n  endsWith?: string;\n  in?: string[];\n};\n\nexport type NumericOperators = {\n  eq?: number;\n  neq?: number;\n  gt?: number;\n  gte?: number;\n  lt?: number;\n  lte?: number;\n  in?: number[];\n};\n\nexport type WhereClause = {\n${schema.fields\n  .map((f) => {\n    if (f.type === \"number\") {\n      return `  ${f.name}?: number | NumericOperators;`;\n    } else if (f.type === \"boolean\") {\n      return `  ${f.name}?: boolean;`;\n    } else {\n      return `  ${f.name}?: string | StringOperators;`;\n    }\n  })\n  .join(\"\\n\")}\n};`;\n}\n\n/**\n * Generate the full client code\n */\nexport function generateClient(schema: Schema, manifest: Manifest): string {\n  const recordInterface = generateRecordInterface(schema);\n  const fieldNamesType = generateFieldNamesType(schema);\n  const whereTypes = generateWhereTypes(schema);\n\n  const sortableFields = schema.fields\n    .filter((f) => f.type === \"number\" || f.type === \"string\" || f.type === \"date\")\n    .map((f) => `\"${f.name}\"`)\n    .join(\" | \");\n\n  return `/**\n * Auto-generated client for static-shard\n * Generated at: ${manifest.generatedAt}\n * Total records: ${manifest.totalRecords}\n * Chunks: ${manifest.chunks.length}\n */\n\n// ============================================================================\n// Types\n// ============================================================================\n\n${recordInterface}\n\n${fieldNamesType}\n\n${whereTypes}\n\nexport type SortableField = ${sortableFields || \"string\"};\n\nexport interface QueryOptions {\n  where?: WhereClause;\n  orderBy?: SortableField | { field: SortableField; direction: \"asc\" | \"desc\" };\n  limit?: number;\n  offset?: number;\n}\n\nexport interface Manifest {\n  version: string;\n  schema: {\n    fields: Array<{\n      name: string;\n      type: string;\n      nullable: boolean;\n      indexed: boolean;\n    }>;\n    primaryField: string | null;\n  };\n  chunks: Array<{\n    id: string;\n    path: string;\n    count: number;\n    byteSize: number;\n    fieldRanges: { [field: string]: { min: unknown; max: unknown } };\n  }>;\n  indices: { [field: string]: { [value: string]: string[] } };\n  totalRecords: number;\n}\n\n// ============================================================================\n// Runtime\n// ============================================================================\n\ninterface ClientOptions {\n  basePath: string;\n}\n\nclass StaticShardClient {\n  private basePath: string;\n  private manifest: Manifest | null = null;\n  private chunkCache: Map<string, Record[]> = new Map();\n\n  constructor(options: ClientOptions) {\n    this.basePath = options.basePath.replace(/\\\\/$/, \"\");\n  }\n\n  /**\n   * Load the manifest file\n   */\n  private async loadManifest(): Promise<Manifest> {\n    if (this.manifest) return this.manifest;\n\n    const response = await fetch(\\`\\${this.basePath}/manifest.json\\`);\n    if (!response.ok) {\n      throw new Error(\\`Failed to load manifest: \\${response.statusText}\\`);\n    }\n\n    this.manifest = await response.json();\n    return this.manifest!;\n  }\n\n  /**\n   * Load a chunk by ID\n   */\n  private async loadChunk(chunkId: string): Promise<Record[]> {\n    const cached = this.chunkCache.get(chunkId);\n    if (cached) return cached;\n\n    const manifest = await this.loadManifest();\n    const chunkMeta = manifest.chunks.find((c) => c.id === chunkId);\n    if (!chunkMeta) {\n      throw new Error(\\`Chunk not found: \\${chunkId}\\`);\n    }\n\n    const response = await fetch(\\`\\${this.basePath}/\\${chunkMeta.path}\\`);\n    if (!response.ok) {\n      throw new Error(\\`Failed to load chunk \\${chunkId}: \\${response.statusText}\\`);\n    }\n\n    const records = await response.json();\n    this.chunkCache.set(chunkId, records);\n    return records;\n  }\n\n  /**\n   * Find chunk IDs that might contain matching records\n   */\n  private findCandidateChunks(manifest: Manifest, where?: WhereClause): string[] {\n    if (!where) {\n      return manifest.chunks.map((c) => c.id);\n    }\n\n    let candidateChunks: Set<string> | null = null;\n\n    for (const [field, condition] of Object.entries(where)) {\n      // Check if we can use the index\n      const index = manifest.indices[field];\n\n      if (index && (typeof condition === \"string\" || typeof condition === \"number\" || typeof condition === \"boolean\")) {\n        const value = String(condition);\n        const chunks = index[value] || [];\n\n        if (candidateChunks === null) {\n          candidateChunks = new Set(chunks);\n        } else {\n          candidateChunks = new Set(Array.from(candidateChunks).filter((c) => chunks.includes(c)));\n        }\n      } else if (typeof condition === \"object\" && condition !== null && \"eq\" in condition) {\n        const value = String(condition.eq);\n        const chunks = index?.[value] || [];\n\n        if (index) {\n          if (candidateChunks === null) {\n            candidateChunks = new Set(chunks);\n          } else {\n            candidateChunks = new Set(Array.from(candidateChunks).filter((c) => chunks.includes(c)));\n          }\n        }\n      }\n\n      // Range pruning using fieldRanges\n      if (typeof condition === \"object\" && condition !== null) {\n        const rangeCondition = condition as { gt?: number; gte?: number; lt?: number; lte?: number };\n        const hasRangeOp = \"gt\" in rangeCondition || \"gte\" in rangeCondition || \"lt\" in rangeCondition || \"lte\" in rangeCondition;\n\n        if (hasRangeOp) {\n          const matchingChunks = manifest.chunks\n            .filter((chunk) => {\n              const range = chunk.fieldRanges[field];\n              if (!range) return true; // Can't prune, include chunk\n\n              const min = range.min as number;\n              const max = range.max as number;\n\n              if (rangeCondition.gt !== undefined && max <= rangeCondition.gt) return false;\n              if (rangeCondition.gte !== undefined && max < rangeCondition.gte) return false;\n              if (rangeCondition.lt !== undefined && min >= rangeCondition.lt) return false;\n              if (rangeCondition.lte !== undefined && min > rangeCondition.lte) return false;\n\n              return true;\n            })\n            .map((c) => c.id);\n\n          if (candidateChunks === null) {\n            candidateChunks = new Set(matchingChunks);\n          } else {\n            candidateChunks = new Set(Array.from(candidateChunks).filter((c) => matchingChunks.includes(c)));\n          }\n        }\n      }\n    }\n\n    return candidateChunks ? Array.from(candidateChunks) : manifest.chunks.map((c) => c.id);\n  }\n\n  /**\n   * Check if a record matches the where clause\n   */\n  private matchesWhere(record: Record, where?: WhereClause): boolean {\n    if (!where) return true;\n\n    for (const [field, condition] of Object.entries(where)) {\n      const value = record[field as keyof Record];\n\n      // Direct value comparison\n      if (typeof condition !== \"object\" || condition === null) {\n        if (value !== condition) return false;\n        continue;\n      }\n\n      const ops = condition as StringOperators & NumericOperators;\n\n      if (\"eq\" in ops && value !== ops.eq) return false;\n      if (\"neq\" in ops && value === ops.neq) return false;\n      if (\"gt\" in ops && (typeof value !== \"number\" || value <= ops.gt!)) return false;\n      if (\"gte\" in ops && (typeof value !== \"number\" || value < ops.gte!)) return false;\n      if (\"lt\" in ops && (typeof value !== \"number\" || value >= ops.lt!)) return false;\n      if (\"lte\" in ops && (typeof value !== \"number\" || value > ops.lte!)) return false;\n      if (\"contains\" in ops && (typeof value !== \"string\" || !value.includes(ops.contains!))) return false;\n      if (\"startsWith\" in ops && (typeof value !== \"string\" || !value.startsWith(ops.startsWith!))) return false;\n      if (\"endsWith\" in ops && (typeof value !== \"string\" || !value.endsWith(ops.endsWith!))) return false;\n      if (\"in\" in ops && !(ops.in as unknown[])!.includes(value)) return false;\n    }\n\n    return true;\n  }\n\n  /**\n   * Query records\n   */\n  async query(options: QueryOptions = {}): Promise<Record[]> {\n    const manifest = await this.loadManifest();\n\n    // Find candidate chunks\n    const candidateChunkIds = this.findCandidateChunks(manifest, options.where);\n\n    // Load chunks in parallel\n    const chunkPromises = candidateChunkIds.map((id) => this.loadChunk(id));\n    const chunks = await Promise.all(chunkPromises);\n\n    // Flatten and filter\n    let results: Record[] = [];\n    for (const chunk of chunks) {\n      for (const record of chunk) {\n        if (this.matchesWhere(record, options.where)) {\n          results.push(record);\n        }\n      }\n    }\n\n    // Sort\n    if (options.orderBy) {\n      const field = typeof options.orderBy === \"string\" ? options.orderBy : options.orderBy.field;\n      const direction = typeof options.orderBy === \"string\" ? \"asc\" : options.orderBy.direction;\n\n      results.sort((a, b) => {\n        const aVal = a[field as keyof Record];\n        const bVal = b[field as keyof Record];\n\n        if (aVal === bVal) return 0;\n        if (aVal === null || aVal === undefined) return 1;\n        if (bVal === null || bVal === undefined) return -1;\n\n        const cmp = aVal < bVal ? -1 : 1;\n        return direction === \"asc\" ? cmp : -cmp;\n      });\n    }\n\n    // Pagination\n    const offset = options.offset || 0;\n    const limit = options.limit;\n\n    if (offset > 0 || limit !== undefined) {\n      results = results.slice(offset, limit !== undefined ? offset + limit : undefined);\n    }\n\n    return results;\n  }\n\n  /**\n   * Get a single record by primary key\n   */\n  async get(id: string | number): Promise<Record | null> {\n    const manifest = await this.loadManifest();\n    const primaryField = manifest.schema.primaryField;\n\n    if (!primaryField) {\n      throw new Error(\"No primary field defined in schema\");\n    }\n\n    const results = await this.query({\n      where: { [primaryField]: id } as WhereClause,\n      limit: 1,\n    });\n\n    return results[0] || null;\n  }\n\n  /**\n   * Count records matching a query\n   */\n  async count(options: { where?: WhereClause } = {}): Promise<number> {\n    const manifest = await this.loadManifest();\n\n    if (!options.where) {\n      return manifest.totalRecords;\n    }\n\n    // For complex queries, we need to load and count\n    const results = await this.query({ where: options.where });\n    return results.length;\n  }\n\n  /**\n   * Get schema information\n   */\n  async getSchema(): Promise<Manifest[\"schema\"]> {\n    const manifest = await this.loadManifest();\n    return manifest.schema;\n  }\n\n  /**\n   * Clear the chunk cache\n   */\n  clearCache(): void {\n    this.chunkCache.clear();\n  }\n}\n\n// ============================================================================\n// Export\n// ============================================================================\n\nexport function createClient(options: ClientOptions): StaticShardClient {\n  return new StaticShardClient(options);\n}\n\n// Default export for convenience\nexport const db = createClient({ basePath: \".\" });\n`;\n}\n","/**\n * Inspect command - analyze data file and suggest chunking strategy\n */\n\nimport * as fs from \"node:fs\";\nimport type { InspectOptions } from \"../../types/index.js\";\nimport { getFileSize, parseFile } from \"../utils/parsers.js\";\nimport { inferSchema, suggestChunkField, getIndexableFields } from \"../utils/schema.js\";\n\nexport async function inspect(\n  inputFile: string,\n  options: InspectOptions\n): Promise<void> {\n  // Validate input file exists\n  if (!fs.existsSync(inputFile)) {\n    throw new Error(`Input file not found: ${inputFile}`);\n  }\n\n  const fileSize = getFileSize(inputFile);\n  console.log(`\\nFile: ${inputFile}`);\n  console.log(`Size: ${formatBytes(fileSize)}`);\n\n  console.log(\"\\nParsing file...\");\n\n  // Parse the file\n  const { records, format } = await parseFile(inputFile, options.format);\n\n  console.log(`Format: ${format}`);\n  console.log(`Total records: ${records.length}`);\n\n  if (records.length === 0) {\n    console.log(\"\\nNo records found.\");\n    return;\n  }\n\n  // Sample records for schema inference if needed\n  const sampleSize = Math.min(options.sample || 1000, records.length);\n  const sampleRecords = records.slice(0, sampleSize);\n\n  console.log(`\\nAnalyzing ${sampleSize} records...`);\n\n  // Infer schema\n  const schema = inferSchema(sampleRecords);\n\n  console.log(\"\\n\" + \"=\".repeat(60));\n  console.log(\"SCHEMA\");\n  console.log(\"=\".repeat(60));\n\n  console.log(`\\nFields (${schema.fields.length}):\\n`);\n\n  for (const field of schema.fields) {\n    const isPrimary = field.name === schema.primaryField ? \" [PRIMARY]\" : \"\";\n    const isIndexed = field.indexed ? \" [INDEXED]\" : \"\";\n    const nullable = field.nullable ? \" (nullable)\" : \"\";\n\n    console.log(`  ${field.name}: ${field.type}${nullable}${isPrimary}${isIndexed}`);\n\n    // Show stats\n    const stats = field.stats;\n    const statParts = [];\n\n    if (stats.cardinality !== undefined) {\n      const cardinalityPct = ((stats.cardinality / records.length) * 100).toFixed(1);\n      statParts.push(`cardinality: ${stats.cardinality} (${cardinalityPct}%)`);\n    }\n\n    if (stats.min !== undefined && stats.max !== undefined) {\n      statParts.push(`range: ${formatValue(stats.min)} - ${formatValue(stats.max)}`);\n    }\n\n    if (stats.nullCount > 0) {\n      statParts.push(`nulls: ${stats.nullCount}`);\n    }\n\n    if (statParts.length > 0) {\n      console.log(`    ${statParts.join(\", \")}`);\n    }\n\n    if (stats.sampleValues && stats.sampleValues.length > 0) {\n      const samples = stats.sampleValues.slice(0, 3).map(formatValue).join(\", \");\n      console.log(`    examples: ${samples}`);\n    }\n  }\n\n  console.log(\"\\n\" + \"=\".repeat(60));\n  console.log(\"RECOMMENDATIONS\");\n  console.log(\"=\".repeat(60));\n\n  // Suggest chunk field\n  const suggestedChunkField = suggestChunkField(schema, records);\n  if (suggestedChunkField) {\n    console.log(`\\nRecommended --chunk-by: ${suggestedChunkField}`);\n  } else {\n    console.log(\"\\nNo specific chunk field recommended (will chunk by record order)\");\n  }\n\n  // Suggest indexed fields\n  const indexableFields = getIndexableFields(schema);\n  if (indexableFields.length > 0) {\n    console.log(`Recommended --index: ${indexableFields.join(\",\")}`);\n  } else {\n    console.log(\"No fields recommended for indexing\");\n  }\n\n  // Estimate chunks\n  const avgRecordSize = fileSize / records.length;\n  const targetChunkSize = 5 * 1024 * 1024; // 5MB\n  const estimatedChunks = Math.ceil(fileSize / targetChunkSize);\n\n  console.log(\"\\n\" + \"=\".repeat(60));\n  console.log(\"SIZE ESTIMATES\");\n  console.log(\"=\".repeat(60));\n\n  console.log(`\\nAverage record size: ${formatBytes(avgRecordSize)}`);\n  console.log(`\\nWith default 5MB chunks:`);\n  console.log(`  Estimated chunks: ${estimatedChunks}`);\n  console.log(`  Records per chunk: ~${Math.ceil(records.length / estimatedChunks)}`);\n\n  // Show example command\n  console.log(\"\\n\" + \"=\".repeat(60));\n  console.log(\"EXAMPLE COMMAND\");\n  console.log(\"=\".repeat(60));\n\n  let cmd = `npx static-shard build ${inputFile} --output ./dist`;\n  if (suggestedChunkField) {\n    cmd += ` --chunk-by ${suggestedChunkField}`;\n  }\n  if (indexableFields.length > 0) {\n    cmd += ` --index \"${indexableFields.join(\",\")}\"`;\n  }\n\n  console.log(`\\n${cmd}\\n`);\n}\n\nfunction formatBytes(bytes: number): string {\n  if (bytes < 1024) return `${bytes} B`;\n  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)} KB`;\n  if (bytes < 1024 * 1024 * 1024) return `${(bytes / (1024 * 1024)).toFixed(1)} MB`;\n  return `${(bytes / (1024 * 1024 * 1024)).toFixed(1)} GB`;\n}\n\nfunction formatValue(value: unknown): string {\n  if (value === null) return \"null\";\n  if (value === undefined) return \"undefined\";\n  if (typeof value === \"string\") {\n    if (value.length > 30) {\n      return `\"${value.slice(0, 27)}...\"`;\n    }\n    return `\"${value}\"`;\n  }\n  return String(value);\n}\n"],"mappings":";;;AAMA,SAAS,eAAe;;;ACFxB,YAAYA,SAAQ;AACpB,YAAY,UAAU;;;ACDtB,YAAY,QAAQ;AACpB,YAAY,cAAc;AAC1B,OAAO,UAAU;AAMV,SAAS,aAAa,UAA8B;AACzD,QAAM,MAAM,SAAS,YAAY,EAAE,MAAM,GAAG,EAAE,IAAI;AAElD,MAAI,QAAQ,MAAO,QAAO;AAC1B,MAAI,QAAQ,YAAY,QAAQ,QAAS,QAAO;AAChD,MAAI,QAAQ,QAAQ;AAElB,UAAM,KAAQ,YAAS,UAAU,GAAG;AACpC,UAAM,SAAS,OAAO,MAAM,IAAI;AAChC,IAAG,YAAS,IAAI,QAAQ,GAAG,MAAM,CAAC;AAClC,IAAG,aAAU,EAAE;AAEf,UAAM,UAAU,OAAO,SAAS,OAAO,EAAE,KAAK;AAC9C,QAAI,QAAQ,WAAW,GAAG,EAAG,QAAO;AACpC,QAAI,QAAQ,WAAW,GAAG,EAAG,QAAO;AAAA,EACtC;AAEA,SAAO;AACT;AAMA,eAAsB,eAAe,UAAyC;AAC5E,QAAM,UAAU,MAAS,YAAS,SAAS,UAAU,OAAO;AAC5D,QAAM,OAAO,KAAK,MAAM,OAAO;AAE/B,MAAI,CAAC,MAAM,QAAQ,IAAI,GAAG;AACxB,UAAM,IAAI,MAAM,4CAA4C;AAAA,EAC9D;AAEA,SAAO;AACT;AAMA,eAAsB,YACpB,UACA,UACuB;AACvB,QAAM,UAAwB,CAAC;AAC/B,MAAI,QAAQ;AAEZ,QAAM,aAAgB,oBAAiB,UAAU,EAAE,UAAU,QAAQ,CAAC;AACtE,QAAM,KAAc,yBAAgB;AAAA,IAClC,OAAO;AAAA,IACP,WAAW;AAAA,EACb,CAAC;AAED,mBAAiB,QAAQ,IAAI;AAC3B,UAAM,UAAU,KAAK,KAAK;AAC1B,QAAI,CAAC,QAAS;AAEd,QAAI;AACF,YAAM,SAAS,KAAK,MAAM,OAAO;AACjC,cAAQ,KAAK,MAAM;AACnB,iBAAW,QAAQ,KAAK;AACxB;AAAA,IACF,SAAS,GAAG;AACV,YAAM,IAAI,MAAM,wBAAwB,QAAQ,CAAC,KAAK,QAAQ,MAAM,GAAG,EAAE,CAAC,KAAK;AAAA,IACjF;AAAA,EACF;AAEA,SAAO;AACT;AAMA,eAAsB,SACpB,UACA,UACuB;AACvB,SAAO,IAAI,QAAQ,CAACC,UAAS,WAAW;AACtC,UAAM,UAAwB,CAAC;AAC/B,QAAI,QAAQ;AAEZ,UAAM,aAAgB,oBAAiB,UAAU,EAAE,UAAU,QAAQ,CAAC;AAEtE,SAAK,MAAM,YAAY;AAAA,MACrB,QAAQ;AAAA,MACR,eAAe;AAAA,MACf,gBAAgB;AAAA,MAChB,MAAM,CAAC,WAAW;AAChB,YAAI,OAAO,OAAO,SAAS,GAAG;AAC5B,iBAAO,IAAI,MAAM,oBAAoB,OAAO,OAAO,CAAC,EAAE,OAAO,EAAE,CAAC;AAChE;AAAA,QACF;AACA,cAAM,SAAS,OAAO;AACtB,gBAAQ,KAAK,MAAM;AACnB,mBAAW,QAAQ,KAAK;AACxB;AAAA,MACF;AAAA,MACA,UAAU,MAAMA,SAAQ,OAAO;AAAA,MAC/B,OAAO,CAAC,UAAU,OAAO,KAAK;AAAA,IAChC,CAAC;AAAA,EACH,CAAC;AACH;AAKA,eAAsB,UACpB,UACA,QACA,UACwD;AACxD,QAAM,iBAAiB,UAAU,aAAa,QAAQ;AAEtD,MAAI;AAEJ,UAAQ,gBAAgB;AAAA,IACtB,KAAK;AACH,gBAAU,MAAM,eAAe,QAAQ;AAEvC,UAAI,UAAU;AACZ,gBAAQ,QAAQ,CAAC,GAAG,MAAM,SAAS,GAAG,CAAC,CAAC;AAAA,MAC1C;AACA;AAAA,IACF,KAAK;AACH,gBAAU,MAAM,YAAY,UAAU,QAAQ;AAC9C;AAAA,IACF,KAAK;AACH,gBAAU,MAAM,SAAS,UAAU,QAAQ;AAC3C;AAAA,IACF;AACE,YAAM,IAAI,MAAM,uBAAuB,cAAc,EAAE;AAAA,EAC3D;AAEA,SAAO,EAAE,SAAS,QAAQ,eAAe;AAC3C;AAKO,SAAS,YAAY,UAA0B;AACpD,QAAM,QAAW,YAAS,QAAQ;AAClC,SAAO,MAAM;AACf;AAKO,SAAS,UAAU,SAAyB;AACjD,QAAM,QAAQ,QAAQ,YAAY,EAAE,MAAM,mCAAmC;AAC7E,MAAI,CAAC,OAAO;AACV,UAAM,IAAI,MAAM,wBAAwB,OAAO,kCAAkC;AAAA,EACnF;AAEA,QAAM,QAAQ,WAAW,MAAM,CAAC,CAAC;AACjC,QAAM,OAAO,MAAM,CAAC,KAAK;AAEzB,QAAM,cAAsC;AAAA,IAC1C,GAAG;AAAA,IACH,IAAI;AAAA,IACJ,IAAI,OAAO;AAAA,IACX,IAAI,OAAO,OAAO;AAAA,EACpB;AAEA,SAAO,KAAK,MAAM,QAAQ,YAAY,IAAI,CAAC;AAC7C;;;ACvKA,SAAS,UAAU,OAA2B;AAC5C,MAAI,UAAU,QAAQ,UAAU,QAAW;AACzC,WAAO;AAAA,EACT;AAEA,QAAM,OAAO,OAAO;AAEpB,MAAI,SAAS,UAAU;AAErB,QAAI,aAAa,KAAe,GAAG;AACjC,aAAO;AAAA,IACT;AACA,WAAO;AAAA,EACT;AAEA,MAAI,SAAS,UAAU;AACrB,WAAO;AAAA,EACT;AAEA,MAAI,SAAS,WAAW;AACtB,WAAO;AAAA,EACT;AAGA,SAAO;AACT;AAKA,SAAS,aAAa,OAAwB;AAE5C,QAAM,aAAa;AACnB,MAAI,WAAW,KAAK,KAAK,GAAG;AAC1B,UAAM,OAAO,IAAI,KAAK,KAAK;AAC3B,WAAO,CAAC,MAAM,KAAK,QAAQ,CAAC;AAAA,EAC9B;AACA,SAAO;AACT;AAKA,SAAS,WAAW,OAAkB,OAA6B;AACjE,MAAI,UAAU,MAAO,QAAO;AAC5B,MAAI,UAAU,OAAQ,QAAO;AAC7B,MAAI,UAAU,OAAQ,QAAO;AAG7B,SAAO;AACT;AAKA,IAAM,sBAAN,MAA0B;AAAA,EAChB,SAAS,oBAAI,IAAY;AAAA,EACzB;AAAA,EACA;AAAA,EACA,YAAY;AAAA,EACZ,QAAQ;AAAA,EACR,OAAkB;AAAA,EAClB,eAAqD,CAAC;AAAA,EAE9D,IAAI,OAAsB;AACxB,SAAK;AAEL,QAAI,UAAU,QAAQ,UAAU,QAAW;AACzC,WAAK;AACL;AAAA,IACF;AAEA,UAAM,YAAY,UAAU,KAAK;AACjC,SAAK,OAAO,WAAW,KAAK,MAAM,SAAS;AAG3C,QAAI,KAAK,OAAO,OAAO,KAAO;AAC5B,WAAK,OAAO,IAAI,OAAO,KAAK,CAAC;AAAA,IAC/B;AAGA,QAAI,KAAK,aAAa,SAAS,KAAK,CAAC,KAAK,aAAa,SAAS,KAAyC,GAAG;AAC1G,WAAK,aAAa,KAAK,KAAyC;AAAA,IAClE;AAGA,QAAI,OAAO,UAAU,UAAU;AAC7B,UAAI,KAAK,QAAQ,UAAa,QAAS,KAAK,KAAgB;AAC1D,aAAK,MAAM;AAAA,MACb;AACA,UAAI,KAAK,QAAQ,UAAa,QAAS,KAAK,KAAgB;AAC1D,aAAK,MAAM;AAAA,MACb;AAAA,IACF,WAAW,OAAO,UAAU,aAAa,KAAK,SAAS,UAAU,KAAK,SAAS,WAAW;AACxF,UAAI,KAAK,QAAQ,UAAa,QAAQ,KAAK,KAAK;AAC9C,aAAK,MAAM;AAAA,MACb;AACA,UAAI,KAAK,QAAQ,UAAa,QAAQ,KAAK,KAAK;AAC9C,aAAK,MAAM;AAAA,MACb;AAAA,IACF;AAAA,EACF;AAAA,EAEA,WAAuB;AACrB,WAAO;AAAA,MACL,KAAK,KAAK;AAAA,MACV,KAAK,KAAK;AAAA,MACV,aAAa,KAAK,OAAO;AAAA,MACzB,WAAW,KAAK;AAAA,MAChB,cAAc,KAAK;AAAA,IACrB;AAAA,EACF;AAAA,EAEA,UAAqB;AACnB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,aAAsB;AACpB,WAAO,KAAK,YAAY;AAAA,EAC1B;AACF;AAKO,SAAS,YAAY,SAA+B;AACzD,MAAI,QAAQ,WAAW,GAAG;AACxB,WAAO,EAAE,QAAQ,CAAC,GAAG,cAAc,KAAK;AAAA,EAC1C;AAGA,QAAM,aAAa,oBAAI,IAAY;AACnC,aAAW,UAAU,SAAS;AAC5B,eAAW,OAAO,OAAO,KAAK,MAAM,GAAG;AACrC,iBAAW,IAAI,GAAG;AAAA,IACpB;AAAA,EACF;AAGA,QAAM,aAAa,oBAAI,IAAiC;AACxD,aAAW,QAAQ,YAAY;AAC7B,eAAW,IAAI,MAAM,IAAI,oBAAoB,CAAC;AAAA,EAChD;AAEA,aAAW,UAAU,SAAS;AAC5B,eAAW,QAAQ,YAAY;AAC7B,YAAM,YAAY,WAAW,IAAI,IAAI;AACrC,gBAAU,IAAI,OAAO,IAAI,CAAC;AAAA,IAC5B;AAAA,EACF;AAGA,QAAM,SAAwB,CAAC;AAC/B,MAAI,eAA8B;AAElC,aAAW,QAAQ,YAAY;AAC7B,UAAM,YAAY,WAAW,IAAI,IAAI;AACrC,UAAM,QAAQ,UAAU,SAAS;AACjC,UAAM,OAAO,UAAU,QAAQ;AAI/B,UAAM,cAAc,MAAM;AAC1B,UAAM,mBAAmB,eAAe,OAAQ,cAAc,QAAQ,SAAS;AAC/E,UAAM,UAAU;AAIhB,QACE,iBAAiB,QACjB,MAAM,gBAAgB,QAAQ,UAC9B,CAAC,UAAU,WAAW,MACrB,KAAK,YAAY,EAAE,SAAS,IAAI,KAAK,KAAK,YAAY,MAAM,QAC7D;AACA,qBAAe;AAAA,IACjB;AAEA,WAAO,KAAK;AAAA,MACV;AAAA,MACA;AAAA,MACA,UAAU,UAAU,WAAW;AAAA,MAC/B;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH;AAGA,SAAO,KAAK,CAAC,GAAG,MAAM;AACpB,QAAI,EAAE,SAAS,aAAc,QAAO;AACpC,QAAI,EAAE,SAAS,aAAc,QAAO;AACpC,WAAO,EAAE,KAAK,cAAc,EAAE,IAAI;AAAA,EACpC,CAAC;AAED,SAAO,EAAE,QAAQ,aAAa;AAChC;AAKO,SAAS,kBAAkB,QAAgB,SAAsC;AAEtF,MAAI,OAAO,cAAc;AACvB,WAAO,OAAO;AAAA,EAChB;AAMA,MAAI,YAA2B;AAC/B,MAAI,YAAY;AAEhB,aAAW,SAAS,OAAO,QAAQ;AACjC,QAAI,MAAM,SAAU;AAEpB,QAAI,QAAQ;AAGZ,QAAI,MAAM,SAAS,YAAY,MAAM,SAAS,QAAQ;AACpD,eAAS;AAAA,IACX;AAGA,UAAM,mBAAmB,MAAM,MAAM,cAAc,QAAQ;AAC3D,QAAI,mBAAmB,OAAO,oBAAoB,GAAG;AACnD,eAAS,mBAAmB;AAAA,IAC9B;AAGA,UAAM,YAAY,MAAM,KAAK,YAAY;AACzC,QAAI,UAAU,SAAS,IAAI,EAAG,UAAS;AACvC,QAAI,UAAU,SAAS,MAAM,KAAK,UAAU,SAAS,MAAM,EAAG,UAAS;AACvE,QAAI,UAAU,SAAS,SAAS,KAAK,UAAU,SAAS,SAAS,EAAG,UAAS;AAE7E,QAAI,QAAQ,WAAW;AACrB,kBAAY;AACZ,kBAAY,MAAM;AAAA,IACpB;AAAA,EACF;AAEA,SAAO;AACT;AAKO,SAAS,mBAAmB,QAA0B;AAC3D,SAAO,OAAO,OACX,OAAO,CAAC,MAAM,EAAE,OAAO,EACvB,IAAI,CAAC,MAAM,EAAE,IAAI;AACtB;;;AC/OA,SAAS,mBAAmB,QAA4B;AACtD,SAAO,KAAK,UAAU,MAAM,EAAE;AAChC;AAKA,SAAS,cAAc,GAAY,GAAoB;AACrD,MAAI,MAAM,EAAG,QAAO;AACpB,MAAI,MAAM,QAAQ,MAAM,OAAW,QAAO;AAC1C,MAAI,MAAM,QAAQ,MAAM,OAAW,QAAO;AAE1C,MAAI,OAAO,MAAM,YAAY,OAAO,MAAM,UAAU;AAClD,WAAO,IAAI;AAAA,EACb;AAEA,SAAO,OAAO,CAAC,EAAE,cAAc,OAAO,CAAC,CAAC;AAC1C;AAKO,SAAS,aACd,SACA,QACA,SACS;AACT,QAAM,EAAE,YAAY,QAAQ,IAAI;AAGhC,MAAI,gBAAgB;AACpB,MAAI,SAAS;AACX,oBAAgB,CAAC,GAAG,OAAO,EAAE;AAAA,MAAK,CAAC,GAAG,MACpC,cAAc,EAAE,OAAO,GAAG,EAAE,OAAO,CAAC;AAAA,IACtC;AAAA,EACF;AAEA,QAAM,SAAkB,CAAC;AACzB,MAAI,eAA6B,CAAC;AAClC,MAAI,cAAc;AAClB,MAAI,aAAa;AAGjB,QAAM,gBAAgB;AACtB,QAAM,gBAAgB;AAEtB,aAAW,UAAU,eAAe;AAClC,UAAM,aAAa,mBAAmB,MAAM,IAAI;AAIhD,QAAI,cAAc,aAAa,cAAc,aAAa,SAAS,GAAG;AACpE,aAAO,KAAK;AAAA,QACV,IAAI,OAAO,UAAU;AAAA,QACrB,SAAS;AAAA,QACT,UAAU,cAAc;AAAA,MAC1B,CAAC;AACD;AACA,qBAAe,CAAC;AAChB,oBAAc;AAAA,IAChB;AAEA,iBAAa,KAAK,MAAM;AACxB,mBAAe;AAAA,EACjB;AAGA,MAAI,aAAa,SAAS,GAAG;AAC3B,WAAO,KAAK;AAAA,MACV,IAAI,OAAO,UAAU;AAAA,MACrB,SAAS;AAAA,MACT,UAAU,cAAc;AAAA,IAC1B,CAAC;AAAA,EACH;AAEA,SAAO;AACT;AAKO,SAAS,qBACd,SACA,QAC0B;AAC1B,QAAM,SAAmC,CAAC;AAE1C,aAAW,SAAS,OAAO,QAAQ;AACjC,UAAM,SAAS,QACZ,IAAI,CAAC,MAAM,EAAE,MAAM,IAAI,CAAC,EACxB,OAAO,CAAC,MAAM,MAAM,QAAQ,MAAM,MAAS;AAE9C,QAAI,OAAO,WAAW,GAAG;AACvB;AAAA,IACF;AAGA,QAAI,MAAM,SAAS,YAAY,MAAM,SAAS,UAAU,MAAM,SAAS,UAAU;AAC/E,YAAM,SAAS,CAAC,GAAG,MAAM,EAAE,KAAK,aAAa;AAC7C,aAAO,MAAM,IAAI,IAAI;AAAA,QACnB,KAAK,OAAO,CAAC;AAAA,QACb,KAAK,OAAO,OAAO,SAAS,CAAC;AAAA,MAC/B;AAAA,IACF;AAAA,EACF;AAEA,SAAO;AACT;AAKO,SAAS,kBACd,OACA,QACA,UACW;AACX,SAAO;AAAA,IACL,IAAI,MAAM;AAAA,IACV,MAAM,GAAG,QAAQ,IAAI,MAAM,EAAE;AAAA,IAC7B,OAAO,MAAM,QAAQ;AAAA,IACrB,UAAU,MAAM;AAAA,IAChB,aAAa,qBAAqB,MAAM,SAAS,MAAM;AAAA,EACzD;AACF;AAKO,SAAS,eAAe,OAAsB;AACnD,SAAO,KAAK,UAAU,MAAM,OAAO;AACrC;;;ACvIO,SAAS,qBACd,QACA,QACA,eACe;AACf,QAAM,UAAyB,CAAC;AAGhC,QAAM,gBAAgB,gBAClB,OAAO,OAAO,OAAO,CAAC,MAAM,cAAc,SAAS,EAAE,IAAI,CAAC,IAC1D,OAAO,OAAO,OAAO,CAAC,MAAM,EAAE,OAAO;AAEzC,aAAW,SAAS,eAAe;AACjC,UAAM,aAAuC,CAAC;AAE9C,eAAW,SAAS,QAAQ;AAC1B,YAAM,gBAAgB,oBAAI,IAAY;AAEtC,iBAAW,UAAU,MAAM,SAAS;AAClC,cAAM,QAAQ,OAAO,MAAM,IAAI;AAC/B,YAAI,UAAU,QAAQ,UAAU,QAAW;AACzC,wBAAc,IAAI,OAAO,KAAK,CAAC;AAAA,QACjC;AAAA,MACF;AAGA,iBAAW,SAAS,eAAe;AACjC,YAAI,CAAC,WAAW,KAAK,GAAG;AACtB,qBAAW,KAAK,IAAI,CAAC;AAAA,QACvB;AACA,mBAAW,KAAK,EAAE,KAAK,MAAM,EAAE;AAAA,MACjC;AAAA,IACF;AAIA,UAAM,eAAe,OAAO,KAAK,UAAU,EAAE;AAC7C,QAAI,gBAAgB,KAAO;AACzB,cAAQ,MAAM,IAAI,IAAI;AAAA,IACxB;AAAA,EACF;AAEA,SAAO;AACT;;;AClDA,SAAS,cAAc,OAA4B;AACjD,MAAI;AAEJ,UAAQ,MAAM,MAAM;AAAA,IAClB,KAAK;AAAA,IACL,KAAK;AACH,eAAS;AACT;AAAA,IACF,KAAK;AACH,eAAS;AACT;AAAA,IACF,KAAK;AACH,eAAS;AACT;AAAA,IACF,KAAK;AACH,eAAS;AACT;AAAA,IACF;AACE,eAAS;AAAA,EACb;AAEA,MAAI,MAAM,UAAU;AAClB,cAAU;AAAA,EACZ;AAEA,SAAO;AACT;AAKA,SAAS,wBAAwB,QAAwB;AACvD,QAAM,SAAS,OAAO,OACnB,IAAI,CAAC,MAAM,KAAK,EAAE,IAAI,KAAK,cAAc,CAAC,CAAC,GAAG,EAC9C,KAAK,IAAI;AAEZ,SAAO;AAAA,EACP,MAAM;AAAA;AAER;AAKA,SAAS,uBAAuB,QAAwB;AACtD,QAAM,QAAQ,OAAO,OAAO,IAAI,CAAC,MAAM,IAAI,EAAE,IAAI,GAAG,EAAE,KAAK,KAAK;AAChE,SAAO,2BAA2B,KAAK;AACzC;AAKA,SAAS,mBAAmB,QAAwB;AAClD,QAAM,eAAe,OAAO,OACzB,OAAO,CAAC,MAAM,EAAE,SAAS,YAAY,EAAE,SAAS,MAAM,EACtD,IAAI,CAAC,MAAM,EAAE,IAAI;AAEpB,QAAM,gBAAgB,OAAO,OAC1B,OAAO,CAAC,MAAM,EAAE,SAAS,QAAQ,EACjC,IAAI,CAAC,MAAM,EAAE,IAAI;AAEpB,SAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAqBP,OAAO,OACN,IAAI,CAAC,MAAM;AACV,QAAI,EAAE,SAAS,UAAU;AACvB,aAAO,KAAK,EAAE,IAAI;AAAA,IACpB,WAAW,EAAE,SAAS,WAAW;AAC/B,aAAO,KAAK,EAAE,IAAI;AAAA,IACpB,OAAO;AACL,aAAO,KAAK,EAAE,IAAI;AAAA,IACpB;AAAA,EACF,CAAC,EACA,KAAK,IAAI,CAAC;AAAA;AAEb;AAKO,SAAS,eAAe,QAAgB,UAA4B;AACzE,QAAM,kBAAkB,wBAAwB,MAAM;AACtD,QAAM,iBAAiB,uBAAuB,MAAM;AACpD,QAAM,aAAa,mBAAmB,MAAM;AAE5C,QAAM,iBAAiB,OAAO,OAC3B,OAAO,CAAC,MAAM,EAAE,SAAS,YAAY,EAAE,SAAS,YAAY,EAAE,SAAS,MAAM,EAC7E,IAAI,CAAC,MAAM,IAAI,EAAE,IAAI,GAAG,EACxB,KAAK,KAAK;AAEb,SAAO;AAAA;AAAA,mBAEU,SAAS,WAAW;AAAA,oBACnB,SAAS,YAAY;AAAA,aAC5B,SAAS,OAAO,MAAM;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOjC,eAAe;AAAA;AAAA,EAEf,cAAc;AAAA;AAAA,EAEd,UAAU;AAAA;AAAA,8BAEkB,kBAAkB,QAAQ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AA8SxD;;;ALxaA,IAAM,UAAU;AAShB,eAAsB,MACpB,WACA,SACsB;AACtB,QAAM,YAAY,KAAK,IAAI;AAG3B,MAAI,CAAI,eAAW,SAAS,GAAG;AAC7B,UAAM,IAAI,MAAM,yBAAyB,SAAS,EAAE;AAAA,EACtD;AAEA,UAAQ,IAAI,WAAW,SAAS,KAAK;AAGrC,QAAM,EAAE,SAAS,OAAO,IAAI,MAAM,UAAU,WAAW,QAAQ,MAAM;AACrE,UAAQ,IAAI,UAAU,QAAQ,MAAM,qBAAqB,MAAM,GAAG;AAElE,MAAI,QAAQ,WAAW,GAAG;AACxB,UAAM,IAAI,MAAM,gCAAgC;AAAA,EAClD;AAGA,UAAQ,IAAI,qBAAqB;AACjC,QAAM,SAAS,YAAY,OAAO;AAClC,UAAQ,IAAI,SAAS,OAAO,OAAO,MAAM,SAAS;AAElD,MAAI,OAAO,cAAc;AACvB,YAAQ,IAAI,2BAA2B,OAAO,YAAY,EAAE;AAAA,EAC9D;AAGA,QAAM,UAAU,QAAQ,WAAW,kBAAkB,QAAQ,OAAO;AACpE,MAAI,SAAS;AACX,YAAQ,IAAI,sBAAsB,OAAO,EAAE;AAAA,EAC7C;AAGA,QAAM,kBAAkB,UAAU,QAAQ,SAAS;AACnD,UAAQ,IAAI,sBAAsB,YAAY,eAAe,CAAC,EAAE;AAGhE,QAAM,gBAAgB,QAAQ,QAC1B,QAAQ,MAAM,MAAM,GAAG,EAAE,IAAI,CAAC,MAAM,EAAE,KAAK,CAAC,IAC5C,OAAO,OAAO,OAAO,CAAC,MAAM,EAAE,OAAO,EAAE,IAAI,CAAC,MAAM,EAAE,IAAI;AAE5D,MAAI,cAAc,SAAS,GAAG;AAC5B,YAAQ,IAAI,oBAAoB,cAAc,KAAK,IAAI,CAAC,EAAE;AAAA,EAC5D;AAGA,aAAW,SAAS,OAAO,QAAQ;AACjC,UAAM,UAAU,cAAc,SAAS,MAAM,IAAI;AAAA,EACnD;AAGA,UAAQ,IAAI,kBAAkB;AAC9B,QAAM,SAAS,aAAa,SAAS,QAAQ;AAAA,IAC3C,YAAY;AAAA,IACZ;AAAA,EACF,CAAC;AACD,UAAQ,IAAI,WAAW,OAAO,MAAM,SAAS;AAG7C,UAAQ,IAAI,qBAAqB;AACjC,QAAM,UAAU,qBAAqB,QAAQ,QAAQ,aAAa;AAClE,UAAQ,IAAI,qBAAqB,OAAO,KAAK,OAAO,EAAE,MAAM,SAAS;AAGrE,QAAM,YAAiB,aAAQ,QAAQ,MAAM;AAC7C,QAAM,YAAiB,UAAK,WAAW,QAAQ;AAC/C,QAAS,aAAS,MAAM,WAAW,EAAE,WAAW,KAAK,CAAC;AAGtD,UAAQ,IAAI,mBAAmB;AAC/B,QAAM,aAAa,CAAC;AACpB,aAAW,SAAS,QAAQ;AAC1B,UAAM,YAAiB,UAAK,WAAW,GAAG,MAAM,EAAE,OAAO;AACzD,UAAS,aAAS,UAAU,WAAW,eAAe,KAAK,CAAC;AAC5D,eAAW,KAAK,kBAAkB,OAAO,QAAQ,QAAQ,CAAC;AAAA,EAC5D;AAGA,QAAM,SAAsB;AAAA,IAC1B,WAAW;AAAA,IACX,SAAS,WAAW;AAAA,IACpB;AAAA,EACF;AAGA,QAAM,WAAqB;AAAA,IACzB,SAAS;AAAA,IACT,cAAa,oBAAI,KAAK,GAAE,YAAY;AAAA,IACpC;AAAA,IACA,QAAQ;AAAA,IACR;AAAA,IACA,cAAc,QAAQ;AAAA,IACtB;AAAA,EACF;AAGA,QAAM,eAAoB,UAAK,WAAW,eAAe;AACzD,QAAS,aAAS,UAAU,cAAc,KAAK,UAAU,UAAU,MAAM,CAAC,CAAC;AAC3E,UAAQ,IAAI,qBAAqB,YAAY,EAAE;AAG/C,UAAQ,IAAI,sBAAsB;AAClC,QAAM,aAAa,eAAe,QAAQ,QAAQ;AAClD,QAAM,aAAkB,UAAK,WAAW,WAAW;AACnD,QAAS,aAAS,UAAU,YAAY,UAAU;AAClD,UAAQ,IAAI,mBAAmB,UAAU,EAAE;AAE3C,QAAM,YAAY,KAAK,IAAI,IAAI,aAAa,KAAM,QAAQ,CAAC;AAC3D,UAAQ,IAAI;AAAA,qBAAwB,OAAO,GAAG;AAC9C,UAAQ,IAAI,WAAW,SAAS,EAAE;AAClC,UAAQ,IAAI,OAAO,OAAO,MAAM,SAAS;AACzC,UAAQ,IAAI,OAAO,QAAQ,MAAM,gBAAgB;AACjD,UAAQ,IAAI,mBAAmB;AAC/B,UAAQ,IAAI,eAAe;AAE3B,SAAO;AAAA,IACL;AAAA,IACA;AAAA,IACA,YAAY,OAAO;AAAA,IACnB,cAAc,QAAQ;AAAA,EACxB;AACF;AAEA,SAAS,YAAY,OAAuB;AAC1C,MAAI,QAAQ,KAAM,QAAO,GAAG,KAAK;AACjC,MAAI,QAAQ,OAAO,KAAM,QAAO,IAAI,QAAQ,MAAM,QAAQ,CAAC,CAAC;AAC5D,MAAI,QAAQ,OAAO,OAAO,KAAM,QAAO,IAAI,SAAS,OAAO,OAAO,QAAQ,CAAC,CAAC;AAC5E,SAAO,IAAI,SAAS,OAAO,OAAO,OAAO,QAAQ,CAAC,CAAC;AACrD;;;AMtJA,YAAYC,SAAQ;AAKpB,eAAsB,QACpB,WACA,SACe;AAEf,MAAI,CAAI,eAAW,SAAS,GAAG;AAC7B,UAAM,IAAI,MAAM,yBAAyB,SAAS,EAAE;AAAA,EACtD;AAEA,QAAM,WAAW,YAAY,SAAS;AACtC,UAAQ,IAAI;AAAA,QAAW,SAAS,EAAE;AAClC,UAAQ,IAAI,SAASC,aAAY,QAAQ,CAAC,EAAE;AAE5C,UAAQ,IAAI,mBAAmB;AAG/B,QAAM,EAAE,SAAS,OAAO,IAAI,MAAM,UAAU,WAAW,QAAQ,MAAM;AAErE,UAAQ,IAAI,WAAW,MAAM,EAAE;AAC/B,UAAQ,IAAI,kBAAkB,QAAQ,MAAM,EAAE;AAE9C,MAAI,QAAQ,WAAW,GAAG;AACxB,YAAQ,IAAI,qBAAqB;AACjC;AAAA,EACF;AAGA,QAAM,aAAa,KAAK,IAAI,QAAQ,UAAU,KAAM,QAAQ,MAAM;AAClE,QAAM,gBAAgB,QAAQ,MAAM,GAAG,UAAU;AAEjD,UAAQ,IAAI;AAAA,YAAe,UAAU,aAAa;AAGlD,QAAM,SAAS,YAAY,aAAa;AAExC,UAAQ,IAAI,OAAO,IAAI,OAAO,EAAE,CAAC;AACjC,UAAQ,IAAI,QAAQ;AACpB,UAAQ,IAAI,IAAI,OAAO,EAAE,CAAC;AAE1B,UAAQ,IAAI;AAAA,UAAa,OAAO,OAAO,MAAM;AAAA,CAAM;AAEnD,aAAW,SAAS,OAAO,QAAQ;AACjC,UAAM,YAAY,MAAM,SAAS,OAAO,eAAe,eAAe;AACtE,UAAM,YAAY,MAAM,UAAU,eAAe;AACjD,UAAM,WAAW,MAAM,WAAW,gBAAgB;AAElD,YAAQ,IAAI,KAAK,MAAM,IAAI,KAAK,MAAM,IAAI,GAAG,QAAQ,GAAG,SAAS,GAAG,SAAS,EAAE;AAG/E,UAAM,QAAQ,MAAM;AACpB,UAAM,YAAY,CAAC;AAEnB,QAAI,MAAM,gBAAgB,QAAW;AACnC,YAAM,kBAAmB,MAAM,cAAc,QAAQ,SAAU,KAAK,QAAQ,CAAC;AAC7E,gBAAU,KAAK,gBAAgB,MAAM,WAAW,KAAK,cAAc,IAAI;AAAA,IACzE;AAEA,QAAI,MAAM,QAAQ,UAAa,MAAM,QAAQ,QAAW;AACtD,gBAAU,KAAK,UAAU,YAAY,MAAM,GAAG,CAAC,MAAM,YAAY,MAAM,GAAG,CAAC,EAAE;AAAA,IAC/E;AAEA,QAAI,MAAM,YAAY,GAAG;AACvB,gBAAU,KAAK,UAAU,MAAM,SAAS,EAAE;AAAA,IAC5C;AAEA,QAAI,UAAU,SAAS,GAAG;AACxB,cAAQ,IAAI,OAAO,UAAU,KAAK,IAAI,CAAC,EAAE;AAAA,IAC3C;AAEA,QAAI,MAAM,gBAAgB,MAAM,aAAa,SAAS,GAAG;AACvD,YAAM,UAAU,MAAM,aAAa,MAAM,GAAG,CAAC,EAAE,IAAI,WAAW,EAAE,KAAK,IAAI;AACzE,cAAQ,IAAI,iBAAiB,OAAO,EAAE;AAAA,IACxC;AAAA,EACF;AAEA,UAAQ,IAAI,OAAO,IAAI,OAAO,EAAE,CAAC;AACjC,UAAQ,IAAI,iBAAiB;AAC7B,UAAQ,IAAI,IAAI,OAAO,EAAE,CAAC;AAG1B,QAAM,sBAAsB,kBAAkB,QAAQ,OAAO;AAC7D,MAAI,qBAAqB;AACvB,YAAQ,IAAI;AAAA,0BAA6B,mBAAmB,EAAE;AAAA,EAChE,OAAO;AACL,YAAQ,IAAI,oEAAoE;AAAA,EAClF;AAGA,QAAM,kBAAkB,mBAAmB,MAAM;AACjD,MAAI,gBAAgB,SAAS,GAAG;AAC9B,YAAQ,IAAI,wBAAwB,gBAAgB,KAAK,GAAG,CAAC,EAAE;AAAA,EACjE,OAAO;AACL,YAAQ,IAAI,oCAAoC;AAAA,EAClD;AAGA,QAAM,gBAAgB,WAAW,QAAQ;AACzC,QAAM,kBAAkB,IAAI,OAAO;AACnC,QAAM,kBAAkB,KAAK,KAAK,WAAW,eAAe;AAE5D,UAAQ,IAAI,OAAO,IAAI,OAAO,EAAE,CAAC;AACjC,UAAQ,IAAI,gBAAgB;AAC5B,UAAQ,IAAI,IAAI,OAAO,EAAE,CAAC;AAE1B,UAAQ,IAAI;AAAA,uBAA0BA,aAAY,aAAa,CAAC,EAAE;AAClE,UAAQ,IAAI;AAAA,yBAA4B;AACxC,UAAQ,IAAI,uBAAuB,eAAe,EAAE;AACpD,UAAQ,IAAI,yBAAyB,KAAK,KAAK,QAAQ,SAAS,eAAe,CAAC,EAAE;AAGlF,UAAQ,IAAI,OAAO,IAAI,OAAO,EAAE,CAAC;AACjC,UAAQ,IAAI,iBAAiB;AAC7B,UAAQ,IAAI,IAAI,OAAO,EAAE,CAAC;AAE1B,MAAI,MAAM,0BAA0B,SAAS;AAC7C,MAAI,qBAAqB;AACvB,WAAO,eAAe,mBAAmB;AAAA,EAC3C;AACA,MAAI,gBAAgB,SAAS,GAAG;AAC9B,WAAO,aAAa,gBAAgB,KAAK,GAAG,CAAC;AAAA,EAC/C;AAEA,UAAQ,IAAI;AAAA,EAAK,GAAG;AAAA,CAAI;AAC1B;AAEA,SAASA,aAAY,OAAuB;AAC1C,MAAI,QAAQ,KAAM,QAAO,GAAG,KAAK;AACjC,MAAI,QAAQ,OAAO,KAAM,QAAO,IAAI,QAAQ,MAAM,QAAQ,CAAC,CAAC;AAC5D,MAAI,QAAQ,OAAO,OAAO,KAAM,QAAO,IAAI,SAAS,OAAO,OAAO,QAAQ,CAAC,CAAC;AAC5E,SAAO,IAAI,SAAS,OAAO,OAAO,OAAO,QAAQ,CAAC,CAAC;AACrD;AAEA,SAAS,YAAY,OAAwB;AAC3C,MAAI,UAAU,KAAM,QAAO;AAC3B,MAAI,UAAU,OAAW,QAAO;AAChC,MAAI,OAAO,UAAU,UAAU;AAC7B,QAAI,MAAM,SAAS,IAAI;AACrB,aAAO,IAAI,MAAM,MAAM,GAAG,EAAE,CAAC;AAAA,IAC/B;AACA,WAAO,IAAI,KAAK;AAAA,EAClB;AACA,SAAO,OAAO,KAAK;AACrB;;;AP7IA,IAAM,UAAU,IAAI,QAAQ;AAE5B,QACG,KAAK,cAAc,EACnB,YAAY,uEAAuE,EACnF,QAAQ,OAAO;AAElB,QACG,QAAQ,OAAO,EACf,YAAY,0CAA0C,EACtD,SAAS,WAAW,wCAAwC,EAC5D,eAAe,sBAAsB,kBAAkB,EACvD,OAAO,2BAA2B,iCAAiC,KAAK,EACxE,OAAO,0BAA0B,4BAA4B,EAC7D,OAAO,wBAAwB,iCAAiC,EAChE,OAAO,yBAAyB,kCAAkC,EAClE,OAAO,OAAO,OAAO,YAAY;AAChC,MAAI;AACF,UAAM,MAAM,OAAO;AAAA,MACjB,QAAQ,QAAQ;AAAA,MAChB,WAAW,QAAQ;AAAA,MACnB,SAAS,QAAQ;AAAA,MACjB,OAAO,QAAQ;AAAA,MACf,QAAQ,QAAQ;AAAA,IAClB,CAAC;AAAA,EACH,SAAS,OAAO;AACd,YAAQ,MAAM,UAAW,MAAgB,OAAO;AAChD,YAAQ,KAAK,CAAC;AAAA,EAChB;AACF,CAAC;AAEH,QACG,QAAQ,SAAS,EACjB,YAAY,mDAAmD,EAC/D,SAAS,WAAW,iBAAiB,EACrC,OAAO,wBAAwB,+BAA+B,MAAM,EACpE,OAAO,yBAAyB,kCAAkC,EAClE,OAAO,OAAO,OAAO,YAAY;AAChC,MAAI;AACF,UAAM,QAAQ,OAAO;AAAA,MACnB,QAAQ,SAAS,QAAQ,QAAQ,EAAE;AAAA,MACnC,QAAQ,QAAQ;AAAA,IAClB,CAAC;AAAA,EACH,SAAS,OAAO;AACd,YAAQ,MAAM,UAAW,MAAgB,OAAO;AAChD,YAAQ,KAAK,CAAC;AAAA,EAChB;AACF,CAAC;AAEH,QAAQ,MAAM;","names":["fs","resolve","fs","formatBytes"]}