{"version":3,"sources":["../../src/cli/index.ts","../../src/cli/commands/build.ts","../../src/cli/utils/parsers.ts","../../src/cli/utils/schema.ts","../../src/cli/utils/chunker.ts","../../src/cli/utils/indexer.ts","../../src/cli/utils/codegen.ts","../../src/cli/commands/inspect.ts","../../src/cli/commands/types.ts"],"sourcesContent":["#!/usr/bin/env node\n\n/**\n * static-shard CLI\n */\n\nimport { Command } from \"commander\";\nimport { build } from \"./commands/build.js\";\nimport { inspect } from \"./commands/inspect.js\";\nimport { types } from \"./commands/types.js\";\n\nconst program = new Command();\n\nprogram\n  .name(\"static-shard\")\n  .description(\"Query large static datasets efficiently by splitting them into chunks\")\n  .version(\"0.1.0\");\n\nprogram\n  .command(\"build\")\n  .description(\"Build chunks and client from a data file\")\n  .argument(\"<input>\", \"Input data file (JSON, NDJSON, or CSV)\")\n  .requiredOption(\"-o, --output <dir>\", \"Output directory\")\n  .option(\"-s, --chunk-size <size>\", \"Target chunk size (e.g., 5mb)\", \"5mb\")\n  .option(\"-c, --chunk-by <field>\", \"Field to sort and chunk by\")\n  .option(\"-i, --index <fields>\", \"Comma-separated fields to index\")\n  .option(\"-f, --format <format>\", \"Input format (json, ndjson, csv)\")\n  .action(async (input, options) => {\n    try {\n      await build(input, {\n        output: options.output,\n        chunkSize: options.chunkSize,\n        chunkBy: options.chunkBy,\n        index: options.index,\n        format: options.format,\n      });\n    } catch (error) {\n      console.error(\"Error:\", (error as Error).message);\n      process.exit(1);\n    }\n  });\n\nprogram\n  .command(\"inspect\")\n  .description(\"Analyze a data file and suggest chunking strategy\")\n  .argument(\"<input>\", \"Input data file\")\n  .option(\"-n, --sample <count>\", \"Number of records to sample\", \"1000\")\n  .option(\"-f, --format <format>\", \"Input format (json, ndjson, csv)\")\n  .option(\"--fast\", \"Fast mode: estimate record count instead of reading entire file\")\n  .action(async (input, options) => {\n    try {\n      await inspect(input, {\n        sample: parseInt(options.sample, 10),\n        format: options.format,\n        fast: options.fast,\n      });\n    } catch (error) {\n      console.error(\"Error:\", (error as Error).message);\n      process.exit(1);\n    }\n  });\n\nprogram\n  .command(\"types\")\n  .description(\"Generate TypeScript types from a data file\")\n  .argument(\"<input>\", \"Input data file (JSON, NDJSON, or CSV)\")\n  .option(\"-n, --sample <count>\", \"Number of records to sample\", \"1000\")\n  .option(\"-f, --format <format>\", \"Input format (json, ndjson, csv)\")\n  .option(\"-o, --output <file>\", \"Output file (default: stdout)\")\n  .action(async (input, options) => {\n    try {\n      await types(input, {\n        sample: parseInt(options.sample, 10),\n        format: options.format,\n        output: options.output,\n      });\n    } catch (error) {\n      console.error(\"Error:\", (error as Error).message);\n      process.exit(1);\n    }\n  });\n\nprogram.parse();\n","/**\n * Build command - process data file and generate output\n * Supports streaming for large files (2GB+)\n */\n\nimport * as fs from \"node:fs\";\nimport * as path from \"node:path\";\nimport type { BuildConfig, BuildOptions, Manifest, DataRecord, ChunkMeta, Schema } from \"../../types/index.js\";\nimport { parseFile, parseSize, streamFile, getFileSize } from \"../utils/parsers.js\";\nimport { inferSchema, suggestChunkField } from \"../utils/schema.js\";\nimport { chunkRecords, generateChunkMeta, serializeChunk, calculateFieldRanges, type Chunk } from \"../utils/chunker.js\";\nimport { buildInvertedIndices } from \"../utils/indexer.js\";\nimport { generateClient } from \"../utils/codegen.js\";\n\nconst VERSION = \"1.0.0\";\n\n// Use streaming for files larger than 100MB\nconst STREAMING_THRESHOLD = 100 * 1024 * 1024;\n\nexport interface BuildResult {\n  manifest: Manifest;\n  outputDir: string;\n  chunkCount: number;\n  totalRecords: number;\n}\n\nexport async function build(\n  inputFile: string,\n  options: BuildOptions\n): Promise<BuildResult> {\n  const startTime = Date.now();\n\n  // Validate input file exists\n  if (!fs.existsSync(inputFile)) {\n    throw new Error(`Input file not found: ${inputFile}`);\n  }\n\n  const fileSize = getFileSize(inputFile);\n  console.log(`Input: ${inputFile} (${formatBytes(fileSize)})`);\n\n  // Use streaming for large files\n  if (fileSize > STREAMING_THRESHOLD) {\n    console.log(\"Using streaming mode for large file...\");\n    return buildStreaming(inputFile, options, startTime);\n  }\n\n  return buildInMemory(inputFile, options, startTime);\n}\n\n/**\n * Standard in-memory build for smaller files\n */\nasync function buildInMemory(\n  inputFile: string,\n  options: BuildOptions,\n  startTime: number\n): Promise<BuildResult> {\n  console.log(\"Reading file...\");\n\n  // Parse input file\n  const { records, format } = await parseFile(inputFile, options.format);\n  console.log(`Parsed ${records.length.toLocaleString()} records (format: ${format})`);\n\n  if (records.length === 0) {\n    throw new Error(\"No records found in input file\");\n  }\n\n  // Infer schema\n  console.log(\"Inferring schema...\");\n  const schema = inferSchema(records);\n  console.log(`Found ${schema.fields.length} fields`);\n\n  if (schema.primaryField) {\n    console.log(`Detected primary field: ${schema.primaryField}`);\n  }\n\n  // Determine chunk field\n  const chunkBy = options.chunkBy || suggestChunkField(schema, records);\n  if (chunkBy) {\n    console.log(`Chunking by field: ${chunkBy}`);\n  }\n\n  // Parse chunk size\n  const targetChunkSize = parseSize(options.chunkSize);\n  console.log(`Target chunk size: ${formatBytes(targetChunkSize)}`);\n\n  // Parse indexed fields (limit auto-detected to 10 to prevent memory issues)\n  const MAX_AUTO_INDEX_FIELDS = 10;\n  let indexedFields: string[];\n  if (options.index) {\n    indexedFields = options.index.split(\",\").map((f) => f.trim());\n  } else {\n    const autoIndexed = schema.fields.filter((f) => f.indexed).map((f) => f.name);\n    if (autoIndexed.length > MAX_AUTO_INDEX_FIELDS) {\n      console.log(`Warning: ${autoIndexed.length} indexable fields detected, limiting to ${MAX_AUTO_INDEX_FIELDS}`);\n      console.log(`Use --index to specify which fields to index`);\n      indexedFields = autoIndexed.slice(0, MAX_AUTO_INDEX_FIELDS);\n    } else {\n      indexedFields = autoIndexed;\n    }\n  }\n\n  if (indexedFields.length > 0) {\n    console.log(`Indexing fields: ${indexedFields.join(\", \")}`);\n  }\n\n  // Update schema with indexed fields\n  for (const field of schema.fields) {\n    field.indexed = indexedFields.includes(field.name);\n  }\n\n  // Chunk the data\n  console.log(\"Chunking data...\");\n  const chunks = chunkRecords(records, schema, {\n    targetSize: targetChunkSize,\n    chunkBy,\n  });\n  console.log(`Created ${chunks.length} chunks`);\n\n  // Build indices\n  console.log(\"Building indices...\");\n  const indices = buildInvertedIndices(chunks, schema, indexedFields);\n  console.log(`Built indices for ${Object.keys(indices).length} fields`);\n\n  // Create output directory\n  const outputDir = path.resolve(options.output);\n  const chunksDir = path.join(outputDir, \"chunks\");\n  await fs.promises.mkdir(chunksDir, { recursive: true });\n\n  // Write chunks\n  console.log(\"Writing chunks...\");\n  const chunkMetas = [];\n  for (const chunk of chunks) {\n    const chunkPath = path.join(chunksDir, `${chunk.id}.json`);\n    await fs.promises.writeFile(chunkPath, serializeChunk(chunk));\n    chunkMetas.push(generateChunkMeta(chunk, schema, \"chunks\"));\n  }\n\n  // Build config\n  const config: BuildConfig = {\n    chunkSize: targetChunkSize,\n    chunkBy: chunkBy || null,\n    indexedFields,\n  };\n\n  // Build manifest\n  const manifest: Manifest = {\n    version: VERSION,\n    generatedAt: new Date().toISOString(),\n    schema,\n    chunks: chunkMetas,\n    indices,\n    totalRecords: records.length,\n    config,\n  };\n\n  // Write manifest\n  const manifestPath = path.join(outputDir, \"manifest.json\");\n  await fs.promises.writeFile(manifestPath, JSON.stringify(manifest, null, 2));\n  console.log(`Wrote manifest to ${manifestPath}`);\n\n  // Generate client (use small sample for type inference - 100 is enough)\n  console.log(\"Generating client...\");\n  const sampleForTypes = records.slice(0, 100);\n  const clientCode = generateClient(schema, manifest, sampleForTypes);\n  const clientPath = path.join(outputDir, \"client.ts\");\n  await fs.promises.writeFile(clientPath, clientCode);\n  console.log(`Wrote client to ${clientPath}`);\n\n  const elapsed = ((Date.now() - startTime) / 1000).toFixed(2);\n  console.log(`\\nBuild completed in ${elapsed}s`);\n  console.log(`Output: ${outputDir}`);\n  console.log(`  - ${chunks.length} chunks`);\n  console.log(`  - ${records.length.toLocaleString()} total records`);\n  console.log(`  - manifest.json`);\n  console.log(`  - client.ts`);\n\n  return {\n    manifest,\n    outputDir,\n    chunkCount: chunks.length,\n    totalRecords: records.length,\n  };\n}\n\n/**\n * Streaming build for large files - processes records incrementally\n * without loading entire file into memory\n */\nasync function buildStreaming(\n  inputFile: string,\n  options: BuildOptions,\n  startTime: number\n): Promise<BuildResult> {\n  // Parse chunk size\n  const targetChunkSize = parseSize(options.chunkSize);\n  console.log(`Target chunk size: ${formatBytes(targetChunkSize)}`);\n\n  // Create output directory\n  const outputDir = path.resolve(options.output);\n  const chunksDir = path.join(outputDir, \"chunks\");\n  await fs.promises.mkdir(chunksDir, { recursive: true });\n\n  // Streaming state\n  let currentChunk: DataRecord[] = [];\n  let currentChunkSize = 0;\n  let chunkId = 0;\n  const chunkMetas: ChunkMeta[] = [];\n  const indices: Record<string, Record<string, string[]>> = {};\n  let schema: Schema | null = null;\n  let indexedFields: string[] = [];\n  let chunkBy: string | null = null;\n\n  // Estimate record size without full serialization (rough heuristic)\n  const estimateRecordSize = (record: DataRecord): number => {\n    let size = 2; // {}\n    for (const key in record) {\n      const value = record[key];\n      size += key.length + 3; // \"key\":\n      if (value === null) size += 4;\n      else if (typeof value === \"string\") size += value.length + 2;\n      else if (typeof value === \"number\") size += String(value).length;\n      else if (typeof value === \"boolean\") size += value ? 4 : 5;\n      else size += 20; // rough estimate for complex values\n    }\n    return size;\n  };\n\n  // Process each record as it streams in\n  const processRecord = (record: DataRecord) => {\n    currentChunk.push(record);\n    currentChunkSize += estimateRecordSize(record);\n\n    // When chunk reaches target size, flush it\n    if (currentChunkSize >= targetChunkSize) {\n      flushChunk();\n    }\n  };\n\n  // Flush current chunk to disk\n  const flushChunk = () => {\n    if (currentChunk.length === 0) return;\n\n    // Sort by chunkBy field if specified\n    if (chunkBy) {\n      currentChunk.sort((a, b) => {\n        const aVal = a[chunkBy];\n        const bVal = b[chunkBy];\n        if (aVal === bVal) return 0;\n        if (aVal === null || aVal === undefined) return 1;\n        if (bVal === null || bVal === undefined) return -1;\n        return aVal < bVal ? -1 : 1;\n      });\n    }\n\n    const chunk: Chunk = {\n      id: String(chunkId),\n      records: currentChunk,\n      byteSize: currentChunkSize,\n    };\n\n    // Write chunk to disk\n    const chunkPath = path.join(chunksDir, `${chunkId}.json`);\n    fs.writeFileSync(chunkPath, JSON.stringify(currentChunk));\n\n    // Generate chunk metadata\n    const fieldRanges = calculateFieldRanges(currentChunk, schema!);\n    chunkMetas.push({\n      id: String(chunkId),\n      path: `chunks/${chunkId}.json`,\n      count: currentChunk.length,\n      byteSize: currentChunkSize,\n      fieldRanges,\n    });\n\n    // Update indices\n    for (const fieldName of indexedFields) {\n      if (!indices[fieldName]) {\n        indices[fieldName] = {};\n      }\n      for (const record of currentChunk) {\n        const value = record[fieldName];\n        if (value === null || value === undefined) continue;\n        const key = String(value);\n        if (!indices[fieldName][key]) {\n          indices[fieldName][key] = [];\n        }\n        if (!indices[fieldName][key].includes(String(chunkId))) {\n          indices[fieldName][key].push(String(chunkId));\n        }\n      }\n    }\n\n    console.log(`  Wrote chunk ${chunkId} (${currentChunk.length.toLocaleString()} records, ${formatBytes(currentChunkSize)})`);\n\n    // Reset for next chunk\n    chunkId++;\n    currentChunk = [];\n    currentChunkSize = 0;\n  };\n\n  // First pass: sample records to infer schema\n  console.log(\"Sampling records for schema inference...\");\n  const sampleResult = await streamFile(\n    inputFile,\n    options.format,\n    () => {}, // No processing in first pass\n    1000,\n    { sampleOnly: true, sampleSize: 1000 }\n  );\n\n  const format = sampleResult.format;\n  const sample = sampleResult.sample;\n\n  if (sample.length === 0) {\n    throw new Error(\"No records found in input file\");\n  }\n\n  // Infer schema from sample\n  console.log(\"Inferring schema from sample...\");\n  schema = inferSchema(sample);\n  console.log(`Found ${schema.fields.length} fields`);\n\n  if (schema.primaryField) {\n    console.log(`Detected primary field: ${schema.primaryField}`);\n  }\n\n  // Determine chunk field\n  chunkBy = options.chunkBy || suggestChunkField(schema, sample);\n  if (chunkBy) {\n    console.log(`Chunking by field: ${chunkBy}`);\n  }\n\n  // Parse indexed fields (limit auto-detected to 10 to prevent memory issues)\n  const MAX_AUTO_INDEX_FIELDS = 10;\n  if (options.index) {\n    indexedFields = options.index.split(\",\").map((f) => f.trim());\n  } else {\n    const autoIndexed = schema.fields.filter((f) => f.indexed).map((f) => f.name);\n    if (autoIndexed.length > MAX_AUTO_INDEX_FIELDS) {\n      console.log(`Warning: ${autoIndexed.length} indexable fields detected, limiting to ${MAX_AUTO_INDEX_FIELDS}`);\n      console.log(`Use --index to specify which fields to index`);\n      indexedFields = autoIndexed.slice(0, MAX_AUTO_INDEX_FIELDS);\n    } else {\n      indexedFields = autoIndexed;\n    }\n  }\n\n  if (indexedFields.length > 0) {\n    console.log(`Indexing fields: ${indexedFields.join(\", \")}`);\n  }\n\n  // Update schema with indexed fields\n  for (const field of schema.fields) {\n    field.indexed = indexedFields.includes(field.name);\n  }\n\n  // Second pass: process all records\n  console.log(\"Processing all records...\");\n  const { count } = await streamFile(\n    inputFile,\n    options.format,\n    processRecord,\n    1000,\n    { showProgress: true }\n  );\n\n  // Flush any remaining records\n  flushChunk();\n\n  console.log(`\\nProcessed ${count.toLocaleString()} records (format: ${format})`);\n\n  // Build config\n  const config: BuildConfig = {\n    chunkSize: targetChunkSize,\n    chunkBy: chunkBy || null,\n    indexedFields,\n  };\n\n  // Build manifest\n  const manifest: Manifest = {\n    version: VERSION,\n    generatedAt: new Date().toISOString(),\n    schema: schema!,\n    chunks: chunkMetas,\n    indices,\n    totalRecords: count,\n    config,\n  };\n\n  // Write manifest\n  const manifestPath = path.join(outputDir, \"manifest.json\");\n  await fs.promises.writeFile(manifestPath, JSON.stringify(manifest, null, 2));\n  console.log(`Wrote manifest to ${manifestPath}`);\n\n  // Generate client (use small sample for type inference - 100 is enough)\n  console.log(\"Generating client...\");\n  const sampleForTypes = sample.slice(0, 100);\n  const clientCode = generateClient(schema!, manifest, sampleForTypes);\n  const clientPath = path.join(outputDir, \"client.ts\");\n  await fs.promises.writeFile(clientPath, clientCode);\n  console.log(`Wrote client to ${clientPath}`);\n\n  const elapsed = ((Date.now() - startTime) / 1000).toFixed(2);\n  console.log(`\\nBuild completed in ${elapsed}s`);\n  console.log(`Output: ${outputDir}`);\n  console.log(`  - ${chunkMetas.length} chunks`);\n  console.log(`  - ${count.toLocaleString()} total records`);\n  console.log(`  - manifest.json`);\n  console.log(`  - client.ts`);\n\n  return {\n    manifest,\n    outputDir,\n    chunkCount: chunkMetas.length,\n    totalRecords: count,\n  };\n}\n\nfunction formatBytes(bytes: number): string {\n  if (bytes < 1024) return `${bytes} B`;\n  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)} KB`;\n  if (bytes < 1024 * 1024 * 1024) return `${(bytes / (1024 * 1024)).toFixed(1)} MB`;\n  return `${(bytes / (1024 * 1024 * 1024)).toFixed(1)} GB`;\n}\n","/**\n * Data file parsers for JSON, NDJSON, and CSV formats\n * Supports streaming for large files (2GB+)\n */\n\nimport * as fs from \"node:fs\";\nimport * as readline from \"node:readline\";\nimport Papa from \"papaparse\";\nimport streamJson from \"stream-json\";\nimport StreamArrayModule from \"stream-json/streamers/StreamArray.js\";\nimport streamChain from \"stream-chain\";\nimport cliProgress from \"cli-progress\";\nimport type { DataFormat, DataRecord } from \"../../types/index.js\";\n\nconst { parser: jsonParser } = streamJson;\nconst { chain } = streamChain;\nconst { streamArray } = StreamArrayModule;\n\n// Threshold for using streaming parser (100MB)\nconst STREAMING_THRESHOLD = 100 * 1024 * 1024;\n\nexport interface StreamOptions {\n  /** Show progress bar */\n  showProgress?: boolean;\n  /** Sample size for schema inference */\n  sampleSize?: number;\n  /** Stop after sampling (don't read entire file) */\n  sampleOnly?: boolean;\n}\n\n/**\n * Detect file format from extension or content\n */\nexport function detectFormat(filePath: string): DataFormat {\n  const ext = filePath.toLowerCase().split(\".\").pop();\n\n  if (ext === \"csv\") return \"csv\";\n  if (ext === \"ndjson\" || ext === \"jsonl\") return \"ndjson\";\n  if (ext === \"json\") {\n    // Peek at file to distinguish JSON array from NDJSON\n    const fd = fs.openSync(filePath, \"r\");\n    const buffer = Buffer.alloc(1024);\n    fs.readSync(fd, buffer, 0, 1024, 0);\n    fs.closeSync(fd);\n\n    const content = buffer.toString(\"utf-8\").trim();\n    if (content.startsWith(\"[\")) return \"json\";\n    if (content.startsWith(\"{\")) return \"ndjson\";\n  }\n\n  return \"json\"; // default\n}\n\n/**\n * Parse a JSON array file using streaming for large files\n */\nexport async function parseJsonArray(\n  filePath: string,\n  onRecord?: (record: DataRecord, index: number) => void\n): Promise<DataRecord[]> {\n  const stats = fs.statSync(filePath);\n\n  // Use streaming for large files\n  if (stats.size > STREAMING_THRESHOLD) {\n    return parseJsonArrayStreaming(filePath, onRecord);\n  }\n\n  // For smaller files, use standard parsing (faster)\n  const content = await fs.promises.readFile(filePath, \"utf-8\");\n  const data = JSON.parse(content);\n\n  if (!Array.isArray(data)) {\n    throw new Error(\"JSON file must contain an array of objects\");\n  }\n\n  if (onRecord) {\n    data.forEach((record, index) => onRecord(record as DataRecord, index));\n  }\n\n  return data as DataRecord[];\n}\n\n/**\n * Stream-parse a large JSON array file\n * Memory efficient - processes records one at a time\n */\nexport async function parseJsonArrayStreaming(\n  filePath: string,\n  onRecord?: (record: DataRecord, index: number) => void\n): Promise<DataRecord[]> {\n  return new Promise((resolve, reject) => {\n    const records: DataRecord[] = [];\n    let index = 0;\n    let lastLogTime = Date.now();\n\n    const pipeline = chain([\n      fs.createReadStream(filePath),\n      jsonParser(),\n      streamArray(),\n    ]);\n\n    pipeline.on(\"data\", (data: { key: number; value: DataRecord }) => {\n      const record = data.value;\n      records.push(record);\n      onRecord?.(record, index);\n      index++;\n\n      // Log progress every 5 seconds for large files\n      if (Date.now() - lastLogTime > 5000) {\n        console.log(`  Parsed ${index.toLocaleString()} records...`);\n        lastLogTime = Date.now();\n      }\n    });\n\n    pipeline.on(\"end\", () => {\n      resolve(records);\n    });\n\n    pipeline.on(\"error\", (err: Error) => {\n      reject(new Error(`JSON parse error: ${err.message}`));\n    });\n  });\n}\n\n/**\n * Parse an NDJSON file (newline-delimited JSON)\n * Streams line-by-line for memory efficiency\n */\nexport async function parseNdjson(\n  filePath: string,\n  onRecord?: (record: DataRecord, index: number) => void\n): Promise<DataRecord[]> {\n  const records: DataRecord[] = [];\n  let index = 0;\n\n  const fileStream = fs.createReadStream(filePath, { encoding: \"utf-8\" });\n  const rl = readline.createInterface({\n    input: fileStream,\n    crlfDelay: Infinity,\n  });\n\n  for await (const line of rl) {\n    const trimmed = line.trim();\n    if (!trimmed) continue;\n\n    try {\n      const record = JSON.parse(trimmed) as DataRecord;\n      records.push(record);\n      onRecord?.(record, index);\n      index++;\n    } catch (e) {\n      throw new Error(`Invalid JSON at line ${index + 1}: ${trimmed.slice(0, 50)}...`);\n    }\n  }\n\n  return records;\n}\n\n/**\n * Parse a CSV file\n * Uses papaparse for robust CSV handling\n */\nexport async function parseCsv(\n  filePath: string,\n  onRecord?: (record: DataRecord, index: number) => void\n): Promise<DataRecord[]> {\n  return new Promise((resolve, reject) => {\n    const records: DataRecord[] = [];\n    let index = 0;\n\n    const fileStream = fs.createReadStream(filePath, { encoding: \"utf-8\" });\n\n    Papa.parse(fileStream, {\n      header: true,\n      dynamicTyping: true,\n      skipEmptyLines: true,\n      step: (result) => {\n        if (result.errors.length > 0) {\n          reject(new Error(`CSV parse error: ${result.errors[0].message}`));\n          return;\n        }\n        const record = result.data as DataRecord;\n        records.push(record);\n        onRecord?.(record, index);\n        index++;\n      },\n      complete: () => resolve(records),\n      error: (error) => reject(error),\n    });\n  });\n}\n\n/**\n * Parse a data file in any supported format\n */\nexport async function parseFile(\n  filePath: string,\n  format?: DataFormat,\n  onRecord?: (record: DataRecord, index: number) => void\n): Promise<{ records: DataRecord[]; format: DataFormat }> {\n  const detectedFormat = format || detectFormat(filePath);\n\n  let records: DataRecord[];\n\n  switch (detectedFormat) {\n    case \"json\":\n      records = await parseJsonArray(filePath, onRecord);\n      break;\n    case \"ndjson\":\n      records = await parseNdjson(filePath, onRecord);\n      break;\n    case \"csv\":\n      records = await parseCsv(filePath, onRecord);\n      break;\n    default:\n      throw new Error(`Unsupported format: ${detectedFormat}`);\n  }\n\n  return { records, format: detectedFormat };\n}\n\n/**\n * Stream-only parsing - doesn't accumulate records in memory\n * Returns count and sample records for schema inference\n */\nexport async function streamFile(\n  filePath: string,\n  format: DataFormat | undefined,\n  onRecord: (record: DataRecord, index: number) => void,\n  sampleSize: number = 1000,\n  options: StreamOptions = {}\n): Promise<{ count: number; sample: DataRecord[]; format: DataFormat; estimatedCount?: number }> {\n  const detectedFormat = format || detectFormat(filePath);\n  const sample: DataRecord[] = [];\n  let count = 0;\n  let bytesProcessed = 0;\n  const fileSize = getFileSize(filePath);\n\n  // Setup progress bar if requested\n  let progressBar: cliProgress.SingleBar | null = null;\n  if (options.showProgress && !options.sampleOnly) {\n    progressBar = new cliProgress.SingleBar({\n      format: '  Processing |{bar}| {percentage}% | {value}/{total} MB | {records} records | ETA: {eta}s',\n      barCompleteChar: '\\u2588',\n      barIncompleteChar: '\\u2591',\n      hideCursor: true,\n    }, cliProgress.Presets.shades_classic);\n    progressBar.start(Math.round(fileSize / (1024 * 1024)), 0, { records: 0 });\n  }\n\n  const collector = (record: DataRecord, index: number, bytes?: number) => {\n    if (sample.length < sampleSize) {\n      sample.push(record);\n    }\n    count++;\n    if (bytes) bytesProcessed = bytes;\n\n    // Update progress bar every 1000 records\n    if (progressBar && count % 1000 === 0) {\n      progressBar.update(Math.round(bytesProcessed / (1024 * 1024)), { records: count.toLocaleString() });\n    }\n\n    onRecord(record, index);\n  };\n\n  // For sample-only mode, just read enough to get the sample\n  if (options.sampleOnly) {\n    switch (detectedFormat) {\n      case \"json\":\n        await streamJsonArraySample(filePath, collector, sampleSize);\n        break;\n      case \"ndjson\":\n        await streamNdjsonSample(filePath, collector, sampleSize);\n        break;\n      case \"csv\":\n        await streamCsvSample(filePath, collector, sampleSize);\n        break;\n      default:\n        throw new Error(`Unsupported format: ${detectedFormat}`);\n    }\n\n    // Estimate total count from sample\n    const avgRecordSize = bytesProcessed / count;\n    const estimatedCount = Math.round(fileSize / avgRecordSize);\n\n    return { count, sample, format: detectedFormat, estimatedCount };\n  }\n\n  switch (detectedFormat) {\n    case \"json\":\n      await streamJsonArray(filePath, collector, options.showProgress);\n      break;\n    case \"ndjson\":\n      await streamNdjson(filePath, collector, options.showProgress);\n      break;\n    case \"csv\":\n      await streamCsv(filePath, collector, options.showProgress);\n      break;\n    default:\n      throw new Error(`Unsupported format: ${detectedFormat}`);\n  }\n\n  if (progressBar) {\n    progressBar.update(Math.round(fileSize / (1024 * 1024)), { records: count.toLocaleString() });\n    progressBar.stop();\n  }\n\n  return { count, sample, format: detectedFormat };\n}\n\n/**\n * Stream JSON array without accumulating in memory\n */\nasync function streamJsonArray(\n  filePath: string,\n  onRecord: (record: DataRecord, index: number, bytes?: number) => void,\n  showProgress?: boolean\n): Promise<void> {\n  return new Promise((resolve, reject) => {\n    let index = 0;\n    let bytesRead = 0;\n\n    const readStream = fs.createReadStream(filePath);\n    readStream.on(\"data\", (chunk: Buffer) => {\n      bytesRead += chunk.length;\n    });\n\n    const pipeline = chain([\n      readStream,\n      jsonParser(),\n      streamArray(),\n    ]);\n\n    pipeline.on(\"data\", (data: { key: number; value: DataRecord }) => {\n      onRecord(data.value, index, bytesRead);\n      index++;\n    });\n\n    pipeline.on(\"end\", () => resolve());\n    pipeline.on(\"error\", (err: Error) => {\n      reject(new Error(`JSON parse error: ${err.message}`));\n    });\n  });\n}\n\n/**\n * Stream JSON array but stop after N records (for sampling)\n */\nasync function streamJsonArraySample(\n  filePath: string,\n  onRecord: (record: DataRecord, index: number, bytes?: number) => void,\n  maxRecords: number\n): Promise<void> {\n  return new Promise((resolve, reject) => {\n    let index = 0;\n    let bytesRead = 0;\n\n    const readStream = fs.createReadStream(filePath);\n    readStream.on(\"data\", (chunk: Buffer) => {\n      bytesRead += chunk.length;\n    });\n\n    const pipeline = chain([\n      readStream,\n      jsonParser(),\n      streamArray(),\n    ]);\n\n    pipeline.on(\"data\", (data: { key: number; value: DataRecord }) => {\n      onRecord(data.value, index, bytesRead);\n      index++;\n\n      if (index >= maxRecords) {\n        readStream.destroy();\n        resolve();\n      }\n    });\n\n    pipeline.on(\"end\", () => resolve());\n    pipeline.on(\"error\", (err: Error) => {\n      // Ignore errors from destroying the stream early\n      if (err.message.includes(\"aborted\") || err.message.includes(\"destroyed\")) {\n        resolve();\n      } else {\n        reject(new Error(`JSON parse error: ${err.message}`));\n      }\n    });\n  });\n}\n\n/**\n * Stream NDJSON without accumulating in memory\n */\nasync function streamNdjson(\n  filePath: string,\n  onRecord: (record: DataRecord, index: number, bytes?: number) => void,\n  showProgress?: boolean\n): Promise<void> {\n  let index = 0;\n  let bytesRead = 0;\n\n  const fileStream = fs.createReadStream(filePath, { encoding: \"utf-8\" });\n  fileStream.on(\"data\", (chunk: string) => {\n    bytesRead += Buffer.byteLength(chunk);\n  });\n\n  const rl = readline.createInterface({\n    input: fileStream,\n    crlfDelay: Infinity,\n  });\n\n  for await (const line of rl) {\n    const trimmed = line.trim();\n    if (!trimmed) continue;\n\n    try {\n      const record = JSON.parse(trimmed) as DataRecord;\n      onRecord(record, index, bytesRead);\n      index++;\n    } catch (e) {\n      throw new Error(`Invalid JSON at line ${index + 1}: ${trimmed.slice(0, 50)}...`);\n    }\n  }\n}\n\n/**\n * Stream NDJSON but stop after N records (for sampling)\n */\nasync function streamNdjsonSample(\n  filePath: string,\n  onRecord: (record: DataRecord, index: number, bytes?: number) => void,\n  maxRecords: number\n): Promise<void> {\n  let index = 0;\n  let bytesRead = 0;\n\n  const fileStream = fs.createReadStream(filePath, { encoding: \"utf-8\" });\n  fileStream.on(\"data\", (chunk: string) => {\n    bytesRead += Buffer.byteLength(chunk);\n  });\n\n  const rl = readline.createInterface({\n    input: fileStream,\n    crlfDelay: Infinity,\n  });\n\n  for await (const line of rl) {\n    const trimmed = line.trim();\n    if (!trimmed) continue;\n\n    try {\n      const record = JSON.parse(trimmed) as DataRecord;\n      onRecord(record, index, bytesRead);\n      index++;\n\n      if (index >= maxRecords) {\n        rl.close();\n        fileStream.destroy();\n        return;\n      }\n    } catch (e) {\n      throw new Error(`Invalid JSON at line ${index + 1}: ${trimmed.slice(0, 50)}...`);\n    }\n  }\n}\n\n/**\n * Stream CSV without accumulating in memory\n */\nasync function streamCsv(\n  filePath: string,\n  onRecord: (record: DataRecord, index: number, bytes?: number) => void,\n  showProgress?: boolean\n): Promise<void> {\n  return new Promise((resolve, reject) => {\n    let index = 0;\n    let bytesRead = 0;\n\n    const fileStream = fs.createReadStream(filePath, { encoding: \"utf-8\" });\n    fileStream.on(\"data\", (chunk: string) => {\n      bytesRead += Buffer.byteLength(chunk);\n    });\n\n    Papa.parse(fileStream, {\n      header: true,\n      dynamicTyping: true,\n      skipEmptyLines: true,\n      step: (result) => {\n        if (result.errors.length > 0) {\n          reject(new Error(`CSV parse error: ${result.errors[0].message}`));\n          return;\n        }\n        onRecord(result.data as DataRecord, index, bytesRead);\n        index++;\n      },\n      complete: () => resolve(),\n      error: (error) => reject(error),\n    });\n  });\n}\n\n/**\n * Stream CSV but stop after N records (for sampling)\n */\nasync function streamCsvSample(\n  filePath: string,\n  onRecord: (record: DataRecord, index: number, bytes?: number) => void,\n  maxRecords: number\n): Promise<void> {\n  return new Promise((resolve, reject) => {\n    let index = 0;\n    let bytesRead = 0;\n    let resolved = false;\n\n    const fileStream = fs.createReadStream(filePath, { encoding: \"utf-8\" });\n    fileStream.on(\"data\", (chunk: string) => {\n      bytesRead += Buffer.byteLength(chunk);\n    });\n\n    Papa.parse(fileStream, {\n      header: true,\n      dynamicTyping: true,\n      skipEmptyLines: true,\n      step: (result, parser) => {\n        if (resolved) return;\n\n        if (result.errors.length > 0) {\n          reject(new Error(`CSV parse error: ${result.errors[0].message}`));\n          return;\n        }\n        onRecord(result.data as DataRecord, index, bytesRead);\n        index++;\n\n        if (index >= maxRecords) {\n          resolved = true;\n          parser.abort();\n          fileStream.destroy();\n          resolve();\n        }\n      },\n      complete: () => {\n        if (!resolved) resolve();\n      },\n      error: (error) => reject(error),\n    });\n  });\n}\n\n/**\n * Get file size in bytes\n */\nexport function getFileSize(filePath: string): number {\n  const stats = fs.statSync(filePath);\n  return stats.size;\n}\n\n/**\n * Parse size string (e.g., \"5mb\", \"1gb\") to bytes\n */\nexport function parseSize(sizeStr: string): number {\n  const match = sizeStr.toLowerCase().match(/^(\\d+(?:\\.\\d+)?)\\s*(b|kb|mb|gb)?$/);\n  if (!match) {\n    throw new Error(`Invalid size format: ${sizeStr}. Use format like \"5mb\" or \"1gb\"`);\n  }\n\n  const value = parseFloat(match[1]);\n  const unit = match[2] || \"b\";\n\n  const multipliers: Record<string, number> = {\n    b: 1,\n    kb: 1024,\n    mb: 1024 * 1024,\n    gb: 1024 * 1024 * 1024,\n  };\n\n  return Math.floor(value * multipliers[unit]);\n}\n","/**\n * Schema inference from data records\n */\n\nimport type { DataRecord, FieldSchema, FieldStats, FieldType, Schema } from \"../../types/index.js\";\n\n/**\n * Infer the type of a value\n */\nfunction inferType(value: unknown): FieldType {\n  if (value === null || value === undefined) {\n    return \"null\";\n  }\n\n  const type = typeof value;\n\n  if (type === \"string\") {\n    // Check if it's a date string\n    if (isDateString(value as string)) {\n      return \"date\";\n    }\n    return \"string\";\n  }\n\n  if (type === \"number\") {\n    return \"number\";\n  }\n\n  if (type === \"boolean\") {\n    return \"boolean\";\n  }\n\n  // Arrays and objects treated as string (JSON serialized)\n  return \"string\";\n}\n\n/**\n * Check if a string looks like a date\n */\nfunction isDateString(value: string): boolean {\n  // ISO 8601 date patterns\n  const isoPattern = /^\\d{4}-\\d{2}-\\d{2}(T\\d{2}:\\d{2}:\\d{2}(\\.\\d{3})?(Z|[+-]\\d{2}:?\\d{2})?)?$/;\n  if (isoPattern.test(value)) {\n    const date = new Date(value);\n    return !isNaN(date.getTime());\n  }\n  return false;\n}\n\n/**\n * Merge two field types, returning the more general type\n */\nfunction mergeTypes(type1: FieldType, type2: FieldType): FieldType {\n  if (type1 === type2) return type1;\n  if (type1 === \"null\") return type2;\n  if (type2 === \"null\") return type1;\n\n  // If types differ, fall back to string\n  return \"string\";\n}\n\n/**\n * Collect statistics for a field across all records\n */\nclass FieldStatsCollector {\n  private values = new Set<string>();\n  private min: string | number | undefined;\n  private max: string | number | undefined;\n  private nullCount = 0;\n  private count = 0;\n  private type: FieldType = \"null\";\n  private sampleValues: (string | number | boolean | null)[] = [];\n\n  add(value: unknown): void {\n    this.count++;\n\n    if (value === null || value === undefined) {\n      this.nullCount++;\n      return;\n    }\n\n    const valueType = inferType(value);\n    this.type = mergeTypes(this.type, valueType);\n\n    // Track unique values for cardinality (limit to avoid memory issues)\n    if (this.values.size < 10000) {\n      this.values.add(String(value));\n    }\n\n    // Collect sample values\n    if (this.sampleValues.length < 5 && !this.sampleValues.includes(value as string | number | boolean | null)) {\n      this.sampleValues.push(value as string | number | boolean | null);\n    }\n\n    // Track min/max for numbers and dates\n    if (typeof value === \"number\") {\n      if (this.min === undefined || value < (this.min as number)) {\n        this.min = value;\n      }\n      if (this.max === undefined || value > (this.max as number)) {\n        this.max = value;\n      }\n    } else if (typeof value === \"string\" && (this.type === \"date\" || this.type === \"string\")) {\n      if (this.min === undefined || value < this.min) {\n        this.min = value;\n      }\n      if (this.max === undefined || value > this.max) {\n        this.max = value;\n      }\n    }\n  }\n\n  getStats(): FieldStats {\n    return {\n      min: this.min,\n      max: this.max,\n      cardinality: this.values.size,\n      nullCount: this.nullCount,\n      sampleValues: this.sampleValues,\n    };\n  }\n\n  getType(): FieldType {\n    return this.type;\n  }\n\n  isNullable(): boolean {\n    return this.nullCount > 0;\n  }\n}\n\n/**\n * Determine if a field should be indexed based on various heuristics\n */\nfunction shouldIndex(\n  name: string,\n  type: FieldType,\n  cardinality: number,\n  totalRecords: number,\n  stats: FieldStats\n): boolean {\n  // Skip if cardinality is 1 (all same value - useless index)\n  if (cardinality <= 1) {\n    return false;\n  }\n\n  // Skip if cardinality is too high (over 50% of records)\n  if (cardinality > totalRecords * 0.5) {\n    return false;\n  }\n\n  // Skip if cardinality exceeds reasonable index size\n  if (cardinality > 1000) {\n    return false;\n  }\n\n  // Check sample values for patterns that indicate non-indexable data\n  const samples = stats.sampleValues || [];\n  for (const sample of samples) {\n    if (sample === null || sample === undefined) continue;\n    const str = String(sample);\n\n    // Skip stringified objects\n    if (str.startsWith(\"[object \") || str === \"[object Object]\") {\n      return false;\n    }\n\n    // Skip URLs (not useful for filtering)\n    if (str.includes(\"://\")) {\n      return false;\n    }\n\n    // Skip very long values (likely URIs, hashes, or encoded data)\n    if (str.length > 100) {\n      return false;\n    }\n  }\n\n  // Skip fields with names suggesting they're not good filter targets\n  const nameLower = name.toLowerCase();\n  const skipPatterns = [\n    \"_uri\", \"_url\", \"_id\", // ID/URL suffixes (unless it's a category ID)\n    \"uri\", \"url\", \"href\", \"link\", // URL fields\n    \"hash\", \"token\", \"secret\", \"key\", // Security/hash fields\n    \"description\", \"text\", \"content\", \"body\", // Long text fields\n  ];\n\n  // But allow specific useful ID patterns\n  const allowPatterns = [\n    \"category\", \"type\", \"status\", \"state\", \"level\",\n    \"color\", \"lang\", \"rarity\", \"set\", \"frame\",\n  ];\n\n  const hasSkipPattern = skipPatterns.some(p => nameLower.includes(p));\n  const hasAllowPattern = allowPatterns.some(p => nameLower.includes(p));\n\n  if (hasSkipPattern && !hasAllowPattern) {\n    return false;\n  }\n\n  // Good candidates: booleans with both true/false values, low-cardinality strings, enums\n  // Note: cardinality check at top already excludes cardinality <= 1\n  if (type === \"boolean\") {\n    return cardinality >= 2; // Only index if has both true and false\n  }\n\n  // Low cardinality is good for filtering\n  return cardinality >= 2 && cardinality <= 500;\n}\n\n/**\n * Infer schema from a set of records\n */\nexport function inferSchema(records: DataRecord[]): Schema {\n  if (records.length === 0) {\n    return { fields: [], primaryField: null };\n  }\n\n  // Collect all field names\n  const fieldNames = new Set<string>();\n  for (const record of records) {\n    for (const key of Object.keys(record)) {\n      fieldNames.add(key);\n    }\n  }\n\n  // Collect stats for each field\n  const collectors = new Map<string, FieldStatsCollector>();\n  for (const name of fieldNames) {\n    collectors.set(name, new FieldStatsCollector());\n  }\n\n  for (const record of records) {\n    for (const name of fieldNames) {\n      const collector = collectors.get(name)!;\n      collector.add(record[name]);\n    }\n  }\n\n  // Build field schemas\n  const fields: FieldSchema[] = [];\n  let primaryField: string | null = null;\n\n  for (const name of fieldNames) {\n    const collector = collectors.get(name)!;\n    const stats = collector.getStats();\n    const type = collector.getType();\n\n    // Determine if field should be indexed\n    const cardinality = stats.cardinality;\n    const indexed = shouldIndex(name, type, cardinality, records.length, stats);\n\n    // Detect potential primary key\n    // High cardinality, not nullable, unique values\n    if (\n      primaryField === null &&\n      stats.cardinality === records.length &&\n      !collector.isNullable() &&\n      (name.toLowerCase().includes(\"id\") || name.toLowerCase() === \"key\")\n    ) {\n      primaryField = name;\n    }\n\n    fields.push({\n      name,\n      type,\n      nullable: collector.isNullable(),\n      indexed,\n      stats,\n    });\n  }\n\n  // Sort fields: primary key first, then alphabetically\n  fields.sort((a, b) => {\n    if (a.name === primaryField) return -1;\n    if (b.name === primaryField) return 1;\n    return a.name.localeCompare(b.name);\n  });\n\n  return { fields, primaryField };\n}\n\n/**\n * Suggest the best field to chunk by\n */\nexport function suggestChunkField(schema: Schema, records: DataRecord[]): string | null {\n  // If there's a primary field, chunk by it\n  if (schema.primaryField) {\n    return schema.primaryField;\n  }\n\n  // Look for a good chunking candidate:\n  // - High cardinality (spreads data across chunks)\n  // - Numeric or date (supports range queries)\n  // - Not nullable\n  let bestField: string | null = null;\n  let bestScore = 0;\n\n  for (const field of schema.fields) {\n    if (field.nullable) continue;\n\n    let score = 0;\n\n    // Prefer numeric/date fields\n    if (field.type === \"number\" || field.type === \"date\") {\n      score += 50;\n    }\n\n    // Prefer higher cardinality (but not too high)\n    const cardinalityRatio = field.stats.cardinality / records.length;\n    if (cardinalityRatio > 0.1 && cardinalityRatio <= 1) {\n      score += cardinalityRatio * 30;\n    }\n\n    // Prefer fields with \"id\", \"key\", \"date\", \"time\" in name\n    const nameLower = field.name.toLowerCase();\n    if (nameLower.includes(\"id\")) score += 20;\n    if (nameLower.includes(\"date\") || nameLower.includes(\"time\")) score += 15;\n    if (nameLower.includes(\"created\") || nameLower.includes(\"updated\")) score += 10;\n\n    if (score > bestScore) {\n      bestScore = score;\n      bestField = field.name;\n    }\n  }\n\n  return bestField;\n}\n\n/**\n * Get fields that should be indexed based on cardinality\n */\nexport function getIndexableFields(schema: Schema): string[] {\n  return schema.fields\n    .filter((f) => f.indexed)\n    .map((f) => f.name);\n}\n","/**\n * Data chunking logic for splitting records into smaller files\n */\n\nimport type { ChunkMeta, DataRecord, Schema } from \"../../types/index.js\";\n\nexport interface Chunk {\n  id: string;\n  records: DataRecord[];\n  byteSize: number;\n}\n\nexport interface ChunkOptions {\n  targetSize: number; // target bytes per chunk\n  chunkBy?: string; // field to sort/chunk by\n}\n\n/**\n * Estimate JSON byte size of a record\n */\nfunction estimateRecordSize(record: DataRecord): number {\n  return JSON.stringify(record).length;\n}\n\n/**\n * Compare two values for sorting\n */\nfunction compareValues(a: unknown, b: unknown): number {\n  if (a === b) return 0;\n  if (a === null || a === undefined) return -1;\n  if (b === null || b === undefined) return 1;\n\n  if (typeof a === \"number\" && typeof b === \"number\") {\n    return a - b;\n  }\n\n  return String(a).localeCompare(String(b));\n}\n\n/**\n * Split records into chunks based on target size\n */\nexport function chunkRecords(\n  records: DataRecord[],\n  schema: Schema,\n  options: ChunkOptions\n): Chunk[] {\n  const { targetSize, chunkBy } = options;\n\n  // Sort records if chunkBy field is specified\n  let sortedRecords = records;\n  if (chunkBy) {\n    sortedRecords = [...records].sort((a, b) =>\n      compareValues(a[chunkBy], b[chunkBy])\n    );\n  }\n\n  const chunks: Chunk[] = [];\n  let currentChunk: DataRecord[] = [];\n  let currentSize = 0;\n  let chunkIndex = 0;\n\n  // Account for JSON array overhead: [ and ] plus commas\n  const arrayOverhead = 2; // []\n  const commaOverhead = 1; // ,\n\n  for (const record of sortedRecords) {\n    const recordSize = estimateRecordSize(record) + commaOverhead;\n\n    // If adding this record would exceed target size and we have records,\n    // start a new chunk\n    if (currentSize + recordSize > targetSize && currentChunk.length > 0) {\n      chunks.push({\n        id: String(chunkIndex),\n        records: currentChunk,\n        byteSize: currentSize + arrayOverhead,\n      });\n      chunkIndex++;\n      currentChunk = [];\n      currentSize = 0;\n    }\n\n    currentChunk.push(record);\n    currentSize += recordSize;\n  }\n\n  // Don't forget the last chunk\n  if (currentChunk.length > 0) {\n    chunks.push({\n      id: String(chunkIndex),\n      records: currentChunk,\n      byteSize: currentSize + arrayOverhead,\n    });\n  }\n\n  return chunks;\n}\n\n/**\n * Calculate field ranges for a chunk (min/max per field)\n */\nexport function calculateFieldRanges(\n  records: DataRecord[],\n  schema: Schema\n): ChunkMeta[\"fieldRanges\"] {\n  const ranges: ChunkMeta[\"fieldRanges\"] = {};\n\n  for (const field of schema.fields) {\n    const values = records\n      .map((r) => r[field.name])\n      .filter((v) => v !== null && v !== undefined);\n\n    if (values.length === 0) {\n      continue;\n    }\n\n    // For numbers and dates, calculate actual min/max\n    if (field.type === \"number\" || field.type === \"date\" || field.type === \"string\") {\n      const sorted = [...values].sort(compareValues);\n      ranges[field.name] = {\n        min: sorted[0],\n        max: sorted[sorted.length - 1],\n      };\n    }\n  }\n\n  return ranges;\n}\n\n/**\n * Generate chunk metadata\n */\nexport function generateChunkMeta(\n  chunk: Chunk,\n  schema: Schema,\n  basePath: string\n): ChunkMeta {\n  return {\n    id: chunk.id,\n    path: `${basePath}/${chunk.id}.json`,\n    count: chunk.records.length,\n    byteSize: chunk.byteSize,\n    fieldRanges: calculateFieldRanges(chunk.records, schema),\n  };\n}\n\n/**\n * Serialize a chunk to JSON string\n */\nexport function serializeChunk(chunk: Chunk): string {\n  return JSON.stringify(chunk.records);\n}\n\n/**\n * Calculate optimal chunk count based on data size and target chunk size\n */\nexport function calculateChunkCount(\n  totalRecords: number,\n  totalSize: number,\n  targetChunkSize: number\n): number {\n  const estimatedChunks = Math.ceil(totalSize / targetChunkSize);\n  // At least 1 chunk, and reasonable upper bound\n  return Math.max(1, Math.min(estimatedChunks, totalRecords));\n}\n","/**\n * Index generation for efficient query pruning\n */\n\nimport type { Chunk } from \"./chunker.js\";\nimport type { Schema } from \"../../types/index.js\";\n\n/**\n * Inverted index: field -> value -> chunk IDs\n */\nexport type InvertedIndex = Record<string, Record<string, string[]>>;\n\n/**\n * Build inverted indices for indexed fields\n * Maps each unique value to the chunk IDs that contain it\n */\nexport function buildInvertedIndices(\n  chunks: Chunk[],\n  schema: Schema,\n  indexedFields?: string[]\n): InvertedIndex {\n  const indices: InvertedIndex = {};\n\n  // Get fields to index\n  const fieldsToIndex = indexedFields\n    ? schema.fields.filter((f) => indexedFields.includes(f.name))\n    : schema.fields.filter((f) => f.indexed);\n\n  for (const field of fieldsToIndex) {\n    const fieldIndex: Record<string, string[]> = {};\n\n    for (const chunk of chunks) {\n      const valuesInChunk = new Set<string>();\n\n      for (const record of chunk.records) {\n        const value = record[field.name];\n        if (value !== null && value !== undefined) {\n          valuesInChunk.add(String(value));\n        }\n      }\n\n      // Add chunk ID to each value's list\n      for (const value of valuesInChunk) {\n        if (!fieldIndex[value]) {\n          fieldIndex[value] = [];\n        }\n        fieldIndex[value].push(chunk.id);\n      }\n    }\n\n    // Only include index if it would be useful\n    // (not too many unique values, which would make the index huge)\n    const uniqueValues = Object.keys(fieldIndex).length;\n    if (uniqueValues <= 10000) {\n      indices[field.name] = fieldIndex;\n    }\n  }\n\n  return indices;\n}\n\n/**\n * Find chunk IDs that might contain records matching a field value\n */\nexport function findChunksForValue(\n  indices: InvertedIndex,\n  fieldName: string,\n  value: unknown\n): string[] | null {\n  const fieldIndex = indices[fieldName];\n  if (!fieldIndex) {\n    // Field not indexed, return null to indicate all chunks must be searched\n    return null;\n  }\n\n  const stringValue = String(value);\n  return fieldIndex[stringValue] || [];\n}\n\n/**\n * Find chunk IDs that match multiple field conditions (AND logic)\n */\nexport function findChunksForConditions(\n  indices: InvertedIndex,\n  conditions: Record<string, unknown>\n): string[] | null {\n  let resultChunks: Set<string> | null = null;\n\n  for (const [field, value] of Object.entries(conditions)) {\n    const chunks = findChunksForValue(indices, field, value);\n\n    if (chunks === null) {\n      // This field isn't indexed, can't narrow down\n      continue;\n    }\n\n    if (resultChunks === null) {\n      resultChunks = new Set(chunks);\n    } else {\n      // Intersect with existing results\n      resultChunks = new Set([...resultChunks].filter((c) => chunks.includes(c)));\n    }\n\n    // Early exit if no chunks match\n    if (resultChunks.size === 0) {\n      return [];\n    }\n  }\n\n  return resultChunks ? [...resultChunks] : null;\n}\n\n/**\n * Estimate index size in bytes\n */\nexport function estimateIndexSize(indices: InvertedIndex): number {\n  return JSON.stringify(indices).length;\n}\n","/**\n * TypeScript client code generator\n * Generates data-specific types that work with the package runtime\n */\n\nimport { json2ts } from \"json-ts\";\nimport type { DataRecord, Manifest, Schema } from \"../../types/index.js\";\n\n/**\n * Post-process json-ts output to clean up interface names\n */\nfunction cleanupTypes(types: string): string {\n  return types\n    .replace(/^type IItem = IItemItem\\[\\];\\n/m, \"\")\n    .replace(/IItemItem/g, \"Item\")\n    .replace(/\\bI([A-Z][a-z_]+)/g, \"$1\")\n    .replace(/^interface /gm, \"export interface \");\n}\n\n/**\n * Generate field names type from schema\n */\nfunction generateFieldNamesType(schema: Schema): string {\n  const names = schema.fields.map((f) => `\"${f.name}\"`).join(\" | \");\n  return `export type FieldName = ${names};`;\n}\n\n/**\n * Generate where clause types based on schema field types\n */\nfunction generateWhereTypes(schema: Schema): string {\n  return `export type WhereClause = {\n${schema.fields\n  .map((f) => {\n    if (f.type === \"number\") {\n      return `  ${f.name}?: number | NumericOperators;`;\n    } else if (f.type === \"boolean\") {\n      return `  ${f.name}?: boolean;`;\n    } else {\n      return `  ${f.name}?: string | StringOperators;`;\n    }\n  })\n  .join(\"\\n\")}\n};`;\n}\n\n/**\n * Generate the client code - now imports runtime from package\n */\nexport function generateClient(\n  schema: Schema,\n  manifest: Manifest,\n  samples: DataRecord[]\n): string {\n  // Use json-ts to generate the Item interface from actual data samples\n  const rawTypes = json2ts(JSON.stringify(samples), { rootName: \"Item\" });\n  const itemInterface = cleanupTypes(rawTypes);\n\n  const fieldNamesType = generateFieldNamesType(schema);\n  const whereTypes = generateWhereTypes(schema);\n\n  const sortableFields = schema.fields\n    .filter((f) => f.type === \"number\" || f.type === \"string\" || f.type === \"date\")\n    .map((f) => `\"${f.name}\"`)\n    .join(\" | \");\n\n  return `/**\n * Auto-generated types for static-shard\n * Generated at: ${manifest.generatedAt}\n * Total records: ${manifest.totalRecords}\n * Chunks: ${manifest.chunks.length}\n */\n\nimport {\n  StaticShardClient,\n  createClient as createBaseClient,\n  StringOperators,\n  NumericOperators,\n  type ClientOptions,\n  type ClientQueryOptions,\n} from \"static-shard\";\n\n// ============================================================================\n// Data Types (generated from your data)\n// ============================================================================\n\n${itemInterface}\n\n${fieldNamesType}\n\n${whereTypes}\n\nexport type SortableField = ${sortableFields || \"string\"};\n\n// ============================================================================\n// Typed Client\n// ============================================================================\n\nexport type TypedQueryOptions = ClientQueryOptions<WhereClause, SortableField>;\n\nexport class Client extends StaticShardClient<Item, WhereClause, SortableField> {}\n\nexport function createClient(options: ClientOptions): Client {\n  return createBaseClient<Item, WhereClause, SortableField>(options);\n}\n\n// Default client for current directory\nexport const db = createClient({ basePath: \".\" });\n`;\n}\n","/**\n * Inspect command - analyze data file and guide through build setup\n * Interactive wizard for selecting indexes and build options\n */\n\nimport * as fs from \"node:fs\";\nimport { checkbox, confirm, input, select } from \"@inquirer/prompts\";\nimport type { InspectOptions, FieldSchema } from \"../../types/index.js\";\nimport { getFileSize, parseFile, streamFile } from \"../utils/parsers.js\";\nimport { inferSchema, suggestChunkField, getIndexableFields } from \"../utils/schema.js\";\nimport { build } from \"./build.js\";\n\n// Use streaming for files larger than 100MB\nconst STREAMING_THRESHOLD = 100 * 1024 * 1024;\n\n// Threshold for prompting index selection\nconst INDEX_PROMPT_THRESHOLD = 5;\n\nexport async function inspect(\n  inputFile: string,\n  options: InspectOptions\n): Promise<void> {\n  // Validate input file exists\n  if (!fs.existsSync(inputFile)) {\n    throw new Error(`Input file not found: ${inputFile}`);\n  }\n\n  const fileSize = getFileSize(inputFile);\n  console.log(`\\nFile: ${inputFile}`);\n  console.log(`Size: ${formatBytes(fileSize)}`);\n\n  let totalRecords: number;\n  let sampleRecords: unknown[];\n  let format: string;\n  let isEstimated = false;\n\n  // Use streaming for large files\n  if (fileSize > STREAMING_THRESHOLD) {\n    console.log(\"\\nUsing streaming mode for large file...\");\n\n    const sampleSize = options.sample || 1000;\n\n    // Fast mode: only sample, don't read entire file\n    if (options.fast) {\n      console.log(\"Fast mode: sampling records (count will be estimated)...\");\n\n      const result = await streamFile(\n        inputFile,\n        options.format,\n        () => {}, // No-op\n        sampleSize,\n        { sampleOnly: true, sampleSize }\n      );\n\n      totalRecords = result.estimatedCount || result.count;\n      sampleRecords = result.sample;\n      format = result.format;\n      isEstimated = true;\n\n      console.log(`Format: ${format}`);\n      console.log(`Estimated records: ~${totalRecords.toLocaleString()}`);\n    } else {\n      console.log(\"Processing all records (use --fast to estimate count instead)...\");\n\n      const result = await streamFile(\n        inputFile,\n        options.format,\n        () => {}, // No-op, we just want the count and sample\n        sampleSize,\n        { showProgress: true }\n      );\n\n      totalRecords = result.count;\n      sampleRecords = result.sample;\n      format = result.format;\n\n      console.log(`Format: ${format}`);\n      console.log(`Total records: ${totalRecords.toLocaleString()}`);\n    }\n  } else {\n    console.log(\"\\nParsing file...\");\n\n    // Parse the file\n    const result = await parseFile(inputFile, options.format);\n    totalRecords = result.records.length;\n    format = result.format;\n\n    console.log(`Format: ${format}`);\n    console.log(`Total records: ${totalRecords.toLocaleString()}`);\n\n    if (totalRecords === 0) {\n      console.log(\"\\nNo records found.\");\n      return;\n    }\n\n    // Sample records for schema inference if needed\n    const sampleSize = Math.min(options.sample || 1000, totalRecords);\n    sampleRecords = result.records.slice(0, sampleSize);\n  }\n\n  if (sampleRecords.length === 0) {\n    console.log(\"\\nNo records found.\");\n    return;\n  }\n\n  const sampleSize = sampleRecords.length;\n  console.log(`\\nAnalyzing ${sampleSize.toLocaleString()} records...`);\n\n  // Infer schema\n  const schema = inferSchema(sampleRecords);\n\n  // Show schema summary\n  console.log(\"\\n\" + \"=\".repeat(60));\n  console.log(\"SCHEMA\");\n  console.log(\"=\".repeat(60));\n\n  console.log(`\\nFields (${schema.fields.length}):\\n`);\n\n  for (const field of schema.fields) {\n    const isPrimary = field.name === schema.primaryField ? \" [PRIMARY]\" : \"\";\n    const isIndexed = field.indexed ? \" [INDEXED]\" : \"\";\n    const nullable = field.nullable ? \" (nullable)\" : \"\";\n\n    console.log(`  ${field.name}: ${field.type}${nullable}${isPrimary}${isIndexed}`);\n\n    // Show stats\n    const stats = field.stats;\n    const statParts = [];\n\n    if (stats.cardinality !== undefined) {\n      const cardinalityPct = ((stats.cardinality / totalRecords) * 100).toFixed(1);\n      statParts.push(`cardinality: ${stats.cardinality} (${cardinalityPct}%)`);\n    }\n\n    if (stats.min !== undefined && stats.max !== undefined) {\n      statParts.push(`range: ${formatValue(stats.min)} - ${formatValue(stats.max)}`);\n    }\n\n    if (stats.nullCount > 0) {\n      statParts.push(`nulls: ${stats.nullCount}`);\n    }\n\n    if (statParts.length > 0) {\n      console.log(`    ${statParts.join(\", \")}`);\n    }\n\n    if (stats.sampleValues && stats.sampleValues.length > 0) {\n      const samples = stats.sampleValues.slice(0, 3).map(formatValue).join(\", \");\n      console.log(`    examples: ${samples}`);\n    }\n  }\n\n  // Size estimates\n  const avgRecordSize = fileSize / totalRecords;\n  const targetChunkSize = 5 * 1024 * 1024; // 5MB\n  const estimatedChunks = Math.ceil(fileSize / targetChunkSize);\n\n  console.log(\"\\n\" + \"=\".repeat(60));\n  console.log(\"SIZE ESTIMATES\");\n  console.log(\"=\".repeat(60));\n\n  console.log(`\\nAverage record size: ${formatBytes(avgRecordSize)}`);\n  console.log(`With default 5MB chunks: ~${estimatedChunks} chunks`);\n\n  // Interactive wizard\n  console.log(\"\\n\" + \"=\".repeat(60));\n  console.log(\"BUILD CONFIGURATION\");\n  console.log(\"=\".repeat(60));\n\n  // 1. Chunk field selection\n  const suggestedChunkField = suggestChunkField(schema, sampleRecords);\n  const chunkableFields = schema.fields\n    .filter(f => f.type === \"number\" || f.type === \"string\" || f.type === \"date\")\n    .map(f => f.name);\n\n  let selectedChunkField: string | null = null;\n\n  if (chunkableFields.length > 0) {\n    const chunkFieldChoices = [\n      { name: \"(none - chunk by record order)\", value: \"\" },\n      ...chunkableFields.map(f => ({\n        name: f === suggestedChunkField ? `${f} (recommended)` : f,\n        value: f\n      }))\n    ];\n\n    // Sort to put recommended first\n    chunkFieldChoices.sort((a, b) => {\n      if (a.value === suggestedChunkField) return -1;\n      if (b.value === suggestedChunkField) return 1;\n      if (a.value === \"\") return 1;\n      if (b.value === \"\") return -1;\n      return 0;\n    });\n\n    selectedChunkField = await select({\n      message: \"Sort and chunk records by which field?\",\n      choices: chunkFieldChoices,\n      default: suggestedChunkField || \"\"\n    });\n\n    if (selectedChunkField === \"\") {\n      selectedChunkField = null;\n    }\n  }\n\n  // 2. Index selection\n  const indexableFields = getIndexableFields(schema);\n  let selectedIndexes: string[] = [];\n\n  if (indexableFields.length > 0) {\n    if (indexableFields.length <= INDEX_PROMPT_THRESHOLD) {\n      // Few fields - show simple yes/no for using recommended indexes\n      const useRecommended = await confirm({\n        message: `Use recommended indexes? (${indexableFields.join(\", \")})`,\n        default: true\n      });\n\n      if (useRecommended) {\n        selectedIndexes = indexableFields;\n      } else {\n        // Let them pick manually\n        selectedIndexes = await promptIndexSelection(schema.fields, indexableFields);\n      }\n    } else {\n      // Many fields - always show selection UI\n      console.log(`\\nFound ${indexableFields.length} indexable fields.`);\n      selectedIndexes = await promptIndexSelection(schema.fields, indexableFields);\n    }\n  }\n\n  // 3. Output directory\n  const outputDir = await input({\n    message: \"Output directory:\",\n    default: \"./output\"\n  });\n\n  // 4. Chunk size\n  const chunkSizeAnswer = await select({\n    message: \"Target chunk size:\",\n    choices: [\n      { name: \"1 MB (more chunks, faster queries)\", value: \"1mb\" },\n      { name: \"5 MB (balanced) - recommended\", value: \"5mb\" },\n      { name: \"10 MB (fewer chunks, larger downloads)\", value: \"10mb\" },\n      { name: \"Custom\", value: \"custom\" }\n    ],\n    default: \"5mb\"\n  });\n\n  let chunkSize = chunkSizeAnswer;\n  if (chunkSizeAnswer === \"custom\") {\n    chunkSize = await input({\n      message: \"Enter chunk size (e.g., 2mb, 500kb):\",\n      default: \"5mb\"\n    });\n  }\n\n  // Summary\n  console.log(\"\\n\" + \"=\".repeat(60));\n  console.log(\"CONFIGURATION SUMMARY\");\n  console.log(\"=\".repeat(60));\n\n  console.log(`\\nInput: ${inputFile}`);\n  console.log(`Output: ${outputDir}`);\n  console.log(`Chunk size: ${chunkSize}`);\n  console.log(`Chunk by: ${selectedChunkField || \"(record order)\"}`);\n  console.log(`Indexes: ${selectedIndexes.length > 0 ? selectedIndexes.join(\", \") : \"(none)\"}`);\n\n  // Build command for reference\n  let cmd = `npx static-shard build \"${inputFile}\" -o \"${outputDir}\" -s ${chunkSize}`;\n  if (selectedChunkField) {\n    cmd += ` -c ${selectedChunkField}`;\n  }\n  if (selectedIndexes.length > 0) {\n    cmd += ` -i \"${selectedIndexes.join(\",\")}\"`;\n  }\n\n  console.log(`\\nEquivalent command:\\n  ${cmd}`);\n\n  // 5. Run build?\n  const runBuild = await confirm({\n    message: \"Run build now?\",\n    default: true\n  });\n\n  if (runBuild) {\n    console.log(\"\\n\");\n    await build(inputFile, {\n      output: outputDir,\n      chunkSize,\n      chunkBy: selectedChunkField || undefined,\n      index: selectedIndexes.length > 0 ? selectedIndexes.join(\",\") : undefined,\n      format: options.format\n    });\n  } else {\n    console.log(\"\\nBuild skipped. Run the command above when ready.\\n\");\n  }\n}\n\n/**\n * Interactive prompt for selecting which fields to index\n */\nasync function promptIndexSelection(\n  allFields: FieldSchema[],\n  recommendedFields: string[]\n): Promise<string[]> {\n  // Group fields by category for better UX\n  const booleanFields = allFields.filter(f => f.type === \"boolean\").map(f => f.name);\n  const enumLikeFields = allFields.filter(f =>\n    f.type === \"string\" &&\n    f.stats.cardinality <= 100 &&\n    !booleanFields.includes(f.name)\n  ).map(f => f.name);\n  const numericFields = allFields.filter(f => f.type === \"number\").map(f => f.name);\n\n  const choices = allFields\n    .filter(f => {\n      // Only show fields that could reasonably be indexed\n      // Must have cardinality >= 2 (cardinality 1 = all same value, useless for filtering)\n      if (f.stats.cardinality < 2) return false;\n      if (f.type === \"boolean\") return true;\n      if (f.stats.cardinality <= 500) return true;\n      return false;\n    })\n    .map(f => {\n      const isRecommended = recommendedFields.includes(f.name);\n      const cardinalityInfo = f.stats.cardinality ? ` (${f.stats.cardinality} values)` : \"\";\n\n      return {\n        name: `${f.name}${cardinalityInfo}${isRecommended ? \" *\" : \"\"}`,\n        value: f.name,\n        checked: isRecommended\n      };\n    })\n    .sort((a, b) => {\n      // Sort: recommended first, then by cardinality\n      const aRec = recommendedFields.includes(a.value) ? 0 : 1;\n      const bRec = recommendedFields.includes(b.value) ? 0 : 1;\n      return aRec - bRec;\n    });\n\n  if (choices.length === 0) {\n    return [];\n  }\n\n  console.log(\"\\n  * = recommended for indexing\");\n  console.log(\"  Controls: <space> toggle, <a> all, <i> invert, <enter> confirm\\n\");\n\n  const selected = await checkbox({\n    message: \"Select fields to index (or none):\",\n    choices,\n    pageSize: 15,\n    instructions: false // We provide our own instructions above\n  });\n\n  return selected;\n}\n\nfunction formatBytes(bytes: number): string {\n  if (bytes < 1024) return `${bytes} B`;\n  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)} KB`;\n  if (bytes < 1024 * 1024 * 1024) return `${(bytes / (1024 * 1024)).toFixed(1)} MB`;\n  return `${(bytes / (1024 * 1024 * 1024)).toFixed(1)} GB`;\n}\n\nfunction formatValue(value: unknown): string {\n  if (value === null) return \"null\";\n  if (value === undefined) return \"undefined\";\n  if (typeof value === \"string\") {\n    if (value.length > 30) {\n      return `\"${value.slice(0, 27)}...\"`;\n    }\n    return `\"${value}\"`;\n  }\n  return String(value);\n}\n","/**\n * Types command - quickly generate TypeScript types from a data file\n * Uses json-ts for accurate type inference from JSON samples\n */\n\nimport * as fs from \"node:fs\";\nimport { json2ts } from \"json-ts\";\nimport type { DataRecord } from \"../../types/index.js\";\nimport { parseFile, streamFile, getFileSize } from \"../utils/parsers.js\";\n\nconst STREAMING_THRESHOLD = 100 * 1024 * 1024;\n\nexport interface TypesOptions {\n  sample?: number;\n  format?: string;\n  output?: string;\n}\n\n/**\n * Post-process json-ts output to clean up interface names\n */\nfunction cleanupTypes(types: string): string {\n  return types\n    // Remove the array type wrapper (we want the item type)\n    .replace(/^type IItem = IItemItem\\[\\];\\n/m, \"\")\n    // Rename IItemItem to Item (both references and declarations)\n    .replace(/IItemItem/g, \"Item\")\n    // Remove I prefix from all interface names (both references and declarations)\n    // This handles IImage_uris -> Image_uris, ILegalities -> Legalities, etc.\n    .replace(/\\bI([A-Z][a-z_]+)/g, \"$1\")\n    // Export all interfaces\n    .replace(/^interface /gm, \"export interface \");\n}\n\n/**\n * Generate field names union type from samples\n */\nfunction generateFieldNamesType(samples: DataRecord[]): string {\n  const fieldNames = new Set<string>();\n  for (const record of samples) {\n    for (const key of Object.keys(record)) {\n      fieldNames.add(key);\n    }\n  }\n  const names = Array.from(fieldNames).sort().map((f) => `\"${f}\"`).join(\" | \");\n  return `export type FieldName = ${names};`;\n}\n\nexport async function types(\n  inputFile: string,\n  options: TypesOptions\n): Promise<void> {\n  // Validate input file exists\n  if (!fs.existsSync(inputFile)) {\n    throw new Error(`Input file not found: ${inputFile}`);\n  }\n\n  const fileSize = getFileSize(inputFile);\n  const sampleSize = options.sample || 1000;\n\n  console.error(`Analyzing: ${inputFile}`);\n  console.error(`Sampling ${sampleSize} records...`);\n\n  let samples: DataRecord[];\n  let format: string;\n\n  if (fileSize > STREAMING_THRESHOLD) {\n    // Use streaming for large files\n    const result = await streamFile(\n      inputFile,\n      options.format,\n      () => {},\n      1000,\n      { sampleOnly: true, sampleSize }\n    );\n    samples = result.sample;\n    format = result.format;\n  } else {\n    // Small file - read directly\n    const parseResult = await parseFile(inputFile, options.format);\n    samples = parseResult.records.slice(0, sampleSize);\n    format = parseResult.format;\n  }\n\n  console.error(`Format: ${format}`);\n  console.error(`Sampled ${samples.length} records\\n`);\n\n  // Generate types using json-ts (limit to 100 samples to avoid memory issues)\n  const typeSamples = samples.slice(0, 100);\n  const rawTypes = json2ts(JSON.stringify(typeSamples), { rootName: \"Item\" });\n  const types = cleanupTypes(rawTypes);\n  const fieldNames = generateFieldNamesType(samples);\n\n  // Generate output\n  const output = `/**\n * Auto-generated TypeScript types\n * Source: ${inputFile}\n * Generated: ${new Date().toISOString()}\n */\n\n${types}\n\n${fieldNames}\n`;\n\n  if (options.output) {\n    await fs.promises.writeFile(options.output, output);\n    console.error(`Types written to: ${options.output}`);\n  } else {\n    // Output to stdout\n    console.log(output);\n  }\n}\n"],"mappings":";;;AAMA,SAAS,eAAe;;;ACDxB,YAAYA,SAAQ;AACpB,YAAY,UAAU;;;ACDtB,YAAY,QAAQ;AACpB,YAAY,cAAc;AAC1B,OAAO,UAAU;AACjB,OAAO,gBAAgB;AACvB,OAAO,uBAAuB;AAC9B,OAAO,iBAAiB;AACxB,OAAO,iBAAiB;AAGxB,IAAM,EAAE,QAAQ,WAAW,IAAI;AAC/B,IAAM,EAAE,MAAM,IAAI;AAClB,IAAM,EAAE,YAAY,IAAI;AAGxB,IAAM,sBAAsB,MAAM,OAAO;AAclC,SAAS,aAAa,UAA8B;AACzD,QAAM,MAAM,SAAS,YAAY,EAAE,MAAM,GAAG,EAAE,IAAI;AAElD,MAAI,QAAQ,MAAO,QAAO;AAC1B,MAAI,QAAQ,YAAY,QAAQ,QAAS,QAAO;AAChD,MAAI,QAAQ,QAAQ;AAElB,UAAM,KAAQ,YAAS,UAAU,GAAG;AACpC,UAAM,SAAS,OAAO,MAAM,IAAI;AAChC,IAAG,YAAS,IAAI,QAAQ,GAAG,MAAM,CAAC;AAClC,IAAG,aAAU,EAAE;AAEf,UAAM,UAAU,OAAO,SAAS,OAAO,EAAE,KAAK;AAC9C,QAAI,QAAQ,WAAW,GAAG,EAAG,QAAO;AACpC,QAAI,QAAQ,WAAW,GAAG,EAAG,QAAO;AAAA,EACtC;AAEA,SAAO;AACT;AAKA,eAAsB,eACpB,UACA,UACuB;AACvB,QAAM,QAAW,YAAS,QAAQ;AAGlC,MAAI,MAAM,OAAO,qBAAqB;AACpC,WAAO,wBAAwB,UAAU,QAAQ;AAAA,EACnD;AAGA,QAAM,UAAU,MAAS,YAAS,SAAS,UAAU,OAAO;AAC5D,QAAM,OAAO,KAAK,MAAM,OAAO;AAE/B,MAAI,CAAC,MAAM,QAAQ,IAAI,GAAG;AACxB,UAAM,IAAI,MAAM,4CAA4C;AAAA,EAC9D;AAEA,MAAI,UAAU;AACZ,SAAK,QAAQ,CAAC,QAAQ,UAAU,SAAS,QAAsB,KAAK,CAAC;AAAA,EACvE;AAEA,SAAO;AACT;AAMA,eAAsB,wBACpB,UACA,UACuB;AACvB,SAAO,IAAI,QAAQ,CAACC,UAAS,WAAW;AACtC,UAAM,UAAwB,CAAC;AAC/B,QAAI,QAAQ;AACZ,QAAI,cAAc,KAAK,IAAI;AAE3B,UAAM,WAAW,MAAM;AAAA,MAClB,oBAAiB,QAAQ;AAAA,MAC5B,WAAW;AAAA,MACX,YAAY;AAAA,IACd,CAAC;AAED,aAAS,GAAG,QAAQ,CAAC,SAA6C;AAChE,YAAM,SAAS,KAAK;AACpB,cAAQ,KAAK,MAAM;AACnB,iBAAW,QAAQ,KAAK;AACxB;AAGA,UAAI,KAAK,IAAI,IAAI,cAAc,KAAM;AACnC,gBAAQ,IAAI,YAAY,MAAM,eAAe,CAAC,aAAa;AAC3D,sBAAc,KAAK,IAAI;AAAA,MACzB;AAAA,IACF,CAAC;AAED,aAAS,GAAG,OAAO,MAAM;AACvB,MAAAA,SAAQ,OAAO;AAAA,IACjB,CAAC;AAED,aAAS,GAAG,SAAS,CAAC,QAAe;AACnC,aAAO,IAAI,MAAM,qBAAqB,IAAI,OAAO,EAAE,CAAC;AAAA,IACtD,CAAC;AAAA,EACH,CAAC;AACH;AAMA,eAAsB,YACpB,UACA,UACuB;AACvB,QAAM,UAAwB,CAAC;AAC/B,MAAI,QAAQ;AAEZ,QAAM,aAAgB,oBAAiB,UAAU,EAAE,UAAU,QAAQ,CAAC;AACtE,QAAM,KAAc,yBAAgB;AAAA,IAClC,OAAO;AAAA,IACP,WAAW;AAAA,EACb,CAAC;AAED,mBAAiB,QAAQ,IAAI;AAC3B,UAAM,UAAU,KAAK,KAAK;AAC1B,QAAI,CAAC,QAAS;AAEd,QAAI;AACF,YAAM,SAAS,KAAK,MAAM,OAAO;AACjC,cAAQ,KAAK,MAAM;AACnB,iBAAW,QAAQ,KAAK;AACxB;AAAA,IACF,SAAS,GAAG;AACV,YAAM,IAAI,MAAM,wBAAwB,QAAQ,CAAC,KAAK,QAAQ,MAAM,GAAG,EAAE,CAAC,KAAK;AAAA,IACjF;AAAA,EACF;AAEA,SAAO;AACT;AAMA,eAAsB,SACpB,UACA,UACuB;AACvB,SAAO,IAAI,QAAQ,CAACA,UAAS,WAAW;AACtC,UAAM,UAAwB,CAAC;AAC/B,QAAI,QAAQ;AAEZ,UAAM,aAAgB,oBAAiB,UAAU,EAAE,UAAU,QAAQ,CAAC;AAEtE,SAAK,MAAM,YAAY;AAAA,MACrB,QAAQ;AAAA,MACR,eAAe;AAAA,MACf,gBAAgB;AAAA,MAChB,MAAM,CAAC,WAAW;AAChB,YAAI,OAAO,OAAO,SAAS,GAAG;AAC5B,iBAAO,IAAI,MAAM,oBAAoB,OAAO,OAAO,CAAC,EAAE,OAAO,EAAE,CAAC;AAChE;AAAA,QACF;AACA,cAAM,SAAS,OAAO;AACtB,gBAAQ,KAAK,MAAM;AACnB,mBAAW,QAAQ,KAAK;AACxB;AAAA,MACF;AAAA,MACA,UAAU,MAAMA,SAAQ,OAAO;AAAA,MAC/B,OAAO,CAAC,UAAU,OAAO,KAAK;AAAA,IAChC,CAAC;AAAA,EACH,CAAC;AACH;AAKA,eAAsB,UACpB,UACA,QACA,UACwD;AACxD,QAAM,iBAAiB,UAAU,aAAa,QAAQ;AAEtD,MAAI;AAEJ,UAAQ,gBAAgB;AAAA,IACtB,KAAK;AACH,gBAAU,MAAM,eAAe,UAAU,QAAQ;AACjD;AAAA,IACF,KAAK;AACH,gBAAU,MAAM,YAAY,UAAU,QAAQ;AAC9C;AAAA,IACF,KAAK;AACH,gBAAU,MAAM,SAAS,UAAU,QAAQ;AAC3C;AAAA,IACF;AACE,YAAM,IAAI,MAAM,uBAAuB,cAAc,EAAE;AAAA,EAC3D;AAEA,SAAO,EAAE,SAAS,QAAQ,eAAe;AAC3C;AAMA,eAAsB,WACpB,UACA,QACA,UACA,aAAqB,KACrB,UAAyB,CAAC,GACqE;AAC/F,QAAM,iBAAiB,UAAU,aAAa,QAAQ;AACtD,QAAM,SAAuB,CAAC;AAC9B,MAAI,QAAQ;AACZ,MAAI,iBAAiB;AACrB,QAAM,WAAW,YAAY,QAAQ;AAGrC,MAAI,cAA4C;AAChD,MAAI,QAAQ,gBAAgB,CAAC,QAAQ,YAAY;AAC/C,kBAAc,IAAI,YAAY,UAAU;AAAA,MACtC,QAAQ;AAAA,MACR,iBAAiB;AAAA,MACjB,mBAAmB;AAAA,MACnB,YAAY;AAAA,IACd,GAAG,YAAY,QAAQ,cAAc;AACrC,gBAAY,MAAM,KAAK,MAAM,YAAY,OAAO,KAAK,GAAG,GAAG,EAAE,SAAS,EAAE,CAAC;AAAA,EAC3E;AAEA,QAAM,YAAY,CAAC,QAAoB,OAAe,UAAmB;AACvE,QAAI,OAAO,SAAS,YAAY;AAC9B,aAAO,KAAK,MAAM;AAAA,IACpB;AACA;AACA,QAAI,MAAO,kBAAiB;AAG5B,QAAI,eAAe,QAAQ,QAAS,GAAG;AACrC,kBAAY,OAAO,KAAK,MAAM,kBAAkB,OAAO,KAAK,GAAG,EAAE,SAAS,MAAM,eAAe,EAAE,CAAC;AAAA,IACpG;AAEA,aAAS,QAAQ,KAAK;AAAA,EACxB;AAGA,MAAI,QAAQ,YAAY;AACtB,YAAQ,gBAAgB;AAAA,MACtB,KAAK;AACH,cAAM,sBAAsB,UAAU,WAAW,UAAU;AAC3D;AAAA,MACF,KAAK;AACH,cAAM,mBAAmB,UAAU,WAAW,UAAU;AACxD;AAAA,MACF,KAAK;AACH,cAAM,gBAAgB,UAAU,WAAW,UAAU;AACrD;AAAA,MACF;AACE,cAAM,IAAI,MAAM,uBAAuB,cAAc,EAAE;AAAA,IAC3D;AAGA,UAAM,gBAAgB,iBAAiB;AACvC,UAAM,iBAAiB,KAAK,MAAM,WAAW,aAAa;AAE1D,WAAO,EAAE,OAAO,QAAQ,QAAQ,gBAAgB,eAAe;AAAA,EACjE;AAEA,UAAQ,gBAAgB;AAAA,IACtB,KAAK;AACH,YAAM,gBAAgB,UAAU,WAAW,QAAQ,YAAY;AAC/D;AAAA,IACF,KAAK;AACH,YAAM,aAAa,UAAU,WAAW,QAAQ,YAAY;AAC5D;AAAA,IACF,KAAK;AACH,YAAM,UAAU,UAAU,WAAW,QAAQ,YAAY;AACzD;AAAA,IACF;AACE,YAAM,IAAI,MAAM,uBAAuB,cAAc,EAAE;AAAA,EAC3D;AAEA,MAAI,aAAa;AACf,gBAAY,OAAO,KAAK,MAAM,YAAY,OAAO,KAAK,GAAG,EAAE,SAAS,MAAM,eAAe,EAAE,CAAC;AAC5F,gBAAY,KAAK;AAAA,EACnB;AAEA,SAAO,EAAE,OAAO,QAAQ,QAAQ,eAAe;AACjD;AAKA,eAAe,gBACb,UACA,UACA,cACe;AACf,SAAO,IAAI,QAAQ,CAACA,UAAS,WAAW;AACtC,QAAI,QAAQ;AACZ,QAAI,YAAY;AAEhB,UAAM,aAAgB,oBAAiB,QAAQ;AAC/C,eAAW,GAAG,QAAQ,CAAC,UAAkB;AACvC,mBAAa,MAAM;AAAA,IACrB,CAAC;AAED,UAAM,WAAW,MAAM;AAAA,MACrB;AAAA,MACA,WAAW;AAAA,MACX,YAAY;AAAA,IACd,CAAC;AAED,aAAS,GAAG,QAAQ,CAAC,SAA6C;AAChE,eAAS,KAAK,OAAO,OAAO,SAAS;AACrC;AAAA,IACF,CAAC;AAED,aAAS,GAAG,OAAO,MAAMA,SAAQ,CAAC;AAClC,aAAS,GAAG,SAAS,CAAC,QAAe;AACnC,aAAO,IAAI,MAAM,qBAAqB,IAAI,OAAO,EAAE,CAAC;AAAA,IACtD,CAAC;AAAA,EACH,CAAC;AACH;AAKA,eAAe,sBACb,UACA,UACA,YACe;AACf,SAAO,IAAI,QAAQ,CAACA,UAAS,WAAW;AACtC,QAAI,QAAQ;AACZ,QAAI,YAAY;AAEhB,UAAM,aAAgB,oBAAiB,QAAQ;AAC/C,eAAW,GAAG,QAAQ,CAAC,UAAkB;AACvC,mBAAa,MAAM;AAAA,IACrB,CAAC;AAED,UAAM,WAAW,MAAM;AAAA,MACrB;AAAA,MACA,WAAW;AAAA,MACX,YAAY;AAAA,IACd,CAAC;AAED,aAAS,GAAG,QAAQ,CAAC,SAA6C;AAChE,eAAS,KAAK,OAAO,OAAO,SAAS;AACrC;AAEA,UAAI,SAAS,YAAY;AACvB,mBAAW,QAAQ;AACnB,QAAAA,SAAQ;AAAA,MACV;AAAA,IACF,CAAC;AAED,aAAS,GAAG,OAAO,MAAMA,SAAQ,CAAC;AAClC,aAAS,GAAG,SAAS,CAAC,QAAe;AAEnC,UAAI,IAAI,QAAQ,SAAS,SAAS,KAAK,IAAI,QAAQ,SAAS,WAAW,GAAG;AACxE,QAAAA,SAAQ;AAAA,MACV,OAAO;AACL,eAAO,IAAI,MAAM,qBAAqB,IAAI,OAAO,EAAE,CAAC;AAAA,MACtD;AAAA,IACF,CAAC;AAAA,EACH,CAAC;AACH;AAKA,eAAe,aACb,UACA,UACA,cACe;AACf,MAAI,QAAQ;AACZ,MAAI,YAAY;AAEhB,QAAM,aAAgB,oBAAiB,UAAU,EAAE,UAAU,QAAQ,CAAC;AACtE,aAAW,GAAG,QAAQ,CAAC,UAAkB;AACvC,iBAAa,OAAO,WAAW,KAAK;AAAA,EACtC,CAAC;AAED,QAAM,KAAc,yBAAgB;AAAA,IAClC,OAAO;AAAA,IACP,WAAW;AAAA,EACb,CAAC;AAED,mBAAiB,QAAQ,IAAI;AAC3B,UAAM,UAAU,KAAK,KAAK;AAC1B,QAAI,CAAC,QAAS;AAEd,QAAI;AACF,YAAM,SAAS,KAAK,MAAM,OAAO;AACjC,eAAS,QAAQ,OAAO,SAAS;AACjC;AAAA,IACF,SAAS,GAAG;AACV,YAAM,IAAI,MAAM,wBAAwB,QAAQ,CAAC,KAAK,QAAQ,MAAM,GAAG,EAAE,CAAC,KAAK;AAAA,IACjF;AAAA,EACF;AACF;AAKA,eAAe,mBACb,UACA,UACA,YACe;AACf,MAAI,QAAQ;AACZ,MAAI,YAAY;AAEhB,QAAM,aAAgB,oBAAiB,UAAU,EAAE,UAAU,QAAQ,CAAC;AACtE,aAAW,GAAG,QAAQ,CAAC,UAAkB;AACvC,iBAAa,OAAO,WAAW,KAAK;AAAA,EACtC,CAAC;AAED,QAAM,KAAc,yBAAgB;AAAA,IAClC,OAAO;AAAA,IACP,WAAW;AAAA,EACb,CAAC;AAED,mBAAiB,QAAQ,IAAI;AAC3B,UAAM,UAAU,KAAK,KAAK;AAC1B,QAAI,CAAC,QAAS;AAEd,QAAI;AACF,YAAM,SAAS,KAAK,MAAM,OAAO;AACjC,eAAS,QAAQ,OAAO,SAAS;AACjC;AAEA,UAAI,SAAS,YAAY;AACvB,WAAG,MAAM;AACT,mBAAW,QAAQ;AACnB;AAAA,MACF;AAAA,IACF,SAAS,GAAG;AACV,YAAM,IAAI,MAAM,wBAAwB,QAAQ,CAAC,KAAK,QAAQ,MAAM,GAAG,EAAE,CAAC,KAAK;AAAA,IACjF;AAAA,EACF;AACF;AAKA,eAAe,UACb,UACA,UACA,cACe;AACf,SAAO,IAAI,QAAQ,CAACA,UAAS,WAAW;AACtC,QAAI,QAAQ;AACZ,QAAI,YAAY;AAEhB,UAAM,aAAgB,oBAAiB,UAAU,EAAE,UAAU,QAAQ,CAAC;AACtE,eAAW,GAAG,QAAQ,CAAC,UAAkB;AACvC,mBAAa,OAAO,WAAW,KAAK;AAAA,IACtC,CAAC;AAED,SAAK,MAAM,YAAY;AAAA,MACrB,QAAQ;AAAA,MACR,eAAe;AAAA,MACf,gBAAgB;AAAA,MAChB,MAAM,CAAC,WAAW;AAChB,YAAI,OAAO,OAAO,SAAS,GAAG;AAC5B,iBAAO,IAAI,MAAM,oBAAoB,OAAO,OAAO,CAAC,EAAE,OAAO,EAAE,CAAC;AAChE;AAAA,QACF;AACA,iBAAS,OAAO,MAAoB,OAAO,SAAS;AACpD;AAAA,MACF;AAAA,MACA,UAAU,MAAMA,SAAQ;AAAA,MACxB,OAAO,CAAC,UAAU,OAAO,KAAK;AAAA,IAChC,CAAC;AAAA,EACH,CAAC;AACH;AAKA,eAAe,gBACb,UACA,UACA,YACe;AACf,SAAO,IAAI,QAAQ,CAACA,UAAS,WAAW;AACtC,QAAI,QAAQ;AACZ,QAAI,YAAY;AAChB,QAAI,WAAW;AAEf,UAAM,aAAgB,oBAAiB,UAAU,EAAE,UAAU,QAAQ,CAAC;AACtE,eAAW,GAAG,QAAQ,CAAC,UAAkB;AACvC,mBAAa,OAAO,WAAW,KAAK;AAAA,IACtC,CAAC;AAED,SAAK,MAAM,YAAY;AAAA,MACrB,QAAQ;AAAA,MACR,eAAe;AAAA,MACf,gBAAgB;AAAA,MAChB,MAAM,CAAC,QAAQ,WAAW;AACxB,YAAI,SAAU;AAEd,YAAI,OAAO,OAAO,SAAS,GAAG;AAC5B,iBAAO,IAAI,MAAM,oBAAoB,OAAO,OAAO,CAAC,EAAE,OAAO,EAAE,CAAC;AAChE;AAAA,QACF;AACA,iBAAS,OAAO,MAAoB,OAAO,SAAS;AACpD;AAEA,YAAI,SAAS,YAAY;AACvB,qBAAW;AACX,iBAAO,MAAM;AACb,qBAAW,QAAQ;AACnB,UAAAA,SAAQ;AAAA,QACV;AAAA,MACF;AAAA,MACA,UAAU,MAAM;AACd,YAAI,CAAC,SAAU,CAAAA,SAAQ;AAAA,MACzB;AAAA,MACA,OAAO,CAAC,UAAU,OAAO,KAAK;AAAA,IAChC,CAAC;AAAA,EACH,CAAC;AACH;AAKO,SAAS,YAAY,UAA0B;AACpD,QAAM,QAAW,YAAS,QAAQ;AAClC,SAAO,MAAM;AACf;AAKO,SAAS,UAAU,SAAyB;AACjD,QAAM,QAAQ,QAAQ,YAAY,EAAE,MAAM,mCAAmC;AAC7E,MAAI,CAAC,OAAO;AACV,UAAM,IAAI,MAAM,wBAAwB,OAAO,kCAAkC;AAAA,EACnF;AAEA,QAAM,QAAQ,WAAW,MAAM,CAAC,CAAC;AACjC,QAAM,OAAO,MAAM,CAAC,KAAK;AAEzB,QAAM,cAAsC;AAAA,IAC1C,GAAG;AAAA,IACH,IAAI;AAAA,IACJ,IAAI,OAAO;AAAA,IACX,IAAI,OAAO,OAAO;AAAA,EACpB;AAEA,SAAO,KAAK,MAAM,QAAQ,YAAY,IAAI,CAAC;AAC7C;;;ACvjBA,SAAS,UAAU,OAA2B;AAC5C,MAAI,UAAU,QAAQ,UAAU,QAAW;AACzC,WAAO;AAAA,EACT;AAEA,QAAM,OAAO,OAAO;AAEpB,MAAI,SAAS,UAAU;AAErB,QAAI,aAAa,KAAe,GAAG;AACjC,aAAO;AAAA,IACT;AACA,WAAO;AAAA,EACT;AAEA,MAAI,SAAS,UAAU;AACrB,WAAO;AAAA,EACT;AAEA,MAAI,SAAS,WAAW;AACtB,WAAO;AAAA,EACT;AAGA,SAAO;AACT;AAKA,SAAS,aAAa,OAAwB;AAE5C,QAAM,aAAa;AACnB,MAAI,WAAW,KAAK,KAAK,GAAG;AAC1B,UAAM,OAAO,IAAI,KAAK,KAAK;AAC3B,WAAO,CAAC,MAAM,KAAK,QAAQ,CAAC;AAAA,EAC9B;AACA,SAAO;AACT;AAKA,SAAS,WAAW,OAAkB,OAA6B;AACjE,MAAI,UAAU,MAAO,QAAO;AAC5B,MAAI,UAAU,OAAQ,QAAO;AAC7B,MAAI,UAAU,OAAQ,QAAO;AAG7B,SAAO;AACT;AAKA,IAAM,sBAAN,MAA0B;AAAA,EAChB,SAAS,oBAAI,IAAY;AAAA,EACzB;AAAA,EACA;AAAA,EACA,YAAY;AAAA,EACZ,QAAQ;AAAA,EACR,OAAkB;AAAA,EAClB,eAAqD,CAAC;AAAA,EAE9D,IAAI,OAAsB;AACxB,SAAK;AAEL,QAAI,UAAU,QAAQ,UAAU,QAAW;AACzC,WAAK;AACL;AAAA,IACF;AAEA,UAAM,YAAY,UAAU,KAAK;AACjC,SAAK,OAAO,WAAW,KAAK,MAAM,SAAS;AAG3C,QAAI,KAAK,OAAO,OAAO,KAAO;AAC5B,WAAK,OAAO,IAAI,OAAO,KAAK,CAAC;AAAA,IAC/B;AAGA,QAAI,KAAK,aAAa,SAAS,KAAK,CAAC,KAAK,aAAa,SAAS,KAAyC,GAAG;AAC1G,WAAK,aAAa,KAAK,KAAyC;AAAA,IAClE;AAGA,QAAI,OAAO,UAAU,UAAU;AAC7B,UAAI,KAAK,QAAQ,UAAa,QAAS,KAAK,KAAgB;AAC1D,aAAK,MAAM;AAAA,MACb;AACA,UAAI,KAAK,QAAQ,UAAa,QAAS,KAAK,KAAgB;AAC1D,aAAK,MAAM;AAAA,MACb;AAAA,IACF,WAAW,OAAO,UAAU,aAAa,KAAK,SAAS,UAAU,KAAK,SAAS,WAAW;AACxF,UAAI,KAAK,QAAQ,UAAa,QAAQ,KAAK,KAAK;AAC9C,aAAK,MAAM;AAAA,MACb;AACA,UAAI,KAAK,QAAQ,UAAa,QAAQ,KAAK,KAAK;AAC9C,aAAK,MAAM;AAAA,MACb;AAAA,IACF;AAAA,EACF;AAAA,EAEA,WAAuB;AACrB,WAAO;AAAA,MACL,KAAK,KAAK;AAAA,MACV,KAAK,KAAK;AAAA,MACV,aAAa,KAAK,OAAO;AAAA,MACzB,WAAW,KAAK;AAAA,MAChB,cAAc,KAAK;AAAA,IACrB;AAAA,EACF;AAAA,EAEA,UAAqB;AACnB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,aAAsB;AACpB,WAAO,KAAK,YAAY;AAAA,EAC1B;AACF;AAKA,SAAS,YACP,MACA,MACA,aACA,cACA,OACS;AAET,MAAI,eAAe,GAAG;AACpB,WAAO;AAAA,EACT;AAGA,MAAI,cAAc,eAAe,KAAK;AACpC,WAAO;AAAA,EACT;AAGA,MAAI,cAAc,KAAM;AACtB,WAAO;AAAA,EACT;AAGA,QAAM,UAAU,MAAM,gBAAgB,CAAC;AACvC,aAAW,UAAU,SAAS;AAC5B,QAAI,WAAW,QAAQ,WAAW,OAAW;AAC7C,UAAM,MAAM,OAAO,MAAM;AAGzB,QAAI,IAAI,WAAW,UAAU,KAAK,QAAQ,mBAAmB;AAC3D,aAAO;AAAA,IACT;AAGA,QAAI,IAAI,SAAS,KAAK,GAAG;AACvB,aAAO;AAAA,IACT;AAGA,QAAI,IAAI,SAAS,KAAK;AACpB,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,YAAY,KAAK,YAAY;AACnC,QAAM,eAAe;AAAA,IACnB;AAAA,IAAQ;AAAA,IAAQ;AAAA;AAAA,IAChB;AAAA,IAAO;AAAA,IAAO;AAAA,IAAQ;AAAA;AAAA,IACtB;AAAA,IAAQ;AAAA,IAAS;AAAA,IAAU;AAAA;AAAA,IAC3B;AAAA,IAAe;AAAA,IAAQ;AAAA,IAAW;AAAA;AAAA,EACpC;AAGA,QAAM,gBAAgB;AAAA,IACpB;AAAA,IAAY;AAAA,IAAQ;AAAA,IAAU;AAAA,IAAS;AAAA,IACvC;AAAA,IAAS;AAAA,IAAQ;AAAA,IAAU;AAAA,IAAO;AAAA,EACpC;AAEA,QAAM,iBAAiB,aAAa,KAAK,OAAK,UAAU,SAAS,CAAC,CAAC;AACnE,QAAM,kBAAkB,cAAc,KAAK,OAAK,UAAU,SAAS,CAAC,CAAC;AAErE,MAAI,kBAAkB,CAAC,iBAAiB;AACtC,WAAO;AAAA,EACT;AAIA,MAAI,SAAS,WAAW;AACtB,WAAO,eAAe;AAAA,EACxB;AAGA,SAAO,eAAe,KAAK,eAAe;AAC5C;AAKO,SAAS,YAAY,SAA+B;AACzD,MAAI,QAAQ,WAAW,GAAG;AACxB,WAAO,EAAE,QAAQ,CAAC,GAAG,cAAc,KAAK;AAAA,EAC1C;AAGA,QAAM,aAAa,oBAAI,IAAY;AACnC,aAAW,UAAU,SAAS;AAC5B,eAAW,OAAO,OAAO,KAAK,MAAM,GAAG;AACrC,iBAAW,IAAI,GAAG;AAAA,IACpB;AAAA,EACF;AAGA,QAAM,aAAa,oBAAI,IAAiC;AACxD,aAAW,QAAQ,YAAY;AAC7B,eAAW,IAAI,MAAM,IAAI,oBAAoB,CAAC;AAAA,EAChD;AAEA,aAAW,UAAU,SAAS;AAC5B,eAAW,QAAQ,YAAY;AAC7B,YAAM,YAAY,WAAW,IAAI,IAAI;AACrC,gBAAU,IAAI,OAAO,IAAI,CAAC;AAAA,IAC5B;AAAA,EACF;AAGA,QAAM,SAAwB,CAAC;AAC/B,MAAI,eAA8B;AAElC,aAAW,QAAQ,YAAY;AAC7B,UAAM,YAAY,WAAW,IAAI,IAAI;AACrC,UAAM,QAAQ,UAAU,SAAS;AACjC,UAAM,OAAO,UAAU,QAAQ;AAG/B,UAAM,cAAc,MAAM;AAC1B,UAAM,UAAU,YAAY,MAAM,MAAM,aAAa,QAAQ,QAAQ,KAAK;AAI1E,QACE,iBAAiB,QACjB,MAAM,gBAAgB,QAAQ,UAC9B,CAAC,UAAU,WAAW,MACrB,KAAK,YAAY,EAAE,SAAS,IAAI,KAAK,KAAK,YAAY,MAAM,QAC7D;AACA,qBAAe;AAAA,IACjB;AAEA,WAAO,KAAK;AAAA,MACV;AAAA,MACA;AAAA,MACA,UAAU,UAAU,WAAW;AAAA,MAC/B;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH;AAGA,SAAO,KAAK,CAAC,GAAG,MAAM;AACpB,QAAI,EAAE,SAAS,aAAc,QAAO;AACpC,QAAI,EAAE,SAAS,aAAc,QAAO;AACpC,WAAO,EAAE,KAAK,cAAc,EAAE,IAAI;AAAA,EACpC,CAAC;AAED,SAAO,EAAE,QAAQ,aAAa;AAChC;AAKO,SAAS,kBAAkB,QAAgB,SAAsC;AAEtF,MAAI,OAAO,cAAc;AACvB,WAAO,OAAO;AAAA,EAChB;AAMA,MAAI,YAA2B;AAC/B,MAAI,YAAY;AAEhB,aAAW,SAAS,OAAO,QAAQ;AACjC,QAAI,MAAM,SAAU;AAEpB,QAAI,QAAQ;AAGZ,QAAI,MAAM,SAAS,YAAY,MAAM,SAAS,QAAQ;AACpD,eAAS;AAAA,IACX;AAGA,UAAM,mBAAmB,MAAM,MAAM,cAAc,QAAQ;AAC3D,QAAI,mBAAmB,OAAO,oBAAoB,GAAG;AACnD,eAAS,mBAAmB;AAAA,IAC9B;AAGA,UAAM,YAAY,MAAM,KAAK,YAAY;AACzC,QAAI,UAAU,SAAS,IAAI,EAAG,UAAS;AACvC,QAAI,UAAU,SAAS,MAAM,KAAK,UAAU,SAAS,MAAM,EAAG,UAAS;AACvE,QAAI,UAAU,SAAS,SAAS,KAAK,UAAU,SAAS,SAAS,EAAG,UAAS;AAE7E,QAAI,QAAQ,WAAW;AACrB,kBAAY;AACZ,kBAAY,MAAM;AAAA,IACpB;AAAA,EACF;AAEA,SAAO;AACT;AAKO,SAAS,mBAAmB,QAA0B;AAC3D,SAAO,OAAO,OACX,OAAO,CAAC,MAAM,EAAE,OAAO,EACvB,IAAI,CAAC,MAAM,EAAE,IAAI;AACtB;;;AC5TA,SAAS,mBAAmB,QAA4B;AACtD,SAAO,KAAK,UAAU,MAAM,EAAE;AAChC;AAKA,SAAS,cAAc,GAAY,GAAoB;AACrD,MAAI,MAAM,EAAG,QAAO;AACpB,MAAI,MAAM,QAAQ,MAAM,OAAW,QAAO;AAC1C,MAAI,MAAM,QAAQ,MAAM,OAAW,QAAO;AAE1C,MAAI,OAAO,MAAM,YAAY,OAAO,MAAM,UAAU;AAClD,WAAO,IAAI;AAAA,EACb;AAEA,SAAO,OAAO,CAAC,EAAE,cAAc,OAAO,CAAC,CAAC;AAC1C;AAKO,SAAS,aACd,SACA,QACA,SACS;AACT,QAAM,EAAE,YAAY,QAAQ,IAAI;AAGhC,MAAI,gBAAgB;AACpB,MAAI,SAAS;AACX,oBAAgB,CAAC,GAAG,OAAO,EAAE;AAAA,MAAK,CAAC,GAAG,MACpC,cAAc,EAAE,OAAO,GAAG,EAAE,OAAO,CAAC;AAAA,IACtC;AAAA,EACF;AAEA,QAAM,SAAkB,CAAC;AACzB,MAAI,eAA6B,CAAC;AAClC,MAAI,cAAc;AAClB,MAAI,aAAa;AAGjB,QAAM,gBAAgB;AACtB,QAAM,gBAAgB;AAEtB,aAAW,UAAU,eAAe;AAClC,UAAM,aAAa,mBAAmB,MAAM,IAAI;AAIhD,QAAI,cAAc,aAAa,cAAc,aAAa,SAAS,GAAG;AACpE,aAAO,KAAK;AAAA,QACV,IAAI,OAAO,UAAU;AAAA,QACrB,SAAS;AAAA,QACT,UAAU,cAAc;AAAA,MAC1B,CAAC;AACD;AACA,qBAAe,CAAC;AAChB,oBAAc;AAAA,IAChB;AAEA,iBAAa,KAAK,MAAM;AACxB,mBAAe;AAAA,EACjB;AAGA,MAAI,aAAa,SAAS,GAAG;AAC3B,WAAO,KAAK;AAAA,MACV,IAAI,OAAO,UAAU;AAAA,MACrB,SAAS;AAAA,MACT,UAAU,cAAc;AAAA,IAC1B,CAAC;AAAA,EACH;AAEA,SAAO;AACT;AAKO,SAAS,qBACd,SACA,QAC0B;AAC1B,QAAM,SAAmC,CAAC;AAE1C,aAAW,SAAS,OAAO,QAAQ;AACjC,UAAM,SAAS,QACZ,IAAI,CAAC,MAAM,EAAE,MAAM,IAAI,CAAC,EACxB,OAAO,CAAC,MAAM,MAAM,QAAQ,MAAM,MAAS;AAE9C,QAAI,OAAO,WAAW,GAAG;AACvB;AAAA,IACF;AAGA,QAAI,MAAM,SAAS,YAAY,MAAM,SAAS,UAAU,MAAM,SAAS,UAAU;AAC/E,YAAM,SAAS,CAAC,GAAG,MAAM,EAAE,KAAK,aAAa;AAC7C,aAAO,MAAM,IAAI,IAAI;AAAA,QACnB,KAAK,OAAO,CAAC;AAAA,QACb,KAAK,OAAO,OAAO,SAAS,CAAC;AAAA,MAC/B;AAAA,IACF;AAAA,EACF;AAEA,SAAO;AACT;AAKO,SAAS,kBACd,OACA,QACA,UACW;AACX,SAAO;AAAA,IACL,IAAI,MAAM;AAAA,IACV,MAAM,GAAG,QAAQ,IAAI,MAAM,EAAE;AAAA,IAC7B,OAAO,MAAM,QAAQ;AAAA,IACrB,UAAU,MAAM;AAAA,IAChB,aAAa,qBAAqB,MAAM,SAAS,MAAM;AAAA,EACzD;AACF;AAKO,SAAS,eAAe,OAAsB;AACnD,SAAO,KAAK,UAAU,MAAM,OAAO;AACrC;;;ACvIO,SAAS,qBACd,QACA,QACA,eACe;AACf,QAAM,UAAyB,CAAC;AAGhC,QAAM,gBAAgB,gBAClB,OAAO,OAAO,OAAO,CAAC,MAAM,cAAc,SAAS,EAAE,IAAI,CAAC,IAC1D,OAAO,OAAO,OAAO,CAAC,MAAM,EAAE,OAAO;AAEzC,aAAW,SAAS,eAAe;AACjC,UAAM,aAAuC,CAAC;AAE9C,eAAW,SAAS,QAAQ;AAC1B,YAAM,gBAAgB,oBAAI,IAAY;AAEtC,iBAAW,UAAU,MAAM,SAAS;AAClC,cAAM,QAAQ,OAAO,MAAM,IAAI;AAC/B,YAAI,UAAU,QAAQ,UAAU,QAAW;AACzC,wBAAc,IAAI,OAAO,KAAK,CAAC;AAAA,QACjC;AAAA,MACF;AAGA,iBAAW,SAAS,eAAe;AACjC,YAAI,CAAC,WAAW,KAAK,GAAG;AACtB,qBAAW,KAAK,IAAI,CAAC;AAAA,QACvB;AACA,mBAAW,KAAK,EAAE,KAAK,MAAM,EAAE;AAAA,MACjC;AAAA,IACF;AAIA,UAAM,eAAe,OAAO,KAAK,UAAU,EAAE;AAC7C,QAAI,gBAAgB,KAAO;AACzB,cAAQ,MAAM,IAAI,IAAI;AAAA,IACxB;AAAA,EACF;AAEA,SAAO;AACT;;;ACtDA,SAAS,eAAe;AAMxB,SAAS,aAAaC,QAAuB;AAC3C,SAAOA,OACJ,QAAQ,mCAAmC,EAAE,EAC7C,QAAQ,cAAc,MAAM,EAC5B,QAAQ,sBAAsB,IAAI,EAClC,QAAQ,iBAAiB,mBAAmB;AACjD;AAKA,SAAS,uBAAuB,QAAwB;AACtD,QAAM,QAAQ,OAAO,OAAO,IAAI,CAAC,MAAM,IAAI,EAAE,IAAI,GAAG,EAAE,KAAK,KAAK;AAChE,SAAO,2BAA2B,KAAK;AACzC;AAKA,SAAS,mBAAmB,QAAwB;AAClD,SAAO;AAAA,EACP,OAAO,OACN,IAAI,CAAC,MAAM;AACV,QAAI,EAAE,SAAS,UAAU;AACvB,aAAO,KAAK,EAAE,IAAI;AAAA,IACpB,WAAW,EAAE,SAAS,WAAW;AAC/B,aAAO,KAAK,EAAE,IAAI;AAAA,IACpB,OAAO;AACL,aAAO,KAAK,EAAE,IAAI;AAAA,IACpB;AAAA,EACF,CAAC,EACA,KAAK,IAAI,CAAC;AAAA;AAEb;AAKO,SAAS,eACd,QACA,UACA,SACQ;AAER,QAAM,WAAW,QAAQ,KAAK,UAAU,OAAO,GAAG,EAAE,UAAU,OAAO,CAAC;AACtE,QAAM,gBAAgB,aAAa,QAAQ;AAE3C,QAAM,iBAAiB,uBAAuB,MAAM;AACpD,QAAM,aAAa,mBAAmB,MAAM;AAE5C,QAAM,iBAAiB,OAAO,OAC3B,OAAO,CAAC,MAAM,EAAE,SAAS,YAAY,EAAE,SAAS,YAAY,EAAE,SAAS,MAAM,EAC7E,IAAI,CAAC,MAAM,IAAI,EAAE,IAAI,GAAG,EACxB,KAAK,KAAK;AAEb,SAAO;AAAA;AAAA,mBAEU,SAAS,WAAW;AAAA,oBACnB,SAAS,YAAY;AAAA,aAC5B,SAAS,OAAO,MAAM;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAgBjC,aAAa;AAAA;AAAA,EAEb,cAAc;AAAA;AAAA,EAEd,UAAU;AAAA;AAAA,8BAEkB,kBAAkB,QAAQ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAiBxD;;;AL/FA,IAAM,UAAU;AAGhB,IAAMC,uBAAsB,MAAM,OAAO;AASzC,eAAsB,MACpB,WACA,SACsB;AACtB,QAAM,YAAY,KAAK,IAAI;AAG3B,MAAI,CAAI,eAAW,SAAS,GAAG;AAC7B,UAAM,IAAI,MAAM,yBAAyB,SAAS,EAAE;AAAA,EACtD;AAEA,QAAM,WAAW,YAAY,SAAS;AACtC,UAAQ,IAAI,UAAU,SAAS,KAAK,YAAY,QAAQ,CAAC,GAAG;AAG5D,MAAI,WAAWA,sBAAqB;AAClC,YAAQ,IAAI,wCAAwC;AACpD,WAAO,eAAe,WAAW,SAAS,SAAS;AAAA,EACrD;AAEA,SAAO,cAAc,WAAW,SAAS,SAAS;AACpD;AAKA,eAAe,cACb,WACA,SACA,WACsB;AACtB,UAAQ,IAAI,iBAAiB;AAG7B,QAAM,EAAE,SAAS,OAAO,IAAI,MAAM,UAAU,WAAW,QAAQ,MAAM;AACrE,UAAQ,IAAI,UAAU,QAAQ,OAAO,eAAe,CAAC,qBAAqB,MAAM,GAAG;AAEnF,MAAI,QAAQ,WAAW,GAAG;AACxB,UAAM,IAAI,MAAM,gCAAgC;AAAA,EAClD;AAGA,UAAQ,IAAI,qBAAqB;AACjC,QAAM,SAAS,YAAY,OAAO;AAClC,UAAQ,IAAI,SAAS,OAAO,OAAO,MAAM,SAAS;AAElD,MAAI,OAAO,cAAc;AACvB,YAAQ,IAAI,2BAA2B,OAAO,YAAY,EAAE;AAAA,EAC9D;AAGA,QAAM,UAAU,QAAQ,WAAW,kBAAkB,QAAQ,OAAO;AACpE,MAAI,SAAS;AACX,YAAQ,IAAI,sBAAsB,OAAO,EAAE;AAAA,EAC7C;AAGA,QAAM,kBAAkB,UAAU,QAAQ,SAAS;AACnD,UAAQ,IAAI,sBAAsB,YAAY,eAAe,CAAC,EAAE;AAGhE,QAAM,wBAAwB;AAC9B,MAAI;AACJ,MAAI,QAAQ,OAAO;AACjB,oBAAgB,QAAQ,MAAM,MAAM,GAAG,EAAE,IAAI,CAAC,MAAM,EAAE,KAAK,CAAC;AAAA,EAC9D,OAAO;AACL,UAAM,cAAc,OAAO,OAAO,OAAO,CAAC,MAAM,EAAE,OAAO,EAAE,IAAI,CAAC,MAAM,EAAE,IAAI;AAC5E,QAAI,YAAY,SAAS,uBAAuB;AAC9C,cAAQ,IAAI,YAAY,YAAY,MAAM,2CAA2C,qBAAqB,EAAE;AAC5G,cAAQ,IAAI,8CAA8C;AAC1D,sBAAgB,YAAY,MAAM,GAAG,qBAAqB;AAAA,IAC5D,OAAO;AACL,sBAAgB;AAAA,IAClB;AAAA,EACF;AAEA,MAAI,cAAc,SAAS,GAAG;AAC5B,YAAQ,IAAI,oBAAoB,cAAc,KAAK,IAAI,CAAC,EAAE;AAAA,EAC5D;AAGA,aAAW,SAAS,OAAO,QAAQ;AACjC,UAAM,UAAU,cAAc,SAAS,MAAM,IAAI;AAAA,EACnD;AAGA,UAAQ,IAAI,kBAAkB;AAC9B,QAAM,SAAS,aAAa,SAAS,QAAQ;AAAA,IAC3C,YAAY;AAAA,IACZ;AAAA,EACF,CAAC;AACD,UAAQ,IAAI,WAAW,OAAO,MAAM,SAAS;AAG7C,UAAQ,IAAI,qBAAqB;AACjC,QAAM,UAAU,qBAAqB,QAAQ,QAAQ,aAAa;AAClE,UAAQ,IAAI,qBAAqB,OAAO,KAAK,OAAO,EAAE,MAAM,SAAS;AAGrE,QAAM,YAAiB,aAAQ,QAAQ,MAAM;AAC7C,QAAM,YAAiB,UAAK,WAAW,QAAQ;AAC/C,QAAS,aAAS,MAAM,WAAW,EAAE,WAAW,KAAK,CAAC;AAGtD,UAAQ,IAAI,mBAAmB;AAC/B,QAAM,aAAa,CAAC;AACpB,aAAW,SAAS,QAAQ;AAC1B,UAAM,YAAiB,UAAK,WAAW,GAAG,MAAM,EAAE,OAAO;AACzD,UAAS,aAAS,UAAU,WAAW,eAAe,KAAK,CAAC;AAC5D,eAAW,KAAK,kBAAkB,OAAO,QAAQ,QAAQ,CAAC;AAAA,EAC5D;AAGA,QAAM,SAAsB;AAAA,IAC1B,WAAW;AAAA,IACX,SAAS,WAAW;AAAA,IACpB;AAAA,EACF;AAGA,QAAM,WAAqB;AAAA,IACzB,SAAS;AAAA,IACT,cAAa,oBAAI,KAAK,GAAE,YAAY;AAAA,IACpC;AAAA,IACA,QAAQ;AAAA,IACR;AAAA,IACA,cAAc,QAAQ;AAAA,IACtB;AAAA,EACF;AAGA,QAAM,eAAoB,UAAK,WAAW,eAAe;AACzD,QAAS,aAAS,UAAU,cAAc,KAAK,UAAU,UAAU,MAAM,CAAC,CAAC;AAC3E,UAAQ,IAAI,qBAAqB,YAAY,EAAE;AAG/C,UAAQ,IAAI,sBAAsB;AAClC,QAAM,iBAAiB,QAAQ,MAAM,GAAG,GAAG;AAC3C,QAAM,aAAa,eAAe,QAAQ,UAAU,cAAc;AAClE,QAAM,aAAkB,UAAK,WAAW,WAAW;AACnD,QAAS,aAAS,UAAU,YAAY,UAAU;AAClD,UAAQ,IAAI,mBAAmB,UAAU,EAAE;AAE3C,QAAM,YAAY,KAAK,IAAI,IAAI,aAAa,KAAM,QAAQ,CAAC;AAC3D,UAAQ,IAAI;AAAA,qBAAwB,OAAO,GAAG;AAC9C,UAAQ,IAAI,WAAW,SAAS,EAAE;AAClC,UAAQ,IAAI,OAAO,OAAO,MAAM,SAAS;AACzC,UAAQ,IAAI,OAAO,QAAQ,OAAO,eAAe,CAAC,gBAAgB;AAClE,UAAQ,IAAI,mBAAmB;AAC/B,UAAQ,IAAI,eAAe;AAE3B,SAAO;AAAA,IACL;AAAA,IACA;AAAA,IACA,YAAY,OAAO;AAAA,IACnB,cAAc,QAAQ;AAAA,EACxB;AACF;AAMA,eAAe,eACb,WACA,SACA,WACsB;AAEtB,QAAM,kBAAkB,UAAU,QAAQ,SAAS;AACnD,UAAQ,IAAI,sBAAsB,YAAY,eAAe,CAAC,EAAE;AAGhE,QAAM,YAAiB,aAAQ,QAAQ,MAAM;AAC7C,QAAM,YAAiB,UAAK,WAAW,QAAQ;AAC/C,QAAS,aAAS,MAAM,WAAW,EAAE,WAAW,KAAK,CAAC;AAGtD,MAAI,eAA6B,CAAC;AAClC,MAAI,mBAAmB;AACvB,MAAI,UAAU;AACd,QAAM,aAA0B,CAAC;AACjC,QAAM,UAAoD,CAAC;AAC3D,MAAI,SAAwB;AAC5B,MAAI,gBAA0B,CAAC;AAC/B,MAAI,UAAyB;AAG7B,QAAMC,sBAAqB,CAAC,WAA+B;AACzD,QAAI,OAAO;AACX,eAAW,OAAO,QAAQ;AACxB,YAAM,QAAQ,OAAO,GAAG;AACxB,cAAQ,IAAI,SAAS;AACrB,UAAI,UAAU,KAAM,SAAQ;AAAA,eACnB,OAAO,UAAU,SAAU,SAAQ,MAAM,SAAS;AAAA,eAClD,OAAO,UAAU,SAAU,SAAQ,OAAO,KAAK,EAAE;AAAA,eACjD,OAAO,UAAU,UAAW,SAAQ,QAAQ,IAAI;AAAA,UACpD,SAAQ;AAAA,IACf;AACA,WAAO;AAAA,EACT;AAGA,QAAM,gBAAgB,CAAC,WAAuB;AAC5C,iBAAa,KAAK,MAAM;AACxB,wBAAoBA,oBAAmB,MAAM;AAG7C,QAAI,oBAAoB,iBAAiB;AACvC,iBAAW;AAAA,IACb;AAAA,EACF;AAGA,QAAM,aAAa,MAAM;AACvB,QAAI,aAAa,WAAW,EAAG;AAG/B,QAAI,SAAS;AACX,mBAAa,KAAK,CAAC,GAAG,MAAM;AAC1B,cAAM,OAAO,EAAE,OAAO;AACtB,cAAM,OAAO,EAAE,OAAO;AACtB,YAAI,SAAS,KAAM,QAAO;AAC1B,YAAI,SAAS,QAAQ,SAAS,OAAW,QAAO;AAChD,YAAI,SAAS,QAAQ,SAAS,OAAW,QAAO;AAChD,eAAO,OAAO,OAAO,KAAK;AAAA,MAC5B,CAAC;AAAA,IACH;AAEA,UAAM,QAAe;AAAA,MACnB,IAAI,OAAO,OAAO;AAAA,MAClB,SAAS;AAAA,MACT,UAAU;AAAA,IACZ;AAGA,UAAM,YAAiB,UAAK,WAAW,GAAG,OAAO,OAAO;AACxD,IAAG,kBAAc,WAAW,KAAK,UAAU,YAAY,CAAC;AAGxD,UAAM,cAAc,qBAAqB,cAAc,MAAO;AAC9D,eAAW,KAAK;AAAA,MACd,IAAI,OAAO,OAAO;AAAA,MAClB,MAAM,UAAU,OAAO;AAAA,MACvB,OAAO,aAAa;AAAA,MACpB,UAAU;AAAA,MACV;AAAA,IACF,CAAC;AAGD,eAAW,aAAa,eAAe;AACrC,UAAI,CAAC,QAAQ,SAAS,GAAG;AACvB,gBAAQ,SAAS,IAAI,CAAC;AAAA,MACxB;AACA,iBAAW,UAAU,cAAc;AACjC,cAAM,QAAQ,OAAO,SAAS;AAC9B,YAAI,UAAU,QAAQ,UAAU,OAAW;AAC3C,cAAM,MAAM,OAAO,KAAK;AACxB,YAAI,CAAC,QAAQ,SAAS,EAAE,GAAG,GAAG;AAC5B,kBAAQ,SAAS,EAAE,GAAG,IAAI,CAAC;AAAA,QAC7B;AACA,YAAI,CAAC,QAAQ,SAAS,EAAE,GAAG,EAAE,SAAS,OAAO,OAAO,CAAC,GAAG;AACtD,kBAAQ,SAAS,EAAE,GAAG,EAAE,KAAK,OAAO,OAAO,CAAC;AAAA,QAC9C;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,IAAI,iBAAiB,OAAO,KAAK,aAAa,OAAO,eAAe,CAAC,aAAa,YAAY,gBAAgB,CAAC,GAAG;AAG1H;AACA,mBAAe,CAAC;AAChB,uBAAmB;AAAA,EACrB;AAGA,UAAQ,IAAI,0CAA0C;AACtD,QAAM,eAAe,MAAM;AAAA,IACzB;AAAA,IACA,QAAQ;AAAA,IACR,MAAM;AAAA,IAAC;AAAA;AAAA,IACP;AAAA,IACA,EAAE,YAAY,MAAM,YAAY,IAAK;AAAA,EACvC;AAEA,QAAM,SAAS,aAAa;AAC5B,QAAM,SAAS,aAAa;AAE5B,MAAI,OAAO,WAAW,GAAG;AACvB,UAAM,IAAI,MAAM,gCAAgC;AAAA,EAClD;AAGA,UAAQ,IAAI,iCAAiC;AAC7C,WAAS,YAAY,MAAM;AAC3B,UAAQ,IAAI,SAAS,OAAO,OAAO,MAAM,SAAS;AAElD,MAAI,OAAO,cAAc;AACvB,YAAQ,IAAI,2BAA2B,OAAO,YAAY,EAAE;AAAA,EAC9D;AAGA,YAAU,QAAQ,WAAW,kBAAkB,QAAQ,MAAM;AAC7D,MAAI,SAAS;AACX,YAAQ,IAAI,sBAAsB,OAAO,EAAE;AAAA,EAC7C;AAGA,QAAM,wBAAwB;AAC9B,MAAI,QAAQ,OAAO;AACjB,oBAAgB,QAAQ,MAAM,MAAM,GAAG,EAAE,IAAI,CAAC,MAAM,EAAE,KAAK,CAAC;AAAA,EAC9D,OAAO;AACL,UAAM,cAAc,OAAO,OAAO,OAAO,CAAC,MAAM,EAAE,OAAO,EAAE,IAAI,CAAC,MAAM,EAAE,IAAI;AAC5E,QAAI,YAAY,SAAS,uBAAuB;AAC9C,cAAQ,IAAI,YAAY,YAAY,MAAM,2CAA2C,qBAAqB,EAAE;AAC5G,cAAQ,IAAI,8CAA8C;AAC1D,sBAAgB,YAAY,MAAM,GAAG,qBAAqB;AAAA,IAC5D,OAAO;AACL,sBAAgB;AAAA,IAClB;AAAA,EACF;AAEA,MAAI,cAAc,SAAS,GAAG;AAC5B,YAAQ,IAAI,oBAAoB,cAAc,KAAK,IAAI,CAAC,EAAE;AAAA,EAC5D;AAGA,aAAW,SAAS,OAAO,QAAQ;AACjC,UAAM,UAAU,cAAc,SAAS,MAAM,IAAI;AAAA,EACnD;AAGA,UAAQ,IAAI,2BAA2B;AACvC,QAAM,EAAE,MAAM,IAAI,MAAM;AAAA,IACtB;AAAA,IACA,QAAQ;AAAA,IACR;AAAA,IACA;AAAA,IACA,EAAE,cAAc,KAAK;AAAA,EACvB;AAGA,aAAW;AAEX,UAAQ,IAAI;AAAA,YAAe,MAAM,eAAe,CAAC,qBAAqB,MAAM,GAAG;AAG/E,QAAM,SAAsB;AAAA,IAC1B,WAAW;AAAA,IACX,SAAS,WAAW;AAAA,IACpB;AAAA,EACF;AAGA,QAAM,WAAqB;AAAA,IACzB,SAAS;AAAA,IACT,cAAa,oBAAI,KAAK,GAAE,YAAY;AAAA,IACpC;AAAA,IACA,QAAQ;AAAA,IACR;AAAA,IACA,cAAc;AAAA,IACd;AAAA,EACF;AAGA,QAAM,eAAoB,UAAK,WAAW,eAAe;AACzD,QAAS,aAAS,UAAU,cAAc,KAAK,UAAU,UAAU,MAAM,CAAC,CAAC;AAC3E,UAAQ,IAAI,qBAAqB,YAAY,EAAE;AAG/C,UAAQ,IAAI,sBAAsB;AAClC,QAAM,iBAAiB,OAAO,MAAM,GAAG,GAAG;AAC1C,QAAM,aAAa,eAAe,QAAS,UAAU,cAAc;AACnE,QAAM,aAAkB,UAAK,WAAW,WAAW;AACnD,QAAS,aAAS,UAAU,YAAY,UAAU;AAClD,UAAQ,IAAI,mBAAmB,UAAU,EAAE;AAE3C,QAAM,YAAY,KAAK,IAAI,IAAI,aAAa,KAAM,QAAQ,CAAC;AAC3D,UAAQ,IAAI;AAAA,qBAAwB,OAAO,GAAG;AAC9C,UAAQ,IAAI,WAAW,SAAS,EAAE;AAClC,UAAQ,IAAI,OAAO,WAAW,MAAM,SAAS;AAC7C,UAAQ,IAAI,OAAO,MAAM,eAAe,CAAC,gBAAgB;AACzD,UAAQ,IAAI,mBAAmB;AAC/B,UAAQ,IAAI,eAAe;AAE3B,SAAO;AAAA,IACL;AAAA,IACA;AAAA,IACA,YAAY,WAAW;AAAA,IACvB,cAAc;AAAA,EAChB;AACF;AAEA,SAAS,YAAY,OAAuB;AAC1C,MAAI,QAAQ,KAAM,QAAO,GAAG,KAAK;AACjC,MAAI,QAAQ,OAAO,KAAM,QAAO,IAAI,QAAQ,MAAM,QAAQ,CAAC,CAAC;AAC5D,MAAI,QAAQ,OAAO,OAAO,KAAM,QAAO,IAAI,SAAS,OAAO,OAAO,QAAQ,CAAC,CAAC;AAC5E,SAAO,IAAI,SAAS,OAAO,OAAO,OAAO,QAAQ,CAAC,CAAC;AACrD;;;AMnaA,YAAYC,SAAQ;AACpB,SAAS,UAAU,SAAS,OAAO,cAAc;AAOjD,IAAMC,uBAAsB,MAAM,OAAO;AAGzC,IAAM,yBAAyB;AAE/B,eAAsB,QACpB,WACA,SACe;AAEf,MAAI,CAAI,eAAW,SAAS,GAAG;AAC7B,UAAM,IAAI,MAAM,yBAAyB,SAAS,EAAE;AAAA,EACtD;AAEA,QAAM,WAAW,YAAY,SAAS;AACtC,UAAQ,IAAI;AAAA,QAAW,SAAS,EAAE;AAClC,UAAQ,IAAI,SAASC,aAAY,QAAQ,CAAC,EAAE;AAE5C,MAAI;AACJ,MAAI;AACJ,MAAI;AACJ,MAAI,cAAc;AAGlB,MAAI,WAAWD,sBAAqB;AAClC,YAAQ,IAAI,0CAA0C;AAEtD,UAAME,cAAa,QAAQ,UAAU;AAGrC,QAAI,QAAQ,MAAM;AAChB,cAAQ,IAAI,0DAA0D;AAEtE,YAAM,SAAS,MAAM;AAAA,QACnB;AAAA,QACA,QAAQ;AAAA,QACR,MAAM;AAAA,QAAC;AAAA;AAAA,QACPA;AAAA,QACA,EAAE,YAAY,MAAM,YAAAA,YAAW;AAAA,MACjC;AAEA,qBAAe,OAAO,kBAAkB,OAAO;AAC/C,sBAAgB,OAAO;AACvB,eAAS,OAAO;AAChB,oBAAc;AAEd,cAAQ,IAAI,WAAW,MAAM,EAAE;AAC/B,cAAQ,IAAI,uBAAuB,aAAa,eAAe,CAAC,EAAE;AAAA,IACpE,OAAO;AACL,cAAQ,IAAI,kEAAkE;AAE9E,YAAM,SAAS,MAAM;AAAA,QACnB;AAAA,QACA,QAAQ;AAAA,QACR,MAAM;AAAA,QAAC;AAAA;AAAA,QACPA;AAAA,QACA,EAAE,cAAc,KAAK;AAAA,MACvB;AAEA,qBAAe,OAAO;AACtB,sBAAgB,OAAO;AACvB,eAAS,OAAO;AAEhB,cAAQ,IAAI,WAAW,MAAM,EAAE;AAC/B,cAAQ,IAAI,kBAAkB,aAAa,eAAe,CAAC,EAAE;AAAA,IAC/D;AAAA,EACF,OAAO;AACL,YAAQ,IAAI,mBAAmB;AAG/B,UAAM,SAAS,MAAM,UAAU,WAAW,QAAQ,MAAM;AACxD,mBAAe,OAAO,QAAQ;AAC9B,aAAS,OAAO;AAEhB,YAAQ,IAAI,WAAW,MAAM,EAAE;AAC/B,YAAQ,IAAI,kBAAkB,aAAa,eAAe,CAAC,EAAE;AAE7D,QAAI,iBAAiB,GAAG;AACtB,cAAQ,IAAI,qBAAqB;AACjC;AAAA,IACF;AAGA,UAAMA,cAAa,KAAK,IAAI,QAAQ,UAAU,KAAM,YAAY;AAChE,oBAAgB,OAAO,QAAQ,MAAM,GAAGA,WAAU;AAAA,EACpD;AAEA,MAAI,cAAc,WAAW,GAAG;AAC9B,YAAQ,IAAI,qBAAqB;AACjC;AAAA,EACF;AAEA,QAAM,aAAa,cAAc;AACjC,UAAQ,IAAI;AAAA,YAAe,WAAW,eAAe,CAAC,aAAa;AAGnE,QAAM,SAAS,YAAY,aAAa;AAGxC,UAAQ,IAAI,OAAO,IAAI,OAAO,EAAE,CAAC;AACjC,UAAQ,IAAI,QAAQ;AACpB,UAAQ,IAAI,IAAI,OAAO,EAAE,CAAC;AAE1B,UAAQ,IAAI;AAAA,UAAa,OAAO,OAAO,MAAM;AAAA,CAAM;AAEnD,aAAW,SAAS,OAAO,QAAQ;AACjC,UAAM,YAAY,MAAM,SAAS,OAAO,eAAe,eAAe;AACtE,UAAM,YAAY,MAAM,UAAU,eAAe;AACjD,UAAM,WAAW,MAAM,WAAW,gBAAgB;AAElD,YAAQ,IAAI,KAAK,MAAM,IAAI,KAAK,MAAM,IAAI,GAAG,QAAQ,GAAG,SAAS,GAAG,SAAS,EAAE;AAG/E,UAAM,QAAQ,MAAM;AACpB,UAAM,YAAY,CAAC;AAEnB,QAAI,MAAM,gBAAgB,QAAW;AACnC,YAAM,kBAAmB,MAAM,cAAc,eAAgB,KAAK,QAAQ,CAAC;AAC3E,gBAAU,KAAK,gBAAgB,MAAM,WAAW,KAAK,cAAc,IAAI;AAAA,IACzE;AAEA,QAAI,MAAM,QAAQ,UAAa,MAAM,QAAQ,QAAW;AACtD,gBAAU,KAAK,UAAU,YAAY,MAAM,GAAG,CAAC,MAAM,YAAY,MAAM,GAAG,CAAC,EAAE;AAAA,IAC/E;AAEA,QAAI,MAAM,YAAY,GAAG;AACvB,gBAAU,KAAK,UAAU,MAAM,SAAS,EAAE;AAAA,IAC5C;AAEA,QAAI,UAAU,SAAS,GAAG;AACxB,cAAQ,IAAI,OAAO,UAAU,KAAK,IAAI,CAAC,EAAE;AAAA,IAC3C;AAEA,QAAI,MAAM,gBAAgB,MAAM,aAAa,SAAS,GAAG;AACvD,YAAM,UAAU,MAAM,aAAa,MAAM,GAAG,CAAC,EAAE,IAAI,WAAW,EAAE,KAAK,IAAI;AACzE,cAAQ,IAAI,iBAAiB,OAAO,EAAE;AAAA,IACxC;AAAA,EACF;AAGA,QAAM,gBAAgB,WAAW;AACjC,QAAM,kBAAkB,IAAI,OAAO;AACnC,QAAM,kBAAkB,KAAK,KAAK,WAAW,eAAe;AAE5D,UAAQ,IAAI,OAAO,IAAI,OAAO,EAAE,CAAC;AACjC,UAAQ,IAAI,gBAAgB;AAC5B,UAAQ,IAAI,IAAI,OAAO,EAAE,CAAC;AAE1B,UAAQ,IAAI;AAAA,uBAA0BD,aAAY,aAAa,CAAC,EAAE;AAClE,UAAQ,IAAI,6BAA6B,eAAe,SAAS;AAGjE,UAAQ,IAAI,OAAO,IAAI,OAAO,EAAE,CAAC;AACjC,UAAQ,IAAI,qBAAqB;AACjC,UAAQ,IAAI,IAAI,OAAO,EAAE,CAAC;AAG1B,QAAM,sBAAsB,kBAAkB,QAAQ,aAAa;AACnE,QAAM,kBAAkB,OAAO,OAC5B,OAAO,OAAK,EAAE,SAAS,YAAY,EAAE,SAAS,YAAY,EAAE,SAAS,MAAM,EAC3E,IAAI,OAAK,EAAE,IAAI;AAElB,MAAI,qBAAoC;AAExC,MAAI,gBAAgB,SAAS,GAAG;AAC9B,UAAM,oBAAoB;AAAA,MACxB,EAAE,MAAM,kCAAkC,OAAO,GAAG;AAAA,MACpD,GAAG,gBAAgB,IAAI,QAAM;AAAA,QAC3B,MAAM,MAAM,sBAAsB,GAAG,CAAC,mBAAmB;AAAA,QACzD,OAAO;AAAA,MACT,EAAE;AAAA,IACJ;AAGA,sBAAkB,KAAK,CAAC,GAAG,MAAM;AAC/B,UAAI,EAAE,UAAU,oBAAqB,QAAO;AAC5C,UAAI,EAAE,UAAU,oBAAqB,QAAO;AAC5C,UAAI,EAAE,UAAU,GAAI,QAAO;AAC3B,UAAI,EAAE,UAAU,GAAI,QAAO;AAC3B,aAAO;AAAA,IACT,CAAC;AAED,yBAAqB,MAAM,OAAO;AAAA,MAChC,SAAS;AAAA,MACT,SAAS;AAAA,MACT,SAAS,uBAAuB;AAAA,IAClC,CAAC;AAED,QAAI,uBAAuB,IAAI;AAC7B,2BAAqB;AAAA,IACvB;AAAA,EACF;AAGA,QAAM,kBAAkB,mBAAmB,MAAM;AACjD,MAAI,kBAA4B,CAAC;AAEjC,MAAI,gBAAgB,SAAS,GAAG;AAC9B,QAAI,gBAAgB,UAAU,wBAAwB;AAEpD,YAAM,iBAAiB,MAAM,QAAQ;AAAA,QACnC,SAAS,6BAA6B,gBAAgB,KAAK,IAAI,CAAC;AAAA,QAChE,SAAS;AAAA,MACX,CAAC;AAED,UAAI,gBAAgB;AAClB,0BAAkB;AAAA,MACpB,OAAO;AAEL,0BAAkB,MAAM,qBAAqB,OAAO,QAAQ,eAAe;AAAA,MAC7E;AAAA,IACF,OAAO;AAEL,cAAQ,IAAI;AAAA,QAAW,gBAAgB,MAAM,oBAAoB;AACjE,wBAAkB,MAAM,qBAAqB,OAAO,QAAQ,eAAe;AAAA,IAC7E;AAAA,EACF;AAGA,QAAM,YAAY,MAAM,MAAM;AAAA,IAC5B,SAAS;AAAA,IACT,SAAS;AAAA,EACX,CAAC;AAGD,QAAM,kBAAkB,MAAM,OAAO;AAAA,IACnC,SAAS;AAAA,IACT,SAAS;AAAA,MACP,EAAE,MAAM,sCAAsC,OAAO,MAAM;AAAA,MAC3D,EAAE,MAAM,iCAAiC,OAAO,MAAM;AAAA,MACtD,EAAE,MAAM,0CAA0C,OAAO,OAAO;AAAA,MAChE,EAAE,MAAM,UAAU,OAAO,SAAS;AAAA,IACpC;AAAA,IACA,SAAS;AAAA,EACX,CAAC;AAED,MAAI,YAAY;AAChB,MAAI,oBAAoB,UAAU;AAChC,gBAAY,MAAM,MAAM;AAAA,MACtB,SAAS;AAAA,MACT,SAAS;AAAA,IACX,CAAC;AAAA,EACH;AAGA,UAAQ,IAAI,OAAO,IAAI,OAAO,EAAE,CAAC;AACjC,UAAQ,IAAI,uBAAuB;AACnC,UAAQ,IAAI,IAAI,OAAO,EAAE,CAAC;AAE1B,UAAQ,IAAI;AAAA,SAAY,SAAS,EAAE;AACnC,UAAQ,IAAI,WAAW,SAAS,EAAE;AAClC,UAAQ,IAAI,eAAe,SAAS,EAAE;AACtC,UAAQ,IAAI,aAAa,sBAAsB,gBAAgB,EAAE;AACjE,UAAQ,IAAI,YAAY,gBAAgB,SAAS,IAAI,gBAAgB,KAAK,IAAI,IAAI,QAAQ,EAAE;AAG5F,MAAI,MAAM,2BAA2B,SAAS,SAAS,SAAS,QAAQ,SAAS;AACjF,MAAI,oBAAoB;AACtB,WAAO,OAAO,kBAAkB;AAAA,EAClC;AACA,MAAI,gBAAgB,SAAS,GAAG;AAC9B,WAAO,QAAQ,gBAAgB,KAAK,GAAG,CAAC;AAAA,EAC1C;AAEA,UAAQ,IAAI;AAAA;AAAA,IAA4B,GAAG,EAAE;AAG7C,QAAM,WAAW,MAAM,QAAQ;AAAA,IAC7B,SAAS;AAAA,IACT,SAAS;AAAA,EACX,CAAC;AAED,MAAI,UAAU;AACZ,YAAQ,IAAI,IAAI;AAChB,UAAM,MAAM,WAAW;AAAA,MACrB,QAAQ;AAAA,MACR;AAAA,MACA,SAAS,sBAAsB;AAAA,MAC/B,OAAO,gBAAgB,SAAS,IAAI,gBAAgB,KAAK,GAAG,IAAI;AAAA,MAChE,QAAQ,QAAQ;AAAA,IAClB,CAAC;AAAA,EACH,OAAO;AACL,YAAQ,IAAI,sDAAsD;AAAA,EACpE;AACF;AAKA,eAAe,qBACb,WACA,mBACmB;AAEnB,QAAM,gBAAgB,UAAU,OAAO,OAAK,EAAE,SAAS,SAAS,EAAE,IAAI,OAAK,EAAE,IAAI;AACjF,QAAM,iBAAiB,UAAU;AAAA,IAAO,OACtC,EAAE,SAAS,YACX,EAAE,MAAM,eAAe,OACvB,CAAC,cAAc,SAAS,EAAE,IAAI;AAAA,EAChC,EAAE,IAAI,OAAK,EAAE,IAAI;AACjB,QAAM,gBAAgB,UAAU,OAAO,OAAK,EAAE,SAAS,QAAQ,EAAE,IAAI,OAAK,EAAE,IAAI;AAEhF,QAAM,UAAU,UACb,OAAO,OAAK;AAGX,QAAI,EAAE,MAAM,cAAc,EAAG,QAAO;AACpC,QAAI,EAAE,SAAS,UAAW,QAAO;AACjC,QAAI,EAAE,MAAM,eAAe,IAAK,QAAO;AACvC,WAAO;AAAA,EACT,CAAC,EACA,IAAI,OAAK;AACR,UAAM,gBAAgB,kBAAkB,SAAS,EAAE,IAAI;AACvD,UAAM,kBAAkB,EAAE,MAAM,cAAc,KAAK,EAAE,MAAM,WAAW,aAAa;AAEnF,WAAO;AAAA,MACL,MAAM,GAAG,EAAE,IAAI,GAAG,eAAe,GAAG,gBAAgB,OAAO,EAAE;AAAA,MAC7D,OAAO,EAAE;AAAA,MACT,SAAS;AAAA,IACX;AAAA,EACF,CAAC,EACA,KAAK,CAAC,GAAG,MAAM;AAEd,UAAM,OAAO,kBAAkB,SAAS,EAAE,KAAK,IAAI,IAAI;AACvD,UAAM,OAAO,kBAAkB,SAAS,EAAE,KAAK,IAAI,IAAI;AACvD,WAAO,OAAO;AAAA,EAChB,CAAC;AAEH,MAAI,QAAQ,WAAW,GAAG;AACxB,WAAO,CAAC;AAAA,EACV;AAEA,UAAQ,IAAI,kCAAkC;AAC9C,UAAQ,IAAI,oEAAoE;AAEhF,QAAM,WAAW,MAAM,SAAS;AAAA,IAC9B,SAAS;AAAA,IACT;AAAA,IACA,UAAU;AAAA,IACV,cAAc;AAAA;AAAA,EAChB,CAAC;AAED,SAAO;AACT;AAEA,SAASA,aAAY,OAAuB;AAC1C,MAAI,QAAQ,KAAM,QAAO,GAAG,KAAK;AACjC,MAAI,QAAQ,OAAO,KAAM,QAAO,IAAI,QAAQ,MAAM,QAAQ,CAAC,CAAC;AAC5D,MAAI,QAAQ,OAAO,OAAO,KAAM,QAAO,IAAI,SAAS,OAAO,OAAO,QAAQ,CAAC,CAAC;AAC5E,SAAO,IAAI,SAAS,OAAO,OAAO,OAAO,QAAQ,CAAC,CAAC;AACrD;AAEA,SAAS,YAAY,OAAwB;AAC3C,MAAI,UAAU,KAAM,QAAO;AAC3B,MAAI,UAAU,OAAW,QAAO;AAChC,MAAI,OAAO,UAAU,UAAU;AAC7B,QAAI,MAAM,SAAS,IAAI;AACrB,aAAO,IAAI,MAAM,MAAM,GAAG,EAAE,CAAC;AAAA,IAC/B;AACA,WAAO,IAAI,KAAK;AAAA,EAClB;AACA,SAAO,OAAO,KAAK;AACrB;;;AClXA,YAAYE,SAAQ;AACpB,SAAS,WAAAC,gBAAe;AAIxB,IAAMC,uBAAsB,MAAM,OAAO;AAWzC,SAASC,cAAaC,QAAuB;AAC3C,SAAOA,OAEJ,QAAQ,mCAAmC,EAAE,EAE7C,QAAQ,cAAc,MAAM,EAG5B,QAAQ,sBAAsB,IAAI,EAElC,QAAQ,iBAAiB,mBAAmB;AACjD;AAKA,SAASC,wBAAuB,SAA+B;AAC7D,QAAM,aAAa,oBAAI,IAAY;AACnC,aAAW,UAAU,SAAS;AAC5B,eAAW,OAAO,OAAO,KAAK,MAAM,GAAG;AACrC,iBAAW,IAAI,GAAG;AAAA,IACpB;AAAA,EACF;AACA,QAAM,QAAQ,MAAM,KAAK,UAAU,EAAE,KAAK,EAAE,IAAI,CAAC,MAAM,IAAI,CAAC,GAAG,EAAE,KAAK,KAAK;AAC3E,SAAO,2BAA2B,KAAK;AACzC;AAEA,eAAsB,MACpB,WACA,SACe;AAEf,MAAI,CAAI,eAAW,SAAS,GAAG;AAC7B,UAAM,IAAI,MAAM,yBAAyB,SAAS,EAAE;AAAA,EACtD;AAEA,QAAM,WAAW,YAAY,SAAS;AACtC,QAAM,aAAa,QAAQ,UAAU;AAErC,UAAQ,MAAM,cAAc,SAAS,EAAE;AACvC,UAAQ,MAAM,YAAY,UAAU,aAAa;AAEjD,MAAI;AACJ,MAAI;AAEJ,MAAI,WAAWH,sBAAqB;AAElC,UAAM,SAAS,MAAM;AAAA,MACnB;AAAA,MACA,QAAQ;AAAA,MACR,MAAM;AAAA,MAAC;AAAA,MACP;AAAA,MACA,EAAE,YAAY,MAAM,WAAW;AAAA,IACjC;AACA,cAAU,OAAO;AACjB,aAAS,OAAO;AAAA,EAClB,OAAO;AAEL,UAAM,cAAc,MAAM,UAAU,WAAW,QAAQ,MAAM;AAC7D,cAAU,YAAY,QAAQ,MAAM,GAAG,UAAU;AACjD,aAAS,YAAY;AAAA,EACvB;AAEA,UAAQ,MAAM,WAAW,MAAM,EAAE;AACjC,UAAQ,MAAM,WAAW,QAAQ,MAAM;AAAA,CAAY;AAGnD,QAAM,cAAc,QAAQ,MAAM,GAAG,GAAG;AACxC,QAAM,WAAWI,SAAQ,KAAK,UAAU,WAAW,GAAG,EAAE,UAAU,OAAO,CAAC;AAC1E,QAAMF,SAAQD,cAAa,QAAQ;AACnC,QAAM,aAAaE,wBAAuB,OAAO;AAGjD,QAAM,SAAS;AAAA;AAAA,aAEJ,SAAS;AAAA,iBACN,oBAAI,KAAK,GAAE,YAAY,CAAC;AAAA;AAAA;AAAA,EAGtCD,MAAK;AAAA;AAAA,EAEL,UAAU;AAAA;AAGV,MAAI,QAAQ,QAAQ;AAClB,UAAS,aAAS,UAAU,QAAQ,QAAQ,MAAM;AAClD,YAAQ,MAAM,qBAAqB,QAAQ,MAAM,EAAE;AAAA,EACrD,OAAO;AAEL,YAAQ,IAAI,MAAM;AAAA,EACpB;AACF;;;ARrGA,IAAM,UAAU,IAAI,QAAQ;AAE5B,QACG,KAAK,cAAc,EACnB,YAAY,uEAAuE,EACnF,QAAQ,OAAO;AAElB,QACG,QAAQ,OAAO,EACf,YAAY,0CAA0C,EACtD,SAAS,WAAW,wCAAwC,EAC5D,eAAe,sBAAsB,kBAAkB,EACvD,OAAO,2BAA2B,iCAAiC,KAAK,EACxE,OAAO,0BAA0B,4BAA4B,EAC7D,OAAO,wBAAwB,iCAAiC,EAChE,OAAO,yBAAyB,kCAAkC,EAClE,OAAO,OAAOG,QAAO,YAAY;AAChC,MAAI;AACF,UAAM,MAAMA,QAAO;AAAA,MACjB,QAAQ,QAAQ;AAAA,MAChB,WAAW,QAAQ;AAAA,MACnB,SAAS,QAAQ;AAAA,MACjB,OAAO,QAAQ;AAAA,MACf,QAAQ,QAAQ;AAAA,IAClB,CAAC;AAAA,EACH,SAAS,OAAO;AACd,YAAQ,MAAM,UAAW,MAAgB,OAAO;AAChD,YAAQ,KAAK,CAAC;AAAA,EAChB;AACF,CAAC;AAEH,QACG,QAAQ,SAAS,EACjB,YAAY,mDAAmD,EAC/D,SAAS,WAAW,iBAAiB,EACrC,OAAO,wBAAwB,+BAA+B,MAAM,EACpE,OAAO,yBAAyB,kCAAkC,EAClE,OAAO,UAAU,iEAAiE,EAClF,OAAO,OAAOA,QAAO,YAAY;AAChC,MAAI;AACF,UAAM,QAAQA,QAAO;AAAA,MACnB,QAAQ,SAAS,QAAQ,QAAQ,EAAE;AAAA,MACnC,QAAQ,QAAQ;AAAA,MAChB,MAAM,QAAQ;AAAA,IAChB,CAAC;AAAA,EACH,SAAS,OAAO;AACd,YAAQ,MAAM,UAAW,MAAgB,OAAO;AAChD,YAAQ,KAAK,CAAC;AAAA,EAChB;AACF,CAAC;AAEH,QACG,QAAQ,OAAO,EACf,YAAY,4CAA4C,EACxD,SAAS,WAAW,wCAAwC,EAC5D,OAAO,wBAAwB,+BAA+B,MAAM,EACpE,OAAO,yBAAyB,kCAAkC,EAClE,OAAO,uBAAuB,+BAA+B,EAC7D,OAAO,OAAOA,QAAO,YAAY;AAChC,MAAI;AACF,UAAM,MAAMA,QAAO;AAAA,MACjB,QAAQ,SAAS,QAAQ,QAAQ,EAAE;AAAA,MACnC,QAAQ,QAAQ;AAAA,MAChB,QAAQ,QAAQ;AAAA,IAClB,CAAC;AAAA,EACH,SAAS,OAAO;AACd,YAAQ,MAAM,UAAW,MAAgB,OAAO;AAChD,YAAQ,KAAK,CAAC;AAAA,EAChB;AACF,CAAC;AAEH,QAAQ,MAAM;","names":["fs","resolve","types","STREAMING_THRESHOLD","estimateRecordSize","fs","STREAMING_THRESHOLD","formatBytes","sampleSize","fs","json2ts","STREAMING_THRESHOLD","cleanupTypes","types","generateFieldNamesType","json2ts","input"]}