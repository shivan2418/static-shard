{"version":3,"sources":["../../src/cli/index.ts","../../src/cli/commands/build.ts","../../src/cli/utils/parsers.ts","../../src/cli/utils/schema.ts","../../src/cli/utils/chunker.ts","../../src/cli/utils/indexer.ts","../../src/cli/utils/codegen.ts","../../src/cli/commands/inspect.ts","../../src/cli/commands/types.ts"],"sourcesContent":["#!/usr/bin/env node\n\n/**\n * static-shard CLI\n */\n\nimport { Command } from \"commander\";\nimport { build } from \"./commands/build.js\";\nimport { inspect } from \"./commands/inspect.js\";\nimport { types } from \"./commands/types.js\";\n\nconst program = new Command();\n\nprogram\n  .name(\"static-shard\")\n  .description(\"Query large static datasets efficiently by splitting them into chunks\")\n  .version(\"0.1.0\");\n\nprogram\n  .command(\"build\")\n  .description(\"Build chunks and client from a data file\")\n  .argument(\"<input>\", \"Input data file (JSON, NDJSON, or CSV)\")\n  .requiredOption(\"-o, --output <dir>\", \"Output directory\")\n  .option(\"-s, --chunk-size <size>\", \"Target chunk size (e.g., 5mb)\", \"5mb\")\n  .option(\"-c, --chunk-by <field>\", \"Field to sort and chunk by\")\n  .option(\"-i, --index <fields>\", \"Comma-separated fields to index\")\n  .option(\"-f, --format <format>\", \"Input format (json, ndjson, csv)\")\n  .action(async (input, options) => {\n    try {\n      await build(input, {\n        output: options.output,\n        chunkSize: options.chunkSize,\n        chunkBy: options.chunkBy,\n        index: options.index,\n        format: options.format,\n      });\n    } catch (error) {\n      console.error(\"Error:\", (error as Error).message);\n      process.exit(1);\n    }\n  });\n\nprogram\n  .command(\"inspect\")\n  .description(\"Analyze a data file and suggest chunking strategy\")\n  .argument(\"<input>\", \"Input data file\")\n  .option(\"-n, --sample <count>\", \"Number of records to sample\", \"1000\")\n  .option(\"-f, --format <format>\", \"Input format (json, ndjson, csv)\")\n  .option(\"--fast\", \"Fast mode: estimate record count instead of reading entire file\")\n  .action(async (input, options) => {\n    try {\n      await inspect(input, {\n        sample: parseInt(options.sample, 10),\n        format: options.format,\n        fast: options.fast,\n      });\n    } catch (error) {\n      console.error(\"Error:\", (error as Error).message);\n      process.exit(1);\n    }\n  });\n\nprogram\n  .command(\"types\")\n  .description(\"Generate TypeScript types from a data file\")\n  .argument(\"<input>\", \"Input data file (JSON, NDJSON, or CSV)\")\n  .option(\"-n, --sample <count>\", \"Number of records to sample\", \"1000\")\n  .option(\"-f, --format <format>\", \"Input format (json, ndjson, csv)\")\n  .option(\"-o, --output <file>\", \"Output file (default: stdout)\")\n  .action(async (input, options) => {\n    try {\n      await types(input, {\n        sample: parseInt(options.sample, 10),\n        format: options.format,\n        output: options.output,\n      });\n    } catch (error) {\n      console.error(\"Error:\", (error as Error).message);\n      process.exit(1);\n    }\n  });\n\nprogram.parse();\n","/**\n * Build command - process data file and generate output\n * Supports streaming for large files (2GB+)\n */\n\nimport * as fs from \"node:fs\";\nimport * as path from \"node:path\";\nimport type { BuildConfig, BuildOptions, Manifest, DataRecord, ChunkMeta, Schema } from \"../../types/index.js\";\nimport { parseFile, parseSize, streamFile, getFileSize } from \"../utils/parsers.js\";\nimport { inferSchema, suggestChunkField } from \"../utils/schema.js\";\nimport { chunkRecords, generateChunkMeta, serializeChunk, calculateFieldRanges, type Chunk } from \"../utils/chunker.js\";\nimport { buildInvertedIndices } from \"../utils/indexer.js\";\nimport { generateClient } from \"../utils/codegen.js\";\n\nconst VERSION = \"1.0.0\";\n\n// Use streaming for files larger than 100MB\nconst STREAMING_THRESHOLD = 100 * 1024 * 1024;\n\nexport interface BuildResult {\n  manifest: Manifest;\n  outputDir: string;\n  chunkCount: number;\n  totalRecords: number;\n}\n\nexport async function build(\n  inputFile: string,\n  options: BuildOptions\n): Promise<BuildResult> {\n  const startTime = Date.now();\n\n  // Validate input file exists\n  if (!fs.existsSync(inputFile)) {\n    throw new Error(`Input file not found: ${inputFile}`);\n  }\n\n  const fileSize = getFileSize(inputFile);\n  console.log(`Input: ${inputFile} (${formatBytes(fileSize)})`);\n\n  // Use streaming for large files\n  if (fileSize > STREAMING_THRESHOLD) {\n    console.log(\"Using streaming mode for large file...\");\n    return buildStreaming(inputFile, options, startTime);\n  }\n\n  return buildInMemory(inputFile, options, startTime);\n}\n\n/**\n * Standard in-memory build for smaller files\n */\nasync function buildInMemory(\n  inputFile: string,\n  options: BuildOptions,\n  startTime: number\n): Promise<BuildResult> {\n  console.log(\"Reading file...\");\n\n  // Parse input file\n  const { records, format } = await parseFile(inputFile, options.format);\n  console.log(`Parsed ${records.length.toLocaleString()} records (format: ${format})`);\n\n  if (records.length === 0) {\n    throw new Error(\"No records found in input file\");\n  }\n\n  // Infer schema\n  console.log(\"Inferring schema...\");\n  const schema = inferSchema(records);\n  console.log(`Found ${schema.fields.length} fields`);\n\n  if (schema.primaryField) {\n    console.log(`Detected primary field: ${schema.primaryField}`);\n  }\n\n  // Determine chunk field\n  const chunkBy = options.chunkBy || suggestChunkField(schema, records);\n  if (chunkBy) {\n    console.log(`Chunking by field: ${chunkBy}`);\n  }\n\n  // Parse chunk size\n  const targetChunkSize = parseSize(options.chunkSize);\n  console.log(`Target chunk size: ${formatBytes(targetChunkSize)}`);\n\n  // Parse indexed fields\n  const indexedFields = options.index\n    ? options.index.split(\",\").map((f) => f.trim())\n    : schema.fields.filter((f) => f.indexed).map((f) => f.name);\n\n  if (indexedFields.length > 0) {\n    console.log(`Indexing fields: ${indexedFields.join(\", \")}`);\n  }\n\n  // Update schema with indexed fields\n  for (const field of schema.fields) {\n    field.indexed = indexedFields.includes(field.name);\n  }\n\n  // Chunk the data\n  console.log(\"Chunking data...\");\n  const chunks = chunkRecords(records, schema, {\n    targetSize: targetChunkSize,\n    chunkBy,\n  });\n  console.log(`Created ${chunks.length} chunks`);\n\n  // Build indices\n  console.log(\"Building indices...\");\n  const indices = buildInvertedIndices(chunks, schema, indexedFields);\n  console.log(`Built indices for ${Object.keys(indices).length} fields`);\n\n  // Create output directory\n  const outputDir = path.resolve(options.output);\n  const chunksDir = path.join(outputDir, \"chunks\");\n  await fs.promises.mkdir(chunksDir, { recursive: true });\n\n  // Write chunks\n  console.log(\"Writing chunks...\");\n  const chunkMetas = [];\n  for (const chunk of chunks) {\n    const chunkPath = path.join(chunksDir, `${chunk.id}.json`);\n    await fs.promises.writeFile(chunkPath, serializeChunk(chunk));\n    chunkMetas.push(generateChunkMeta(chunk, schema, \"chunks\"));\n  }\n\n  // Build config\n  const config: BuildConfig = {\n    chunkSize: targetChunkSize,\n    chunkBy: chunkBy || null,\n    indexedFields,\n  };\n\n  // Build manifest\n  const manifest: Manifest = {\n    version: VERSION,\n    generatedAt: new Date().toISOString(),\n    schema,\n    chunks: chunkMetas,\n    indices,\n    totalRecords: records.length,\n    config,\n  };\n\n  // Write manifest\n  const manifestPath = path.join(outputDir, \"manifest.json\");\n  await fs.promises.writeFile(manifestPath, JSON.stringify(manifest, null, 2));\n  console.log(`Wrote manifest to ${manifestPath}`);\n\n  // Generate client\n  console.log(\"Generating client...\");\n  const clientCode = generateClient(schema, manifest);\n  const clientPath = path.join(outputDir, \"client.ts\");\n  await fs.promises.writeFile(clientPath, clientCode);\n  console.log(`Wrote client to ${clientPath}`);\n\n  const elapsed = ((Date.now() - startTime) / 1000).toFixed(2);\n  console.log(`\\nBuild completed in ${elapsed}s`);\n  console.log(`Output: ${outputDir}`);\n  console.log(`  - ${chunks.length} chunks`);\n  console.log(`  - ${records.length.toLocaleString()} total records`);\n  console.log(`  - manifest.json`);\n  console.log(`  - client.ts`);\n\n  return {\n    manifest,\n    outputDir,\n    chunkCount: chunks.length,\n    totalRecords: records.length,\n  };\n}\n\n/**\n * Streaming build for large files - processes records incrementally\n * without loading entire file into memory\n */\nasync function buildStreaming(\n  inputFile: string,\n  options: BuildOptions,\n  startTime: number\n): Promise<BuildResult> {\n  // Parse chunk size\n  const targetChunkSize = parseSize(options.chunkSize);\n  console.log(`Target chunk size: ${formatBytes(targetChunkSize)}`);\n\n  // Create output directory\n  const outputDir = path.resolve(options.output);\n  const chunksDir = path.join(outputDir, \"chunks\");\n  await fs.promises.mkdir(chunksDir, { recursive: true });\n\n  // Streaming state\n  let currentChunk: DataRecord[] = [];\n  let currentChunkSize = 0;\n  let chunkId = 0;\n  const chunkMetas: ChunkMeta[] = [];\n  const indices: Record<string, Record<string, string[]>> = {};\n  let schema: Schema | null = null;\n  let indexedFields: string[] = [];\n  let chunkBy: string | null = null;\n\n  // Process each record as it streams in\n  const processRecord = (record: DataRecord) => {\n    currentChunk.push(record);\n    currentChunkSize += JSON.stringify(record).length;\n\n    // When chunk reaches target size, flush it\n    if (currentChunkSize >= targetChunkSize) {\n      flushChunk();\n    }\n  };\n\n  // Flush current chunk to disk\n  const flushChunk = () => {\n    if (currentChunk.length === 0) return;\n\n    // Sort by chunkBy field if specified\n    if (chunkBy) {\n      currentChunk.sort((a, b) => {\n        const aVal = a[chunkBy];\n        const bVal = b[chunkBy];\n        if (aVal === bVal) return 0;\n        if (aVal === null || aVal === undefined) return 1;\n        if (bVal === null || bVal === undefined) return -1;\n        return aVal < bVal ? -1 : 1;\n      });\n    }\n\n    const chunk: Chunk = {\n      id: String(chunkId),\n      records: currentChunk,\n      byteSize: currentChunkSize,\n    };\n\n    // Write chunk to disk\n    const chunkPath = path.join(chunksDir, `${chunkId}.json`);\n    fs.writeFileSync(chunkPath, JSON.stringify(currentChunk));\n\n    // Generate chunk metadata\n    const fieldRanges = calculateFieldRanges(currentChunk, schema!);\n    chunkMetas.push({\n      id: String(chunkId),\n      path: `chunks/${chunkId}.json`,\n      count: currentChunk.length,\n      byteSize: currentChunkSize,\n      fieldRanges,\n    });\n\n    // Update indices\n    for (const fieldName of indexedFields) {\n      if (!indices[fieldName]) {\n        indices[fieldName] = {};\n      }\n      for (const record of currentChunk) {\n        const value = record[fieldName];\n        if (value === null || value === undefined) continue;\n        const key = String(value);\n        if (!indices[fieldName][key]) {\n          indices[fieldName][key] = [];\n        }\n        if (!indices[fieldName][key].includes(String(chunkId))) {\n          indices[fieldName][key].push(String(chunkId));\n        }\n      }\n    }\n\n    console.log(`  Wrote chunk ${chunkId} (${currentChunk.length.toLocaleString()} records, ${formatBytes(currentChunkSize)})`);\n\n    // Reset for next chunk\n    chunkId++;\n    currentChunk = [];\n    currentChunkSize = 0;\n  };\n\n  // First pass: sample records to infer schema\n  console.log(\"Sampling records for schema inference...\");\n  const sampleResult = await streamFile(\n    inputFile,\n    options.format,\n    () => {}, // No processing in first pass\n    1000,\n    { sampleOnly: true, sampleSize: 1000 }\n  );\n\n  const format = sampleResult.format;\n  const sample = sampleResult.sample;\n\n  if (sample.length === 0) {\n    throw new Error(\"No records found in input file\");\n  }\n\n  // Infer schema from sample\n  console.log(\"Inferring schema from sample...\");\n  schema = inferSchema(sample);\n  console.log(`Found ${schema.fields.length} fields`);\n\n  if (schema.primaryField) {\n    console.log(`Detected primary field: ${schema.primaryField}`);\n  }\n\n  // Determine chunk field\n  chunkBy = options.chunkBy || suggestChunkField(schema, sample);\n  if (chunkBy) {\n    console.log(`Chunking by field: ${chunkBy}`);\n  }\n\n  // Parse indexed fields\n  indexedFields = options.index\n    ? options.index.split(\",\").map((f) => f.trim())\n    : schema.fields.filter((f) => f.indexed).map((f) => f.name);\n\n  if (indexedFields.length > 0) {\n    console.log(`Indexing fields: ${indexedFields.join(\", \")}`);\n  }\n\n  // Update schema with indexed fields\n  for (const field of schema.fields) {\n    field.indexed = indexedFields.includes(field.name);\n  }\n\n  // Second pass: process all records\n  console.log(\"Processing all records...\");\n  const { count } = await streamFile(\n    inputFile,\n    options.format,\n    processRecord,\n    1000,\n    { showProgress: true }\n  );\n\n  // Flush any remaining records\n  flushChunk();\n\n  console.log(`\\nProcessed ${count.toLocaleString()} records (format: ${format})`);\n\n  // Build config\n  const config: BuildConfig = {\n    chunkSize: targetChunkSize,\n    chunkBy: chunkBy || null,\n    indexedFields,\n  };\n\n  // Build manifest\n  const manifest: Manifest = {\n    version: VERSION,\n    generatedAt: new Date().toISOString(),\n    schema: schema!,\n    chunks: chunkMetas,\n    indices,\n    totalRecords: count,\n    config,\n  };\n\n  // Write manifest\n  const manifestPath = path.join(outputDir, \"manifest.json\");\n  await fs.promises.writeFile(manifestPath, JSON.stringify(manifest, null, 2));\n  console.log(`Wrote manifest to ${manifestPath}`);\n\n  // Generate client\n  console.log(\"Generating client...\");\n  const clientCode = generateClient(schema!, manifest);\n  const clientPath = path.join(outputDir, \"client.ts\");\n  await fs.promises.writeFile(clientPath, clientCode);\n  console.log(`Wrote client to ${clientPath}`);\n\n  const elapsed = ((Date.now() - startTime) / 1000).toFixed(2);\n  console.log(`\\nBuild completed in ${elapsed}s`);\n  console.log(`Output: ${outputDir}`);\n  console.log(`  - ${chunkMetas.length} chunks`);\n  console.log(`  - ${count.toLocaleString()} total records`);\n  console.log(`  - manifest.json`);\n  console.log(`  - client.ts`);\n\n  return {\n    manifest,\n    outputDir,\n    chunkCount: chunkMetas.length,\n    totalRecords: count,\n  };\n}\n\nfunction formatBytes(bytes: number): string {\n  if (bytes < 1024) return `${bytes} B`;\n  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)} KB`;\n  if (bytes < 1024 * 1024 * 1024) return `${(bytes / (1024 * 1024)).toFixed(1)} MB`;\n  return `${(bytes / (1024 * 1024 * 1024)).toFixed(1)} GB`;\n}\n","/**\n * Data file parsers for JSON, NDJSON, and CSV formats\n * Supports streaming for large files (2GB+)\n */\n\nimport * as fs from \"node:fs\";\nimport * as readline from \"node:readline\";\nimport Papa from \"papaparse\";\nimport streamJson from \"stream-json\";\nimport StreamArrayModule from \"stream-json/streamers/StreamArray.js\";\nimport streamChain from \"stream-chain\";\nimport cliProgress from \"cli-progress\";\nimport type { DataFormat, DataRecord } from \"../../types/index.js\";\n\nconst { parser: jsonParser } = streamJson;\nconst { chain } = streamChain;\nconst { streamArray } = StreamArrayModule;\n\n// Threshold for using streaming parser (100MB)\nconst STREAMING_THRESHOLD = 100 * 1024 * 1024;\n\nexport interface StreamOptions {\n  /** Show progress bar */\n  showProgress?: boolean;\n  /** Sample size for schema inference */\n  sampleSize?: number;\n  /** Stop after sampling (don't read entire file) */\n  sampleOnly?: boolean;\n}\n\n/**\n * Detect file format from extension or content\n */\nexport function detectFormat(filePath: string): DataFormat {\n  const ext = filePath.toLowerCase().split(\".\").pop();\n\n  if (ext === \"csv\") return \"csv\";\n  if (ext === \"ndjson\" || ext === \"jsonl\") return \"ndjson\";\n  if (ext === \"json\") {\n    // Peek at file to distinguish JSON array from NDJSON\n    const fd = fs.openSync(filePath, \"r\");\n    const buffer = Buffer.alloc(1024);\n    fs.readSync(fd, buffer, 0, 1024, 0);\n    fs.closeSync(fd);\n\n    const content = buffer.toString(\"utf-8\").trim();\n    if (content.startsWith(\"[\")) return \"json\";\n    if (content.startsWith(\"{\")) return \"ndjson\";\n  }\n\n  return \"json\"; // default\n}\n\n/**\n * Parse a JSON array file using streaming for large files\n */\nexport async function parseJsonArray(\n  filePath: string,\n  onRecord?: (record: DataRecord, index: number) => void\n): Promise<DataRecord[]> {\n  const stats = fs.statSync(filePath);\n\n  // Use streaming for large files\n  if (stats.size > STREAMING_THRESHOLD) {\n    return parseJsonArrayStreaming(filePath, onRecord);\n  }\n\n  // For smaller files, use standard parsing (faster)\n  const content = await fs.promises.readFile(filePath, \"utf-8\");\n  const data = JSON.parse(content);\n\n  if (!Array.isArray(data)) {\n    throw new Error(\"JSON file must contain an array of objects\");\n  }\n\n  if (onRecord) {\n    data.forEach((record, index) => onRecord(record as DataRecord, index));\n  }\n\n  return data as DataRecord[];\n}\n\n/**\n * Stream-parse a large JSON array file\n * Memory efficient - processes records one at a time\n */\nexport async function parseJsonArrayStreaming(\n  filePath: string,\n  onRecord?: (record: DataRecord, index: number) => void\n): Promise<DataRecord[]> {\n  return new Promise((resolve, reject) => {\n    const records: DataRecord[] = [];\n    let index = 0;\n    let lastLogTime = Date.now();\n\n    const pipeline = chain([\n      fs.createReadStream(filePath),\n      jsonParser(),\n      streamArray(),\n    ]);\n\n    pipeline.on(\"data\", (data: { key: number; value: DataRecord }) => {\n      const record = data.value;\n      records.push(record);\n      onRecord?.(record, index);\n      index++;\n\n      // Log progress every 5 seconds for large files\n      if (Date.now() - lastLogTime > 5000) {\n        console.log(`  Parsed ${index.toLocaleString()} records...`);\n        lastLogTime = Date.now();\n      }\n    });\n\n    pipeline.on(\"end\", () => {\n      resolve(records);\n    });\n\n    pipeline.on(\"error\", (err: Error) => {\n      reject(new Error(`JSON parse error: ${err.message}`));\n    });\n  });\n}\n\n/**\n * Parse an NDJSON file (newline-delimited JSON)\n * Streams line-by-line for memory efficiency\n */\nexport async function parseNdjson(\n  filePath: string,\n  onRecord?: (record: DataRecord, index: number) => void\n): Promise<DataRecord[]> {\n  const records: DataRecord[] = [];\n  let index = 0;\n\n  const fileStream = fs.createReadStream(filePath, { encoding: \"utf-8\" });\n  const rl = readline.createInterface({\n    input: fileStream,\n    crlfDelay: Infinity,\n  });\n\n  for await (const line of rl) {\n    const trimmed = line.trim();\n    if (!trimmed) continue;\n\n    try {\n      const record = JSON.parse(trimmed) as DataRecord;\n      records.push(record);\n      onRecord?.(record, index);\n      index++;\n    } catch (e) {\n      throw new Error(`Invalid JSON at line ${index + 1}: ${trimmed.slice(0, 50)}...`);\n    }\n  }\n\n  return records;\n}\n\n/**\n * Parse a CSV file\n * Uses papaparse for robust CSV handling\n */\nexport async function parseCsv(\n  filePath: string,\n  onRecord?: (record: DataRecord, index: number) => void\n): Promise<DataRecord[]> {\n  return new Promise((resolve, reject) => {\n    const records: DataRecord[] = [];\n    let index = 0;\n\n    const fileStream = fs.createReadStream(filePath, { encoding: \"utf-8\" });\n\n    Papa.parse(fileStream, {\n      header: true,\n      dynamicTyping: true,\n      skipEmptyLines: true,\n      step: (result) => {\n        if (result.errors.length > 0) {\n          reject(new Error(`CSV parse error: ${result.errors[0].message}`));\n          return;\n        }\n        const record = result.data as DataRecord;\n        records.push(record);\n        onRecord?.(record, index);\n        index++;\n      },\n      complete: () => resolve(records),\n      error: (error) => reject(error),\n    });\n  });\n}\n\n/**\n * Parse a data file in any supported format\n */\nexport async function parseFile(\n  filePath: string,\n  format?: DataFormat,\n  onRecord?: (record: DataRecord, index: number) => void\n): Promise<{ records: DataRecord[]; format: DataFormat }> {\n  const detectedFormat = format || detectFormat(filePath);\n\n  let records: DataRecord[];\n\n  switch (detectedFormat) {\n    case \"json\":\n      records = await parseJsonArray(filePath, onRecord);\n      break;\n    case \"ndjson\":\n      records = await parseNdjson(filePath, onRecord);\n      break;\n    case \"csv\":\n      records = await parseCsv(filePath, onRecord);\n      break;\n    default:\n      throw new Error(`Unsupported format: ${detectedFormat}`);\n  }\n\n  return { records, format: detectedFormat };\n}\n\n/**\n * Stream-only parsing - doesn't accumulate records in memory\n * Returns count and sample records for schema inference\n */\nexport async function streamFile(\n  filePath: string,\n  format: DataFormat | undefined,\n  onRecord: (record: DataRecord, index: number) => void,\n  sampleSize: number = 1000,\n  options: StreamOptions = {}\n): Promise<{ count: number; sample: DataRecord[]; format: DataFormat; estimatedCount?: number }> {\n  const detectedFormat = format || detectFormat(filePath);\n  const sample: DataRecord[] = [];\n  let count = 0;\n  let bytesProcessed = 0;\n  const fileSize = getFileSize(filePath);\n\n  // Setup progress bar if requested\n  let progressBar: cliProgress.SingleBar | null = null;\n  if (options.showProgress && !options.sampleOnly) {\n    progressBar = new cliProgress.SingleBar({\n      format: '  Processing |{bar}| {percentage}% | {value}/{total} MB | {records} records | ETA: {eta}s',\n      barCompleteChar: '\\u2588',\n      barIncompleteChar: '\\u2591',\n      hideCursor: true,\n    }, cliProgress.Presets.shades_classic);\n    progressBar.start(Math.round(fileSize / (1024 * 1024)), 0, { records: 0 });\n  }\n\n  const collector = (record: DataRecord, index: number, bytes?: number) => {\n    if (sample.length < sampleSize) {\n      sample.push(record);\n    }\n    count++;\n    if (bytes) bytesProcessed = bytes;\n\n    // Update progress bar every 1000 records\n    if (progressBar && count % 1000 === 0) {\n      progressBar.update(Math.round(bytesProcessed / (1024 * 1024)), { records: count.toLocaleString() });\n    }\n\n    onRecord(record, index);\n  };\n\n  // For sample-only mode, just read enough to get the sample\n  if (options.sampleOnly) {\n    switch (detectedFormat) {\n      case \"json\":\n        await streamJsonArraySample(filePath, collector, sampleSize);\n        break;\n      case \"ndjson\":\n        await streamNdjsonSample(filePath, collector, sampleSize);\n        break;\n      case \"csv\":\n        await streamCsvSample(filePath, collector, sampleSize);\n        break;\n      default:\n        throw new Error(`Unsupported format: ${detectedFormat}`);\n    }\n\n    // Estimate total count from sample\n    const avgRecordSize = bytesProcessed / count;\n    const estimatedCount = Math.round(fileSize / avgRecordSize);\n\n    return { count, sample, format: detectedFormat, estimatedCount };\n  }\n\n  switch (detectedFormat) {\n    case \"json\":\n      await streamJsonArray(filePath, collector, options.showProgress);\n      break;\n    case \"ndjson\":\n      await streamNdjson(filePath, collector, options.showProgress);\n      break;\n    case \"csv\":\n      await streamCsv(filePath, collector, options.showProgress);\n      break;\n    default:\n      throw new Error(`Unsupported format: ${detectedFormat}`);\n  }\n\n  if (progressBar) {\n    progressBar.update(Math.round(fileSize / (1024 * 1024)), { records: count.toLocaleString() });\n    progressBar.stop();\n  }\n\n  return { count, sample, format: detectedFormat };\n}\n\n/**\n * Stream JSON array without accumulating in memory\n */\nasync function streamJsonArray(\n  filePath: string,\n  onRecord: (record: DataRecord, index: number, bytes?: number) => void,\n  showProgress?: boolean\n): Promise<void> {\n  return new Promise((resolve, reject) => {\n    let index = 0;\n    let bytesRead = 0;\n\n    const readStream = fs.createReadStream(filePath);\n    readStream.on(\"data\", (chunk: Buffer) => {\n      bytesRead += chunk.length;\n    });\n\n    const pipeline = chain([\n      readStream,\n      jsonParser(),\n      streamArray(),\n    ]);\n\n    pipeline.on(\"data\", (data: { key: number; value: DataRecord }) => {\n      onRecord(data.value, index, bytesRead);\n      index++;\n    });\n\n    pipeline.on(\"end\", () => resolve());\n    pipeline.on(\"error\", (err: Error) => {\n      reject(new Error(`JSON parse error: ${err.message}`));\n    });\n  });\n}\n\n/**\n * Stream JSON array but stop after N records (for sampling)\n */\nasync function streamJsonArraySample(\n  filePath: string,\n  onRecord: (record: DataRecord, index: number, bytes?: number) => void,\n  maxRecords: number\n): Promise<void> {\n  return new Promise((resolve, reject) => {\n    let index = 0;\n    let bytesRead = 0;\n\n    const readStream = fs.createReadStream(filePath);\n    readStream.on(\"data\", (chunk: Buffer) => {\n      bytesRead += chunk.length;\n    });\n\n    const pipeline = chain([\n      readStream,\n      jsonParser(),\n      streamArray(),\n    ]);\n\n    pipeline.on(\"data\", (data: { key: number; value: DataRecord }) => {\n      onRecord(data.value, index, bytesRead);\n      index++;\n\n      if (index >= maxRecords) {\n        readStream.destroy();\n        resolve();\n      }\n    });\n\n    pipeline.on(\"end\", () => resolve());\n    pipeline.on(\"error\", (err: Error) => {\n      // Ignore errors from destroying the stream early\n      if (err.message.includes(\"aborted\") || err.message.includes(\"destroyed\")) {\n        resolve();\n      } else {\n        reject(new Error(`JSON parse error: ${err.message}`));\n      }\n    });\n  });\n}\n\n/**\n * Stream NDJSON without accumulating in memory\n */\nasync function streamNdjson(\n  filePath: string,\n  onRecord: (record: DataRecord, index: number, bytes?: number) => void,\n  showProgress?: boolean\n): Promise<void> {\n  let index = 0;\n  let bytesRead = 0;\n\n  const fileStream = fs.createReadStream(filePath, { encoding: \"utf-8\" });\n  fileStream.on(\"data\", (chunk: string) => {\n    bytesRead += Buffer.byteLength(chunk);\n  });\n\n  const rl = readline.createInterface({\n    input: fileStream,\n    crlfDelay: Infinity,\n  });\n\n  for await (const line of rl) {\n    const trimmed = line.trim();\n    if (!trimmed) continue;\n\n    try {\n      const record = JSON.parse(trimmed) as DataRecord;\n      onRecord(record, index, bytesRead);\n      index++;\n    } catch (e) {\n      throw new Error(`Invalid JSON at line ${index + 1}: ${trimmed.slice(0, 50)}...`);\n    }\n  }\n}\n\n/**\n * Stream NDJSON but stop after N records (for sampling)\n */\nasync function streamNdjsonSample(\n  filePath: string,\n  onRecord: (record: DataRecord, index: number, bytes?: number) => void,\n  maxRecords: number\n): Promise<void> {\n  let index = 0;\n  let bytesRead = 0;\n\n  const fileStream = fs.createReadStream(filePath, { encoding: \"utf-8\" });\n  fileStream.on(\"data\", (chunk: string) => {\n    bytesRead += Buffer.byteLength(chunk);\n  });\n\n  const rl = readline.createInterface({\n    input: fileStream,\n    crlfDelay: Infinity,\n  });\n\n  for await (const line of rl) {\n    const trimmed = line.trim();\n    if (!trimmed) continue;\n\n    try {\n      const record = JSON.parse(trimmed) as DataRecord;\n      onRecord(record, index, bytesRead);\n      index++;\n\n      if (index >= maxRecords) {\n        rl.close();\n        fileStream.destroy();\n        return;\n      }\n    } catch (e) {\n      throw new Error(`Invalid JSON at line ${index + 1}: ${trimmed.slice(0, 50)}...`);\n    }\n  }\n}\n\n/**\n * Stream CSV without accumulating in memory\n */\nasync function streamCsv(\n  filePath: string,\n  onRecord: (record: DataRecord, index: number, bytes?: number) => void,\n  showProgress?: boolean\n): Promise<void> {\n  return new Promise((resolve, reject) => {\n    let index = 0;\n    let bytesRead = 0;\n\n    const fileStream = fs.createReadStream(filePath, { encoding: \"utf-8\" });\n    fileStream.on(\"data\", (chunk: string) => {\n      bytesRead += Buffer.byteLength(chunk);\n    });\n\n    Papa.parse(fileStream, {\n      header: true,\n      dynamicTyping: true,\n      skipEmptyLines: true,\n      step: (result) => {\n        if (result.errors.length > 0) {\n          reject(new Error(`CSV parse error: ${result.errors[0].message}`));\n          return;\n        }\n        onRecord(result.data as DataRecord, index, bytesRead);\n        index++;\n      },\n      complete: () => resolve(),\n      error: (error) => reject(error),\n    });\n  });\n}\n\n/**\n * Stream CSV but stop after N records (for sampling)\n */\nasync function streamCsvSample(\n  filePath: string,\n  onRecord: (record: DataRecord, index: number, bytes?: number) => void,\n  maxRecords: number\n): Promise<void> {\n  return new Promise((resolve, reject) => {\n    let index = 0;\n    let bytesRead = 0;\n    let resolved = false;\n\n    const fileStream = fs.createReadStream(filePath, { encoding: \"utf-8\" });\n    fileStream.on(\"data\", (chunk: string) => {\n      bytesRead += Buffer.byteLength(chunk);\n    });\n\n    Papa.parse(fileStream, {\n      header: true,\n      dynamicTyping: true,\n      skipEmptyLines: true,\n      step: (result, parser) => {\n        if (resolved) return;\n\n        if (result.errors.length > 0) {\n          reject(new Error(`CSV parse error: ${result.errors[0].message}`));\n          return;\n        }\n        onRecord(result.data as DataRecord, index, bytesRead);\n        index++;\n\n        if (index >= maxRecords) {\n          resolved = true;\n          parser.abort();\n          fileStream.destroy();\n          resolve();\n        }\n      },\n      complete: () => {\n        if (!resolved) resolve();\n      },\n      error: (error) => reject(error),\n    });\n  });\n}\n\n/**\n * Get file size in bytes\n */\nexport function getFileSize(filePath: string): number {\n  const stats = fs.statSync(filePath);\n  return stats.size;\n}\n\n/**\n * Parse size string (e.g., \"5mb\", \"1gb\") to bytes\n */\nexport function parseSize(sizeStr: string): number {\n  const match = sizeStr.toLowerCase().match(/^(\\d+(?:\\.\\d+)?)\\s*(b|kb|mb|gb)?$/);\n  if (!match) {\n    throw new Error(`Invalid size format: ${sizeStr}. Use format like \"5mb\" or \"1gb\"`);\n  }\n\n  const value = parseFloat(match[1]);\n  const unit = match[2] || \"b\";\n\n  const multipliers: Record<string, number> = {\n    b: 1,\n    kb: 1024,\n    mb: 1024 * 1024,\n    gb: 1024 * 1024 * 1024,\n  };\n\n  return Math.floor(value * multipliers[unit]);\n}\n","/**\n * Schema inference from data records\n */\n\nimport type { DataRecord, FieldSchema, FieldStats, FieldType, Schema } from \"../../types/index.js\";\n\n/**\n * Infer the type of a value\n */\nfunction inferType(value: unknown): FieldType {\n  if (value === null || value === undefined) {\n    return \"null\";\n  }\n\n  const type = typeof value;\n\n  if (type === \"string\") {\n    // Check if it's a date string\n    if (isDateString(value as string)) {\n      return \"date\";\n    }\n    return \"string\";\n  }\n\n  if (type === \"number\") {\n    return \"number\";\n  }\n\n  if (type === \"boolean\") {\n    return \"boolean\";\n  }\n\n  // Arrays and objects treated as string (JSON serialized)\n  return \"string\";\n}\n\n/**\n * Check if a string looks like a date\n */\nfunction isDateString(value: string): boolean {\n  // ISO 8601 date patterns\n  const isoPattern = /^\\d{4}-\\d{2}-\\d{2}(T\\d{2}:\\d{2}:\\d{2}(\\.\\d{3})?(Z|[+-]\\d{2}:?\\d{2})?)?$/;\n  if (isoPattern.test(value)) {\n    const date = new Date(value);\n    return !isNaN(date.getTime());\n  }\n  return false;\n}\n\n/**\n * Merge two field types, returning the more general type\n */\nfunction mergeTypes(type1: FieldType, type2: FieldType): FieldType {\n  if (type1 === type2) return type1;\n  if (type1 === \"null\") return type2;\n  if (type2 === \"null\") return type1;\n\n  // If types differ, fall back to string\n  return \"string\";\n}\n\n/**\n * Collect statistics for a field across all records\n */\nclass FieldStatsCollector {\n  private values = new Set<string>();\n  private min: string | number | undefined;\n  private max: string | number | undefined;\n  private nullCount = 0;\n  private count = 0;\n  private type: FieldType = \"null\";\n  private sampleValues: (string | number | boolean | null)[] = [];\n\n  add(value: unknown): void {\n    this.count++;\n\n    if (value === null || value === undefined) {\n      this.nullCount++;\n      return;\n    }\n\n    const valueType = inferType(value);\n    this.type = mergeTypes(this.type, valueType);\n\n    // Track unique values for cardinality (limit to avoid memory issues)\n    if (this.values.size < 10000) {\n      this.values.add(String(value));\n    }\n\n    // Collect sample values\n    if (this.sampleValues.length < 5 && !this.sampleValues.includes(value as string | number | boolean | null)) {\n      this.sampleValues.push(value as string | number | boolean | null);\n    }\n\n    // Track min/max for numbers and dates\n    if (typeof value === \"number\") {\n      if (this.min === undefined || value < (this.min as number)) {\n        this.min = value;\n      }\n      if (this.max === undefined || value > (this.max as number)) {\n        this.max = value;\n      }\n    } else if (typeof value === \"string\" && (this.type === \"date\" || this.type === \"string\")) {\n      if (this.min === undefined || value < this.min) {\n        this.min = value;\n      }\n      if (this.max === undefined || value > this.max) {\n        this.max = value;\n      }\n    }\n  }\n\n  getStats(): FieldStats {\n    return {\n      min: this.min,\n      max: this.max,\n      cardinality: this.values.size,\n      nullCount: this.nullCount,\n      sampleValues: this.sampleValues,\n    };\n  }\n\n  getType(): FieldType {\n    return this.type;\n  }\n\n  isNullable(): boolean {\n    return this.nullCount > 0;\n  }\n}\n\n/**\n * Determine if a field should be indexed based on various heuristics\n */\nfunction shouldIndex(\n  name: string,\n  type: FieldType,\n  cardinality: number,\n  totalRecords: number,\n  stats: FieldStats\n): boolean {\n  // Skip if cardinality is 1 (all same value - useless index)\n  if (cardinality <= 1) {\n    return false;\n  }\n\n  // Skip if cardinality is too high (over 50% of records)\n  if (cardinality > totalRecords * 0.5) {\n    return false;\n  }\n\n  // Skip if cardinality exceeds reasonable index size\n  if (cardinality > 1000) {\n    return false;\n  }\n\n  // Check sample values for patterns that indicate non-indexable data\n  const samples = stats.sampleValues || [];\n  for (const sample of samples) {\n    if (sample === null || sample === undefined) continue;\n    const str = String(sample);\n\n    // Skip stringified objects\n    if (str.startsWith(\"[object \") || str === \"[object Object]\") {\n      return false;\n    }\n\n    // Skip URLs (not useful for filtering)\n    if (str.includes(\"://\")) {\n      return false;\n    }\n\n    // Skip very long values (likely URIs, hashes, or encoded data)\n    if (str.length > 100) {\n      return false;\n    }\n  }\n\n  // Skip fields with names suggesting they're not good filter targets\n  const nameLower = name.toLowerCase();\n  const skipPatterns = [\n    \"_uri\", \"_url\", \"_id\", // ID/URL suffixes (unless it's a category ID)\n    \"uri\", \"url\", \"href\", \"link\", // URL fields\n    \"hash\", \"token\", \"secret\", \"key\", // Security/hash fields\n    \"description\", \"text\", \"content\", \"body\", // Long text fields\n  ];\n\n  // But allow specific useful ID patterns\n  const allowPatterns = [\n    \"category\", \"type\", \"status\", \"state\", \"level\",\n    \"color\", \"lang\", \"rarity\", \"set\", \"frame\",\n  ];\n\n  const hasSkipPattern = skipPatterns.some(p => nameLower.includes(p));\n  const hasAllowPattern = allowPatterns.some(p => nameLower.includes(p));\n\n  if (hasSkipPattern && !hasAllowPattern) {\n    return false;\n  }\n\n  // Good candidates: booleans, low-cardinality strings, enums\n  if (type === \"boolean\") {\n    return true;\n  }\n\n  // Low cardinality is good for filtering\n  return cardinality >= 2 && cardinality <= 500;\n}\n\n/**\n * Infer schema from a set of records\n */\nexport function inferSchema(records: DataRecord[]): Schema {\n  if (records.length === 0) {\n    return { fields: [], primaryField: null };\n  }\n\n  // Collect all field names\n  const fieldNames = new Set<string>();\n  for (const record of records) {\n    for (const key of Object.keys(record)) {\n      fieldNames.add(key);\n    }\n  }\n\n  // Collect stats for each field\n  const collectors = new Map<string, FieldStatsCollector>();\n  for (const name of fieldNames) {\n    collectors.set(name, new FieldStatsCollector());\n  }\n\n  for (const record of records) {\n    for (const name of fieldNames) {\n      const collector = collectors.get(name)!;\n      collector.add(record[name]);\n    }\n  }\n\n  // Build field schemas\n  const fields: FieldSchema[] = [];\n  let primaryField: string | null = null;\n\n  for (const name of fieldNames) {\n    const collector = collectors.get(name)!;\n    const stats = collector.getStats();\n    const type = collector.getType();\n\n    // Determine if field should be indexed\n    const cardinality = stats.cardinality;\n    const indexed = shouldIndex(name, type, cardinality, records.length, stats);\n\n    // Detect potential primary key\n    // High cardinality, not nullable, unique values\n    if (\n      primaryField === null &&\n      stats.cardinality === records.length &&\n      !collector.isNullable() &&\n      (name.toLowerCase().includes(\"id\") || name.toLowerCase() === \"key\")\n    ) {\n      primaryField = name;\n    }\n\n    fields.push({\n      name,\n      type,\n      nullable: collector.isNullable(),\n      indexed,\n      stats,\n    });\n  }\n\n  // Sort fields: primary key first, then alphabetically\n  fields.sort((a, b) => {\n    if (a.name === primaryField) return -1;\n    if (b.name === primaryField) return 1;\n    return a.name.localeCompare(b.name);\n  });\n\n  return { fields, primaryField };\n}\n\n/**\n * Suggest the best field to chunk by\n */\nexport function suggestChunkField(schema: Schema, records: DataRecord[]): string | null {\n  // If there's a primary field, chunk by it\n  if (schema.primaryField) {\n    return schema.primaryField;\n  }\n\n  // Look for a good chunking candidate:\n  // - High cardinality (spreads data across chunks)\n  // - Numeric or date (supports range queries)\n  // - Not nullable\n  let bestField: string | null = null;\n  let bestScore = 0;\n\n  for (const field of schema.fields) {\n    if (field.nullable) continue;\n\n    let score = 0;\n\n    // Prefer numeric/date fields\n    if (field.type === \"number\" || field.type === \"date\") {\n      score += 50;\n    }\n\n    // Prefer higher cardinality (but not too high)\n    const cardinalityRatio = field.stats.cardinality / records.length;\n    if (cardinalityRatio > 0.1 && cardinalityRatio <= 1) {\n      score += cardinalityRatio * 30;\n    }\n\n    // Prefer fields with \"id\", \"key\", \"date\", \"time\" in name\n    const nameLower = field.name.toLowerCase();\n    if (nameLower.includes(\"id\")) score += 20;\n    if (nameLower.includes(\"date\") || nameLower.includes(\"time\")) score += 15;\n    if (nameLower.includes(\"created\") || nameLower.includes(\"updated\")) score += 10;\n\n    if (score > bestScore) {\n      bestScore = score;\n      bestField = field.name;\n    }\n  }\n\n  return bestField;\n}\n\n/**\n * Get fields that should be indexed based on cardinality\n */\nexport function getIndexableFields(schema: Schema): string[] {\n  return schema.fields\n    .filter((f) => f.indexed)\n    .map((f) => f.name);\n}\n","/**\n * Data chunking logic for splitting records into smaller files\n */\n\nimport type { ChunkMeta, DataRecord, Schema } from \"../../types/index.js\";\n\nexport interface Chunk {\n  id: string;\n  records: DataRecord[];\n  byteSize: number;\n}\n\nexport interface ChunkOptions {\n  targetSize: number; // target bytes per chunk\n  chunkBy?: string; // field to sort/chunk by\n}\n\n/**\n * Estimate JSON byte size of a record\n */\nfunction estimateRecordSize(record: DataRecord): number {\n  return JSON.stringify(record).length;\n}\n\n/**\n * Compare two values for sorting\n */\nfunction compareValues(a: unknown, b: unknown): number {\n  if (a === b) return 0;\n  if (a === null || a === undefined) return -1;\n  if (b === null || b === undefined) return 1;\n\n  if (typeof a === \"number\" && typeof b === \"number\") {\n    return a - b;\n  }\n\n  return String(a).localeCompare(String(b));\n}\n\n/**\n * Split records into chunks based on target size\n */\nexport function chunkRecords(\n  records: DataRecord[],\n  schema: Schema,\n  options: ChunkOptions\n): Chunk[] {\n  const { targetSize, chunkBy } = options;\n\n  // Sort records if chunkBy field is specified\n  let sortedRecords = records;\n  if (chunkBy) {\n    sortedRecords = [...records].sort((a, b) =>\n      compareValues(a[chunkBy], b[chunkBy])\n    );\n  }\n\n  const chunks: Chunk[] = [];\n  let currentChunk: DataRecord[] = [];\n  let currentSize = 0;\n  let chunkIndex = 0;\n\n  // Account for JSON array overhead: [ and ] plus commas\n  const arrayOverhead = 2; // []\n  const commaOverhead = 1; // ,\n\n  for (const record of sortedRecords) {\n    const recordSize = estimateRecordSize(record) + commaOverhead;\n\n    // If adding this record would exceed target size and we have records,\n    // start a new chunk\n    if (currentSize + recordSize > targetSize && currentChunk.length > 0) {\n      chunks.push({\n        id: String(chunkIndex),\n        records: currentChunk,\n        byteSize: currentSize + arrayOverhead,\n      });\n      chunkIndex++;\n      currentChunk = [];\n      currentSize = 0;\n    }\n\n    currentChunk.push(record);\n    currentSize += recordSize;\n  }\n\n  // Don't forget the last chunk\n  if (currentChunk.length > 0) {\n    chunks.push({\n      id: String(chunkIndex),\n      records: currentChunk,\n      byteSize: currentSize + arrayOverhead,\n    });\n  }\n\n  return chunks;\n}\n\n/**\n * Calculate field ranges for a chunk (min/max per field)\n */\nexport function calculateFieldRanges(\n  records: DataRecord[],\n  schema: Schema\n): ChunkMeta[\"fieldRanges\"] {\n  const ranges: ChunkMeta[\"fieldRanges\"] = {};\n\n  for (const field of schema.fields) {\n    const values = records\n      .map((r) => r[field.name])\n      .filter((v) => v !== null && v !== undefined);\n\n    if (values.length === 0) {\n      continue;\n    }\n\n    // For numbers and dates, calculate actual min/max\n    if (field.type === \"number\" || field.type === \"date\" || field.type === \"string\") {\n      const sorted = [...values].sort(compareValues);\n      ranges[field.name] = {\n        min: sorted[0],\n        max: sorted[sorted.length - 1],\n      };\n    }\n  }\n\n  return ranges;\n}\n\n/**\n * Generate chunk metadata\n */\nexport function generateChunkMeta(\n  chunk: Chunk,\n  schema: Schema,\n  basePath: string\n): ChunkMeta {\n  return {\n    id: chunk.id,\n    path: `${basePath}/${chunk.id}.json`,\n    count: chunk.records.length,\n    byteSize: chunk.byteSize,\n    fieldRanges: calculateFieldRanges(chunk.records, schema),\n  };\n}\n\n/**\n * Serialize a chunk to JSON string\n */\nexport function serializeChunk(chunk: Chunk): string {\n  return JSON.stringify(chunk.records);\n}\n\n/**\n * Calculate optimal chunk count based on data size and target chunk size\n */\nexport function calculateChunkCount(\n  totalRecords: number,\n  totalSize: number,\n  targetChunkSize: number\n): number {\n  const estimatedChunks = Math.ceil(totalSize / targetChunkSize);\n  // At least 1 chunk, and reasonable upper bound\n  return Math.max(1, Math.min(estimatedChunks, totalRecords));\n}\n","/**\n * Index generation for efficient query pruning\n */\n\nimport type { Chunk } from \"./chunker.js\";\nimport type { Schema } from \"../../types/index.js\";\n\n/**\n * Inverted index: field -> value -> chunk IDs\n */\nexport type InvertedIndex = Record<string, Record<string, string[]>>;\n\n/**\n * Build inverted indices for indexed fields\n * Maps each unique value to the chunk IDs that contain it\n */\nexport function buildInvertedIndices(\n  chunks: Chunk[],\n  schema: Schema,\n  indexedFields?: string[]\n): InvertedIndex {\n  const indices: InvertedIndex = {};\n\n  // Get fields to index\n  const fieldsToIndex = indexedFields\n    ? schema.fields.filter((f) => indexedFields.includes(f.name))\n    : schema.fields.filter((f) => f.indexed);\n\n  for (const field of fieldsToIndex) {\n    const fieldIndex: Record<string, string[]> = {};\n\n    for (const chunk of chunks) {\n      const valuesInChunk = new Set<string>();\n\n      for (const record of chunk.records) {\n        const value = record[field.name];\n        if (value !== null && value !== undefined) {\n          valuesInChunk.add(String(value));\n        }\n      }\n\n      // Add chunk ID to each value's list\n      for (const value of valuesInChunk) {\n        if (!fieldIndex[value]) {\n          fieldIndex[value] = [];\n        }\n        fieldIndex[value].push(chunk.id);\n      }\n    }\n\n    // Only include index if it would be useful\n    // (not too many unique values, which would make the index huge)\n    const uniqueValues = Object.keys(fieldIndex).length;\n    if (uniqueValues <= 10000) {\n      indices[field.name] = fieldIndex;\n    }\n  }\n\n  return indices;\n}\n\n/**\n * Find chunk IDs that might contain records matching a field value\n */\nexport function findChunksForValue(\n  indices: InvertedIndex,\n  fieldName: string,\n  value: unknown\n): string[] | null {\n  const fieldIndex = indices[fieldName];\n  if (!fieldIndex) {\n    // Field not indexed, return null to indicate all chunks must be searched\n    return null;\n  }\n\n  const stringValue = String(value);\n  return fieldIndex[stringValue] || [];\n}\n\n/**\n * Find chunk IDs that match multiple field conditions (AND logic)\n */\nexport function findChunksForConditions(\n  indices: InvertedIndex,\n  conditions: Record<string, unknown>\n): string[] | null {\n  let resultChunks: Set<string> | null = null;\n\n  for (const [field, value] of Object.entries(conditions)) {\n    const chunks = findChunksForValue(indices, field, value);\n\n    if (chunks === null) {\n      // This field isn't indexed, can't narrow down\n      continue;\n    }\n\n    if (resultChunks === null) {\n      resultChunks = new Set(chunks);\n    } else {\n      // Intersect with existing results\n      resultChunks = new Set([...resultChunks].filter((c) => chunks.includes(c)));\n    }\n\n    // Early exit if no chunks match\n    if (resultChunks.size === 0) {\n      return [];\n    }\n  }\n\n  return resultChunks ? [...resultChunks] : null;\n}\n\n/**\n * Estimate index size in bytes\n */\nexport function estimateIndexSize(indices: InvertedIndex): number {\n  return JSON.stringify(indices).length;\n}\n","/**\n * TypeScript client code generator\n */\n\nimport type { FieldSchema, Manifest, Schema } from \"../../types/index.js\";\n\n/**\n * Convert field type to TypeScript type\n */\nfunction fieldTypeToTs(field: FieldSchema): string {\n  let tsType: string;\n\n  switch (field.type) {\n    case \"string\":\n    case \"date\":\n      tsType = \"string\";\n      break;\n    case \"number\":\n      tsType = \"number\";\n      break;\n    case \"boolean\":\n      tsType = \"boolean\";\n      break;\n    case \"null\":\n      tsType = \"null\";\n      break;\n    default:\n      tsType = \"unknown\";\n  }\n\n  if (field.nullable) {\n    tsType += \" | null\";\n  }\n\n  return tsType;\n}\n\n/**\n * Generate TypeScript interface for records\n * Note: We use \"Item\" instead of \"Record\" to avoid shadowing TypeScript's built-in Record type\n */\nfunction generateRecordInterface(schema: Schema): string {\n  const fields = schema.fields\n    .map((f) => `  ${f.name}: ${fieldTypeToTs(f)};`)\n    .join(\"\\n\");\n\n  return `export interface Item {\n${fields}\n}`;\n}\n\n/**\n * Generate field names type\n */\nfunction generateFieldNamesType(schema: Schema): string {\n  const names = schema.fields.map((f) => `\"${f.name}\"`).join(\" | \");\n  return `export type FieldName = ${names};`;\n}\n\n/**\n * Generate where clause types\n */\nfunction generateWhereTypes(schema: Schema): string {\n  const stringFields = schema.fields\n    .filter((f) => f.type === \"string\" || f.type === \"date\")\n    .map((f) => f.name);\n\n  const numericFields = schema.fields\n    .filter((f) => f.type === \"number\")\n    .map((f) => f.name);\n\n  return `\nexport type StringOperators = {\n  eq?: string;\n  neq?: string;\n  contains?: string;\n  startsWith?: string;\n  endsWith?: string;\n  in?: string[];\n};\n\nexport type NumericOperators = {\n  eq?: number;\n  neq?: number;\n  gt?: number;\n  gte?: number;\n  lt?: number;\n  lte?: number;\n  in?: number[];\n};\n\nexport type WhereClause = {\n${schema.fields\n  .map((f) => {\n    if (f.type === \"number\") {\n      return `  ${f.name}?: number | NumericOperators;`;\n    } else if (f.type === \"boolean\") {\n      return `  ${f.name}?: boolean;`;\n    } else {\n      return `  ${f.name}?: string | StringOperators;`;\n    }\n  })\n  .join(\"\\n\")}\n};`;\n}\n\n/**\n * Generate the full client code\n */\nexport function generateClient(schema: Schema, manifest: Manifest): string {\n  const recordInterface = generateRecordInterface(schema);\n  const fieldNamesType = generateFieldNamesType(schema);\n  const whereTypes = generateWhereTypes(schema);\n\n  const sortableFields = schema.fields\n    .filter((f) => f.type === \"number\" || f.type === \"string\" || f.type === \"date\")\n    .map((f) => `\"${f.name}\"`)\n    .join(\" | \");\n\n  return `/**\n * Auto-generated client for static-shard\n * Generated at: ${manifest.generatedAt}\n * Total records: ${manifest.totalRecords}\n * Chunks: ${manifest.chunks.length}\n */\n\n// ============================================================================\n// Types\n// ============================================================================\n\n${recordInterface}\n\n${fieldNamesType}\n\n${whereTypes}\n\nexport type SortableField = ${sortableFields || \"string\"};\n\nexport interface QueryOptions {\n  where?: WhereClause;\n  orderBy?: SortableField | { field: SortableField; direction: \"asc\" | \"desc\" };\n  limit?: number;\n  offset?: number;\n}\n\nexport interface Manifest {\n  version: string;\n  schema: {\n    fields: Array<{\n      name: string;\n      type: string;\n      nullable: boolean;\n      indexed: boolean;\n    }>;\n    primaryField: string | null;\n  };\n  chunks: Array<{\n    id: string;\n    path: string;\n    count: number;\n    byteSize: number;\n    fieldRanges: { [field: string]: { min: unknown; max: unknown } };\n  }>;\n  indices: { [field: string]: { [value: string]: string[] } };\n  totalRecords: number;\n}\n\n// ============================================================================\n// Runtime\n// ============================================================================\n\ninterface ClientOptions {\n  basePath: string;\n}\n\nclass StaticShardClient {\n  private basePath: string;\n  private manifest: Manifest | null = null;\n  private chunkCache: Map<string, Item[]> = new Map();\n\n  constructor(options: ClientOptions) {\n    this.basePath = options.basePath.replace(/\\\\/$/, \"\");\n  }\n\n  /**\n   * Load the manifest file\n   */\n  private async loadManifest(): Promise<Manifest> {\n    if (this.manifest) return this.manifest;\n\n    const response = await fetch(\\`\\${this.basePath}/manifest.json\\`);\n    if (!response.ok) {\n      throw new Error(\\`Failed to load manifest: \\${response.statusText}\\`);\n    }\n\n    this.manifest = await response.json();\n    return this.manifest!;\n  }\n\n  /**\n   * Load a chunk by ID\n   */\n  private async loadChunk(chunkId: string): Promise<Item[]> {\n    const cached = this.chunkCache.get(chunkId);\n    if (cached) return cached;\n\n    const manifest = await this.loadManifest();\n    const chunkMeta = manifest.chunks.find((c) => c.id === chunkId);\n    if (!chunkMeta) {\n      throw new Error(\\`Chunk not found: \\${chunkId}\\`);\n    }\n\n    const response = await fetch(\\`\\${this.basePath}/\\${chunkMeta.path}\\`);\n    if (!response.ok) {\n      throw new Error(\\`Failed to load chunk \\${chunkId}: \\${response.statusText}\\`);\n    }\n\n    const records = await response.json();\n    this.chunkCache.set(chunkId, records);\n    return records;\n  }\n\n  /**\n   * Find chunk IDs that might contain matching records\n   */\n  private findCandidateChunks(manifest: Manifest, where?: WhereClause): string[] {\n    if (!where) {\n      return manifest.chunks.map((c) => c.id);\n    }\n\n    let candidateChunks: Set<string> | null = null;\n\n    for (const [field, condition] of Object.entries(where)) {\n      // Check if we can use the index\n      const index = manifest.indices[field];\n\n      if (index && (typeof condition === \"string\" || typeof condition === \"number\" || typeof condition === \"boolean\")) {\n        const value = String(condition);\n        const chunks = index[value] || [];\n\n        if (candidateChunks === null) {\n          candidateChunks = new Set(chunks);\n        } else {\n          candidateChunks = new Set(Array.from(candidateChunks).filter((c) => chunks.includes(c)));\n        }\n      } else if (typeof condition === \"object\" && condition !== null && \"eq\" in condition) {\n        const value = String(condition.eq);\n        const chunks = index?.[value] || [];\n\n        if (index) {\n          if (candidateChunks === null) {\n            candidateChunks = new Set(chunks);\n          } else {\n            candidateChunks = new Set(Array.from(candidateChunks).filter((c) => chunks.includes(c)));\n          }\n        }\n      }\n\n      // Range pruning using fieldRanges\n      if (typeof condition === \"object\" && condition !== null) {\n        const rangeCondition = condition as { gt?: number; gte?: number; lt?: number; lte?: number };\n        const hasRangeOp = \"gt\" in rangeCondition || \"gte\" in rangeCondition || \"lt\" in rangeCondition || \"lte\" in rangeCondition;\n\n        if (hasRangeOp) {\n          const matchingChunks = manifest.chunks\n            .filter((chunk) => {\n              const range = chunk.fieldRanges[field];\n              if (!range) return true; // Can't prune, include chunk\n\n              const min = range.min as number;\n              const max = range.max as number;\n\n              if (rangeCondition.gt !== undefined && max <= rangeCondition.gt) return false;\n              if (rangeCondition.gte !== undefined && max < rangeCondition.gte) return false;\n              if (rangeCondition.lt !== undefined && min >= rangeCondition.lt) return false;\n              if (rangeCondition.lte !== undefined && min > rangeCondition.lte) return false;\n\n              return true;\n            })\n            .map((c) => c.id);\n\n          if (candidateChunks === null) {\n            candidateChunks = new Set(matchingChunks);\n          } else {\n            candidateChunks = new Set(Array.from(candidateChunks).filter((c) => matchingChunks.includes(c)));\n          }\n        }\n      }\n    }\n\n    return candidateChunks ? Array.from(candidateChunks) : manifest.chunks.map((c) => c.id);\n  }\n\n  /**\n   * Check if a record matches the where clause\n   */\n  private matchesWhere(record: Item, where?: WhereClause): boolean {\n    if (!where) return true;\n\n    for (const [field, condition] of Object.entries(where)) {\n      const value = record[field as keyof Item];\n\n      // Direct value comparison\n      if (typeof condition !== \"object\" || condition === null) {\n        if (value !== condition) return false;\n        continue;\n      }\n\n      const ops = condition as StringOperators & NumericOperators;\n\n      if (\"eq\" in ops && value !== ops.eq) return false;\n      if (\"neq\" in ops && value === ops.neq) return false;\n      if (\"gt\" in ops && (typeof value !== \"number\" || value <= ops.gt!)) return false;\n      if (\"gte\" in ops && (typeof value !== \"number\" || value < ops.gte!)) return false;\n      if (\"lt\" in ops && (typeof value !== \"number\" || value >= ops.lt!)) return false;\n      if (\"lte\" in ops && (typeof value !== \"number\" || value > ops.lte!)) return false;\n      if (\"contains\" in ops && (typeof value !== \"string\" || !value.includes(ops.contains!))) return false;\n      if (\"startsWith\" in ops && (typeof value !== \"string\" || !value.startsWith(ops.startsWith!))) return false;\n      if (\"endsWith\" in ops && (typeof value !== \"string\" || !value.endsWith(ops.endsWith!))) return false;\n      if (\"in\" in ops && !(ops.in as unknown[])!.includes(value)) return false;\n    }\n\n    return true;\n  }\n\n  /**\n   * Query records\n   */\n  async query(options: QueryOptions = {}): Promise<Item[]> {\n    const manifest = await this.loadManifest();\n\n    // Find candidate chunks\n    const candidateChunkIds = this.findCandidateChunks(manifest, options.where);\n\n    // Load chunks in parallel\n    const chunkPromises = candidateChunkIds.map((id) => this.loadChunk(id));\n    const chunks = await Promise.all(chunkPromises);\n\n    // Flatten and filter\n    let results: Item[] = [];\n    for (const chunk of chunks) {\n      for (const record of chunk) {\n        if (this.matchesWhere(record, options.where)) {\n          results.push(record);\n        }\n      }\n    }\n\n    // Sort\n    if (options.orderBy) {\n      const field = typeof options.orderBy === \"string\" ? options.orderBy : options.orderBy.field;\n      const direction = typeof options.orderBy === \"string\" ? \"asc\" : options.orderBy.direction;\n\n      results.sort((a, b) => {\n        const aVal = a[field as keyof Item];\n        const bVal = b[field as keyof Item];\n\n        if (aVal === bVal) return 0;\n        if (aVal === null || aVal === undefined) return 1;\n        if (bVal === null || bVal === undefined) return -1;\n\n        const cmp = aVal < bVal ? -1 : 1;\n        return direction === \"asc\" ? cmp : -cmp;\n      });\n    }\n\n    // Pagination\n    const offset = options.offset || 0;\n    const limit = options.limit;\n\n    if (offset > 0 || limit !== undefined) {\n      results = results.slice(offset, limit !== undefined ? offset + limit : undefined);\n    }\n\n    return results;\n  }\n\n  /**\n   * Get a single record by primary key\n   */\n  async get(id: string | number): Promise<Item | null> {\n    const manifest = await this.loadManifest();\n    const primaryField = manifest.schema.primaryField;\n\n    if (!primaryField) {\n      throw new Error(\"No primary field defined in schema\");\n    }\n\n    const results = await this.query({\n      where: { [primaryField]: id } as WhereClause,\n      limit: 1,\n    });\n\n    return results[0] || null;\n  }\n\n  /**\n   * Count records matching a query\n   */\n  async count(options: { where?: WhereClause } = {}): Promise<number> {\n    const manifest = await this.loadManifest();\n\n    if (!options.where) {\n      return manifest.totalRecords;\n    }\n\n    // For complex queries, we need to load and count\n    const results = await this.query({ where: options.where });\n    return results.length;\n  }\n\n  /**\n   * Get schema information\n   */\n  async getSchema(): Promise<Manifest[\"schema\"]> {\n    const manifest = await this.loadManifest();\n    return manifest.schema;\n  }\n\n  /**\n   * Clear the chunk cache\n   */\n  clearCache(): void {\n    this.chunkCache.clear();\n  }\n}\n\n// ============================================================================\n// Export\n// ============================================================================\n\nexport function createClient(options: ClientOptions): StaticShardClient {\n  return new StaticShardClient(options);\n}\n\n// Default export for convenience\nexport const db = createClient({ basePath: \".\" });\n`;\n}\n","/**\n * Inspect command - analyze data file and suggest chunking strategy\n * Supports streaming for large files\n */\n\nimport * as fs from \"node:fs\";\nimport type { InspectOptions } from \"../../types/index.js\";\nimport { getFileSize, parseFile, streamFile } from \"../utils/parsers.js\";\nimport { inferSchema, suggestChunkField, getIndexableFields } from \"../utils/schema.js\";\n\n// Use streaming for files larger than 100MB\nconst STREAMING_THRESHOLD = 100 * 1024 * 1024;\n\nexport async function inspect(\n  inputFile: string,\n  options: InspectOptions\n): Promise<void> {\n  // Validate input file exists\n  if (!fs.existsSync(inputFile)) {\n    throw new Error(`Input file not found: ${inputFile}`);\n  }\n\n  const fileSize = getFileSize(inputFile);\n  console.log(`\\nFile: ${inputFile}`);\n  console.log(`Size: ${formatBytes(fileSize)}`);\n\n  let totalRecords: number;\n  let sampleRecords: unknown[];\n  let format: string;\n  let isEstimated = false;\n\n  // Use streaming for large files\n  if (fileSize > STREAMING_THRESHOLD) {\n    console.log(\"\\nUsing streaming mode for large file...\");\n\n    const sampleSize = options.sample || 1000;\n\n    // Fast mode: only sample, don't read entire file\n    if (options.fast) {\n      console.log(\"Fast mode: sampling records (count will be estimated)...\");\n\n      const result = await streamFile(\n        inputFile,\n        options.format,\n        () => {}, // No-op\n        sampleSize,\n        { sampleOnly: true, sampleSize }\n      );\n\n      totalRecords = result.estimatedCount || result.count;\n      sampleRecords = result.sample;\n      format = result.format;\n      isEstimated = true;\n\n      console.log(`Format: ${format}`);\n      console.log(`Estimated records: ~${totalRecords.toLocaleString()}`);\n    } else {\n      console.log(\"Processing all records (use --fast to estimate count instead)...\");\n\n      const result = await streamFile(\n        inputFile,\n        options.format,\n        () => {}, // No-op, we just want the count and sample\n        sampleSize,\n        { showProgress: true }\n      );\n\n      totalRecords = result.count;\n      sampleRecords = result.sample;\n      format = result.format;\n\n      console.log(`Format: ${format}`);\n      console.log(`Total records: ${totalRecords.toLocaleString()}`);\n    }\n  } else {\n    console.log(\"\\nParsing file...\");\n\n    // Parse the file\n    const result = await parseFile(inputFile, options.format);\n    totalRecords = result.records.length;\n    format = result.format;\n\n    console.log(`Format: ${format}`);\n    console.log(`Total records: ${totalRecords.toLocaleString()}`);\n\n    if (totalRecords === 0) {\n      console.log(\"\\nNo records found.\");\n      return;\n    }\n\n    // Sample records for schema inference if needed\n    const sampleSize = Math.min(options.sample || 1000, totalRecords);\n    sampleRecords = result.records.slice(0, sampleSize);\n  }\n\n  if (sampleRecords.length === 0) {\n    console.log(\"\\nNo records found.\");\n    return;\n  }\n\n  const sampleSize = sampleRecords.length;\n  console.log(`\\nAnalyzing ${sampleSize.toLocaleString()} records...`);\n\n  // Infer schema\n  const schema = inferSchema(sampleRecords);\n\n  console.log(\"\\n\" + \"=\".repeat(60));\n  console.log(\"SCHEMA\");\n  console.log(\"=\".repeat(60));\n\n  console.log(`\\nFields (${schema.fields.length}):\\n`);\n\n  for (const field of schema.fields) {\n    const isPrimary = field.name === schema.primaryField ? \" [PRIMARY]\" : \"\";\n    const isIndexed = field.indexed ? \" [INDEXED]\" : \"\";\n    const nullable = field.nullable ? \" (nullable)\" : \"\";\n\n    console.log(`  ${field.name}: ${field.type}${nullable}${isPrimary}${isIndexed}`);\n\n    // Show stats\n    const stats = field.stats;\n    const statParts = [];\n\n    if (stats.cardinality !== undefined) {\n      const cardinalityPct = ((stats.cardinality / totalRecords) * 100).toFixed(1);\n      statParts.push(`cardinality: ${stats.cardinality} (${cardinalityPct}%)`);\n    }\n\n    if (stats.min !== undefined && stats.max !== undefined) {\n      statParts.push(`range: ${formatValue(stats.min)} - ${formatValue(stats.max)}`);\n    }\n\n    if (stats.nullCount > 0) {\n      statParts.push(`nulls: ${stats.nullCount}`);\n    }\n\n    if (statParts.length > 0) {\n      console.log(`    ${statParts.join(\", \")}`);\n    }\n\n    if (stats.sampleValues && stats.sampleValues.length > 0) {\n      const samples = stats.sampleValues.slice(0, 3).map(formatValue).join(\", \");\n      console.log(`    examples: ${samples}`);\n    }\n  }\n\n  console.log(\"\\n\" + \"=\".repeat(60));\n  console.log(\"RECOMMENDATIONS\");\n  console.log(\"=\".repeat(60));\n\n  // Suggest chunk field\n  const suggestedChunkField = suggestChunkField(schema, sampleRecords);\n  if (suggestedChunkField) {\n    console.log(`\\nRecommended --chunk-by: ${suggestedChunkField}`);\n  } else {\n    console.log(\"\\nNo specific chunk field recommended (will chunk by record order)\");\n  }\n\n  // Suggest indexed fields\n  const indexableFields = getIndexableFields(schema);\n  if (indexableFields.length > 0) {\n    console.log(`Recommended --index: ${indexableFields.join(\",\")}`);\n  } else {\n    console.log(\"No fields recommended for indexing\");\n  }\n\n  // Estimate chunks\n  const avgRecordSize = fileSize / totalRecords;\n  const targetChunkSize = 5 * 1024 * 1024; // 5MB\n  const estimatedChunks = Math.ceil(fileSize / targetChunkSize);\n\n  console.log(\"\\n\" + \"=\".repeat(60));\n  console.log(\"SIZE ESTIMATES\");\n  console.log(\"=\".repeat(60));\n\n  console.log(`\\nAverage record size: ${formatBytes(avgRecordSize)}`);\n  console.log(`\\nWith default 5MB chunks:`);\n  console.log(`  Estimated chunks: ${estimatedChunks}`);\n  console.log(`  Records per chunk: ~${Math.ceil(totalRecords / estimatedChunks).toLocaleString()}`);\n\n  // Show example command\n  console.log(\"\\n\" + \"=\".repeat(60));\n  console.log(\"EXAMPLE COMMAND\");\n  console.log(\"=\".repeat(60));\n\n  let cmd = `npx static-shard build ${inputFile} --output ./dist`;\n  if (suggestedChunkField) {\n    cmd += ` --chunk-by ${suggestedChunkField}`;\n  }\n  if (indexableFields.length > 0) {\n    cmd += ` --index \"${indexableFields.join(\",\")}\"`;\n  }\n\n  console.log(`\\n${cmd}\\n`);\n}\n\nfunction formatBytes(bytes: number): string {\n  if (bytes < 1024) return `${bytes} B`;\n  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)} KB`;\n  if (bytes < 1024 * 1024 * 1024) return `${(bytes / (1024 * 1024)).toFixed(1)} MB`;\n  return `${(bytes / (1024 * 1024 * 1024)).toFixed(1)} GB`;\n}\n\nfunction formatValue(value: unknown): string {\n  if (value === null) return \"null\";\n  if (value === undefined) return \"undefined\";\n  if (typeof value === \"string\") {\n    if (value.length > 30) {\n      return `\"${value.slice(0, 27)}...\"`;\n    }\n    return `\"${value}\"`;\n  }\n  return String(value);\n}\n","/**\n * Types command - quickly generate TypeScript types from a data file\n * Uses json-ts for accurate type inference from JSON samples\n */\n\nimport * as fs from \"node:fs\";\nimport { json2ts } from \"json-ts\";\nimport type { DataRecord } from \"../../types/index.js\";\nimport { parseFile, streamFile, getFileSize } from \"../utils/parsers.js\";\n\nconst STREAMING_THRESHOLD = 100 * 1024 * 1024;\n\nexport interface TypesOptions {\n  sample?: number;\n  format?: string;\n  output?: string;\n}\n\n/**\n * Post-process json-ts output to clean up interface names\n */\nfunction cleanupTypes(types: string): string {\n  return types\n    // Remove the array type wrapper (we want the item type)\n    .replace(/^type IItem = IItemItem\\[\\];\\n/m, \"\")\n    // Rename IItemItem to Item\n    .replace(/IItemItem/g, \"Item\")\n    // Export the interface\n    .replace(/^interface Item/m, \"export interface Item\")\n    // Remove I prefix from nested interfaces\n    .replace(/interface I([A-Z])/g, \"export interface $1\");\n}\n\n/**\n * Generate field names union type from samples\n */\nfunction generateFieldNamesType(samples: DataRecord[]): string {\n  const fieldNames = new Set<string>();\n  for (const record of samples) {\n    for (const key of Object.keys(record)) {\n      fieldNames.add(key);\n    }\n  }\n  const names = Array.from(fieldNames).sort().map((f) => `\"${f}\"`).join(\" | \");\n  return `export type FieldName = ${names};`;\n}\n\nexport async function types(\n  inputFile: string,\n  options: TypesOptions\n): Promise<void> {\n  // Validate input file exists\n  if (!fs.existsSync(inputFile)) {\n    throw new Error(`Input file not found: ${inputFile}`);\n  }\n\n  const fileSize = getFileSize(inputFile);\n  const sampleSize = options.sample || 1000;\n\n  console.error(`Analyzing: ${inputFile}`);\n  console.error(`Sampling ${sampleSize} records...`);\n\n  let samples: DataRecord[];\n  let format: string;\n\n  if (fileSize > STREAMING_THRESHOLD) {\n    // Use streaming for large files\n    const result = await streamFile(\n      inputFile,\n      options.format,\n      () => {},\n      1000,\n      { sampleOnly: true, sampleSize }\n    );\n    samples = result.sample;\n    format = result.format;\n  } else {\n    // Small file - read directly\n    const parseResult = await parseFile(inputFile, options.format);\n    samples = parseResult.records.slice(0, sampleSize);\n    format = parseResult.format;\n  }\n\n  console.error(`Format: ${format}`);\n  console.error(`Sampled ${samples.length} records\\n`);\n\n  // Generate types using json-ts\n  const rawTypes = json2ts(JSON.stringify(samples), { rootName: \"Item\" });\n  const types = cleanupTypes(rawTypes);\n  const fieldNames = generateFieldNamesType(samples);\n\n  // Generate output\n  const output = `/**\n * Auto-generated TypeScript types\n * Source: ${inputFile}\n * Generated: ${new Date().toISOString()}\n */\n\n${types}\n\n${fieldNames}\n`;\n\n  if (options.output) {\n    await fs.promises.writeFile(options.output, output);\n    console.error(`Types written to: ${options.output}`);\n  } else {\n    // Output to stdout\n    console.log(output);\n  }\n}\n"],"mappings":";;;AAMA,SAAS,eAAe;;;ACDxB,YAAYA,SAAQ;AACpB,YAAY,UAAU;;;ACDtB,YAAY,QAAQ;AACpB,YAAY,cAAc;AAC1B,OAAO,UAAU;AACjB,OAAO,gBAAgB;AACvB,OAAO,uBAAuB;AAC9B,OAAO,iBAAiB;AACxB,OAAO,iBAAiB;AAGxB,IAAM,EAAE,QAAQ,WAAW,IAAI;AAC/B,IAAM,EAAE,MAAM,IAAI;AAClB,IAAM,EAAE,YAAY,IAAI;AAGxB,IAAM,sBAAsB,MAAM,OAAO;AAclC,SAAS,aAAa,UAA8B;AACzD,QAAM,MAAM,SAAS,YAAY,EAAE,MAAM,GAAG,EAAE,IAAI;AAElD,MAAI,QAAQ,MAAO,QAAO;AAC1B,MAAI,QAAQ,YAAY,QAAQ,QAAS,QAAO;AAChD,MAAI,QAAQ,QAAQ;AAElB,UAAM,KAAQ,YAAS,UAAU,GAAG;AACpC,UAAM,SAAS,OAAO,MAAM,IAAI;AAChC,IAAG,YAAS,IAAI,QAAQ,GAAG,MAAM,CAAC;AAClC,IAAG,aAAU,EAAE;AAEf,UAAM,UAAU,OAAO,SAAS,OAAO,EAAE,KAAK;AAC9C,QAAI,QAAQ,WAAW,GAAG,EAAG,QAAO;AACpC,QAAI,QAAQ,WAAW,GAAG,EAAG,QAAO;AAAA,EACtC;AAEA,SAAO;AACT;AAKA,eAAsB,eACpB,UACA,UACuB;AACvB,QAAM,QAAW,YAAS,QAAQ;AAGlC,MAAI,MAAM,OAAO,qBAAqB;AACpC,WAAO,wBAAwB,UAAU,QAAQ;AAAA,EACnD;AAGA,QAAM,UAAU,MAAS,YAAS,SAAS,UAAU,OAAO;AAC5D,QAAM,OAAO,KAAK,MAAM,OAAO;AAE/B,MAAI,CAAC,MAAM,QAAQ,IAAI,GAAG;AACxB,UAAM,IAAI,MAAM,4CAA4C;AAAA,EAC9D;AAEA,MAAI,UAAU;AACZ,SAAK,QAAQ,CAAC,QAAQ,UAAU,SAAS,QAAsB,KAAK,CAAC;AAAA,EACvE;AAEA,SAAO;AACT;AAMA,eAAsB,wBACpB,UACA,UACuB;AACvB,SAAO,IAAI,QAAQ,CAACC,UAAS,WAAW;AACtC,UAAM,UAAwB,CAAC;AAC/B,QAAI,QAAQ;AACZ,QAAI,cAAc,KAAK,IAAI;AAE3B,UAAM,WAAW,MAAM;AAAA,MAClB,oBAAiB,QAAQ;AAAA,MAC5B,WAAW;AAAA,MACX,YAAY;AAAA,IACd,CAAC;AAED,aAAS,GAAG,QAAQ,CAAC,SAA6C;AAChE,YAAM,SAAS,KAAK;AACpB,cAAQ,KAAK,MAAM;AACnB,iBAAW,QAAQ,KAAK;AACxB;AAGA,UAAI,KAAK,IAAI,IAAI,cAAc,KAAM;AACnC,gBAAQ,IAAI,YAAY,MAAM,eAAe,CAAC,aAAa;AAC3D,sBAAc,KAAK,IAAI;AAAA,MACzB;AAAA,IACF,CAAC;AAED,aAAS,GAAG,OAAO,MAAM;AACvB,MAAAA,SAAQ,OAAO;AAAA,IACjB,CAAC;AAED,aAAS,GAAG,SAAS,CAAC,QAAe;AACnC,aAAO,IAAI,MAAM,qBAAqB,IAAI,OAAO,EAAE,CAAC;AAAA,IACtD,CAAC;AAAA,EACH,CAAC;AACH;AAMA,eAAsB,YACpB,UACA,UACuB;AACvB,QAAM,UAAwB,CAAC;AAC/B,MAAI,QAAQ;AAEZ,QAAM,aAAgB,oBAAiB,UAAU,EAAE,UAAU,QAAQ,CAAC;AACtE,QAAM,KAAc,yBAAgB;AAAA,IAClC,OAAO;AAAA,IACP,WAAW;AAAA,EACb,CAAC;AAED,mBAAiB,QAAQ,IAAI;AAC3B,UAAM,UAAU,KAAK,KAAK;AAC1B,QAAI,CAAC,QAAS;AAEd,QAAI;AACF,YAAM,SAAS,KAAK,MAAM,OAAO;AACjC,cAAQ,KAAK,MAAM;AACnB,iBAAW,QAAQ,KAAK;AACxB;AAAA,IACF,SAAS,GAAG;AACV,YAAM,IAAI,MAAM,wBAAwB,QAAQ,CAAC,KAAK,QAAQ,MAAM,GAAG,EAAE,CAAC,KAAK;AAAA,IACjF;AAAA,EACF;AAEA,SAAO;AACT;AAMA,eAAsB,SACpB,UACA,UACuB;AACvB,SAAO,IAAI,QAAQ,CAACA,UAAS,WAAW;AACtC,UAAM,UAAwB,CAAC;AAC/B,QAAI,QAAQ;AAEZ,UAAM,aAAgB,oBAAiB,UAAU,EAAE,UAAU,QAAQ,CAAC;AAEtE,SAAK,MAAM,YAAY;AAAA,MACrB,QAAQ;AAAA,MACR,eAAe;AAAA,MACf,gBAAgB;AAAA,MAChB,MAAM,CAAC,WAAW;AAChB,YAAI,OAAO,OAAO,SAAS,GAAG;AAC5B,iBAAO,IAAI,MAAM,oBAAoB,OAAO,OAAO,CAAC,EAAE,OAAO,EAAE,CAAC;AAChE;AAAA,QACF;AACA,cAAM,SAAS,OAAO;AACtB,gBAAQ,KAAK,MAAM;AACnB,mBAAW,QAAQ,KAAK;AACxB;AAAA,MACF;AAAA,MACA,UAAU,MAAMA,SAAQ,OAAO;AAAA,MAC/B,OAAO,CAAC,UAAU,OAAO,KAAK;AAAA,IAChC,CAAC;AAAA,EACH,CAAC;AACH;AAKA,eAAsB,UACpB,UACA,QACA,UACwD;AACxD,QAAM,iBAAiB,UAAU,aAAa,QAAQ;AAEtD,MAAI;AAEJ,UAAQ,gBAAgB;AAAA,IACtB,KAAK;AACH,gBAAU,MAAM,eAAe,UAAU,QAAQ;AACjD;AAAA,IACF,KAAK;AACH,gBAAU,MAAM,YAAY,UAAU,QAAQ;AAC9C;AAAA,IACF,KAAK;AACH,gBAAU,MAAM,SAAS,UAAU,QAAQ;AAC3C;AAAA,IACF;AACE,YAAM,IAAI,MAAM,uBAAuB,cAAc,EAAE;AAAA,EAC3D;AAEA,SAAO,EAAE,SAAS,QAAQ,eAAe;AAC3C;AAMA,eAAsB,WACpB,UACA,QACA,UACA,aAAqB,KACrB,UAAyB,CAAC,GACqE;AAC/F,QAAM,iBAAiB,UAAU,aAAa,QAAQ;AACtD,QAAM,SAAuB,CAAC;AAC9B,MAAI,QAAQ;AACZ,MAAI,iBAAiB;AACrB,QAAM,WAAW,YAAY,QAAQ;AAGrC,MAAI,cAA4C;AAChD,MAAI,QAAQ,gBAAgB,CAAC,QAAQ,YAAY;AAC/C,kBAAc,IAAI,YAAY,UAAU;AAAA,MACtC,QAAQ;AAAA,MACR,iBAAiB;AAAA,MACjB,mBAAmB;AAAA,MACnB,YAAY;AAAA,IACd,GAAG,YAAY,QAAQ,cAAc;AACrC,gBAAY,MAAM,KAAK,MAAM,YAAY,OAAO,KAAK,GAAG,GAAG,EAAE,SAAS,EAAE,CAAC;AAAA,EAC3E;AAEA,QAAM,YAAY,CAAC,QAAoB,OAAe,UAAmB;AACvE,QAAI,OAAO,SAAS,YAAY;AAC9B,aAAO,KAAK,MAAM;AAAA,IACpB;AACA;AACA,QAAI,MAAO,kBAAiB;AAG5B,QAAI,eAAe,QAAQ,QAAS,GAAG;AACrC,kBAAY,OAAO,KAAK,MAAM,kBAAkB,OAAO,KAAK,GAAG,EAAE,SAAS,MAAM,eAAe,EAAE,CAAC;AAAA,IACpG;AAEA,aAAS,QAAQ,KAAK;AAAA,EACxB;AAGA,MAAI,QAAQ,YAAY;AACtB,YAAQ,gBAAgB;AAAA,MACtB,KAAK;AACH,cAAM,sBAAsB,UAAU,WAAW,UAAU;AAC3D;AAAA,MACF,KAAK;AACH,cAAM,mBAAmB,UAAU,WAAW,UAAU;AACxD;AAAA,MACF,KAAK;AACH,cAAM,gBAAgB,UAAU,WAAW,UAAU;AACrD;AAAA,MACF;AACE,cAAM,IAAI,MAAM,uBAAuB,cAAc,EAAE;AAAA,IAC3D;AAGA,UAAM,gBAAgB,iBAAiB;AACvC,UAAM,iBAAiB,KAAK,MAAM,WAAW,aAAa;AAE1D,WAAO,EAAE,OAAO,QAAQ,QAAQ,gBAAgB,eAAe;AAAA,EACjE;AAEA,UAAQ,gBAAgB;AAAA,IACtB,KAAK;AACH,YAAM,gBAAgB,UAAU,WAAW,QAAQ,YAAY;AAC/D;AAAA,IACF,KAAK;AACH,YAAM,aAAa,UAAU,WAAW,QAAQ,YAAY;AAC5D;AAAA,IACF,KAAK;AACH,YAAM,UAAU,UAAU,WAAW,QAAQ,YAAY;AACzD;AAAA,IACF;AACE,YAAM,IAAI,MAAM,uBAAuB,cAAc,EAAE;AAAA,EAC3D;AAEA,MAAI,aAAa;AACf,gBAAY,OAAO,KAAK,MAAM,YAAY,OAAO,KAAK,GAAG,EAAE,SAAS,MAAM,eAAe,EAAE,CAAC;AAC5F,gBAAY,KAAK;AAAA,EACnB;AAEA,SAAO,EAAE,OAAO,QAAQ,QAAQ,eAAe;AACjD;AAKA,eAAe,gBACb,UACA,UACA,cACe;AACf,SAAO,IAAI,QAAQ,CAACA,UAAS,WAAW;AACtC,QAAI,QAAQ;AACZ,QAAI,YAAY;AAEhB,UAAM,aAAgB,oBAAiB,QAAQ;AAC/C,eAAW,GAAG,QAAQ,CAAC,UAAkB;AACvC,mBAAa,MAAM;AAAA,IACrB,CAAC;AAED,UAAM,WAAW,MAAM;AAAA,MACrB;AAAA,MACA,WAAW;AAAA,MACX,YAAY;AAAA,IACd,CAAC;AAED,aAAS,GAAG,QAAQ,CAAC,SAA6C;AAChE,eAAS,KAAK,OAAO,OAAO,SAAS;AACrC;AAAA,IACF,CAAC;AAED,aAAS,GAAG,OAAO,MAAMA,SAAQ,CAAC;AAClC,aAAS,GAAG,SAAS,CAAC,QAAe;AACnC,aAAO,IAAI,MAAM,qBAAqB,IAAI,OAAO,EAAE,CAAC;AAAA,IACtD,CAAC;AAAA,EACH,CAAC;AACH;AAKA,eAAe,sBACb,UACA,UACA,YACe;AACf,SAAO,IAAI,QAAQ,CAACA,UAAS,WAAW;AACtC,QAAI,QAAQ;AACZ,QAAI,YAAY;AAEhB,UAAM,aAAgB,oBAAiB,QAAQ;AAC/C,eAAW,GAAG,QAAQ,CAAC,UAAkB;AACvC,mBAAa,MAAM;AAAA,IACrB,CAAC;AAED,UAAM,WAAW,MAAM;AAAA,MACrB;AAAA,MACA,WAAW;AAAA,MACX,YAAY;AAAA,IACd,CAAC;AAED,aAAS,GAAG,QAAQ,CAAC,SAA6C;AAChE,eAAS,KAAK,OAAO,OAAO,SAAS;AACrC;AAEA,UAAI,SAAS,YAAY;AACvB,mBAAW,QAAQ;AACnB,QAAAA,SAAQ;AAAA,MACV;AAAA,IACF,CAAC;AAED,aAAS,GAAG,OAAO,MAAMA,SAAQ,CAAC;AAClC,aAAS,GAAG,SAAS,CAAC,QAAe;AAEnC,UAAI,IAAI,QAAQ,SAAS,SAAS,KAAK,IAAI,QAAQ,SAAS,WAAW,GAAG;AACxE,QAAAA,SAAQ;AAAA,MACV,OAAO;AACL,eAAO,IAAI,MAAM,qBAAqB,IAAI,OAAO,EAAE,CAAC;AAAA,MACtD;AAAA,IACF,CAAC;AAAA,EACH,CAAC;AACH;AAKA,eAAe,aACb,UACA,UACA,cACe;AACf,MAAI,QAAQ;AACZ,MAAI,YAAY;AAEhB,QAAM,aAAgB,oBAAiB,UAAU,EAAE,UAAU,QAAQ,CAAC;AACtE,aAAW,GAAG,QAAQ,CAAC,UAAkB;AACvC,iBAAa,OAAO,WAAW,KAAK;AAAA,EACtC,CAAC;AAED,QAAM,KAAc,yBAAgB;AAAA,IAClC,OAAO;AAAA,IACP,WAAW;AAAA,EACb,CAAC;AAED,mBAAiB,QAAQ,IAAI;AAC3B,UAAM,UAAU,KAAK,KAAK;AAC1B,QAAI,CAAC,QAAS;AAEd,QAAI;AACF,YAAM,SAAS,KAAK,MAAM,OAAO;AACjC,eAAS,QAAQ,OAAO,SAAS;AACjC;AAAA,IACF,SAAS,GAAG;AACV,YAAM,IAAI,MAAM,wBAAwB,QAAQ,CAAC,KAAK,QAAQ,MAAM,GAAG,EAAE,CAAC,KAAK;AAAA,IACjF;AAAA,EACF;AACF;AAKA,eAAe,mBACb,UACA,UACA,YACe;AACf,MAAI,QAAQ;AACZ,MAAI,YAAY;AAEhB,QAAM,aAAgB,oBAAiB,UAAU,EAAE,UAAU,QAAQ,CAAC;AACtE,aAAW,GAAG,QAAQ,CAAC,UAAkB;AACvC,iBAAa,OAAO,WAAW,KAAK;AAAA,EACtC,CAAC;AAED,QAAM,KAAc,yBAAgB;AAAA,IAClC,OAAO;AAAA,IACP,WAAW;AAAA,EACb,CAAC;AAED,mBAAiB,QAAQ,IAAI;AAC3B,UAAM,UAAU,KAAK,KAAK;AAC1B,QAAI,CAAC,QAAS;AAEd,QAAI;AACF,YAAM,SAAS,KAAK,MAAM,OAAO;AACjC,eAAS,QAAQ,OAAO,SAAS;AACjC;AAEA,UAAI,SAAS,YAAY;AACvB,WAAG,MAAM;AACT,mBAAW,QAAQ;AACnB;AAAA,MACF;AAAA,IACF,SAAS,GAAG;AACV,YAAM,IAAI,MAAM,wBAAwB,QAAQ,CAAC,KAAK,QAAQ,MAAM,GAAG,EAAE,CAAC,KAAK;AAAA,IACjF;AAAA,EACF;AACF;AAKA,eAAe,UACb,UACA,UACA,cACe;AACf,SAAO,IAAI,QAAQ,CAACA,UAAS,WAAW;AACtC,QAAI,QAAQ;AACZ,QAAI,YAAY;AAEhB,UAAM,aAAgB,oBAAiB,UAAU,EAAE,UAAU,QAAQ,CAAC;AACtE,eAAW,GAAG,QAAQ,CAAC,UAAkB;AACvC,mBAAa,OAAO,WAAW,KAAK;AAAA,IACtC,CAAC;AAED,SAAK,MAAM,YAAY;AAAA,MACrB,QAAQ;AAAA,MACR,eAAe;AAAA,MACf,gBAAgB;AAAA,MAChB,MAAM,CAAC,WAAW;AAChB,YAAI,OAAO,OAAO,SAAS,GAAG;AAC5B,iBAAO,IAAI,MAAM,oBAAoB,OAAO,OAAO,CAAC,EAAE,OAAO,EAAE,CAAC;AAChE;AAAA,QACF;AACA,iBAAS,OAAO,MAAoB,OAAO,SAAS;AACpD;AAAA,MACF;AAAA,MACA,UAAU,MAAMA,SAAQ;AAAA,MACxB,OAAO,CAAC,UAAU,OAAO,KAAK;AAAA,IAChC,CAAC;AAAA,EACH,CAAC;AACH;AAKA,eAAe,gBACb,UACA,UACA,YACe;AACf,SAAO,IAAI,QAAQ,CAACA,UAAS,WAAW;AACtC,QAAI,QAAQ;AACZ,QAAI,YAAY;AAChB,QAAI,WAAW;AAEf,UAAM,aAAgB,oBAAiB,UAAU,EAAE,UAAU,QAAQ,CAAC;AACtE,eAAW,GAAG,QAAQ,CAAC,UAAkB;AACvC,mBAAa,OAAO,WAAW,KAAK;AAAA,IACtC,CAAC;AAED,SAAK,MAAM,YAAY;AAAA,MACrB,QAAQ;AAAA,MACR,eAAe;AAAA,MACf,gBAAgB;AAAA,MAChB,MAAM,CAAC,QAAQ,WAAW;AACxB,YAAI,SAAU;AAEd,YAAI,OAAO,OAAO,SAAS,GAAG;AAC5B,iBAAO,IAAI,MAAM,oBAAoB,OAAO,OAAO,CAAC,EAAE,OAAO,EAAE,CAAC;AAChE;AAAA,QACF;AACA,iBAAS,OAAO,MAAoB,OAAO,SAAS;AACpD;AAEA,YAAI,SAAS,YAAY;AACvB,qBAAW;AACX,iBAAO,MAAM;AACb,qBAAW,QAAQ;AACnB,UAAAA,SAAQ;AAAA,QACV;AAAA,MACF;AAAA,MACA,UAAU,MAAM;AACd,YAAI,CAAC,SAAU,CAAAA,SAAQ;AAAA,MACzB;AAAA,MACA,OAAO,CAAC,UAAU,OAAO,KAAK;AAAA,IAChC,CAAC;AAAA,EACH,CAAC;AACH;AAKO,SAAS,YAAY,UAA0B;AACpD,QAAM,QAAW,YAAS,QAAQ;AAClC,SAAO,MAAM;AACf;AAKO,SAAS,UAAU,SAAyB;AACjD,QAAM,QAAQ,QAAQ,YAAY,EAAE,MAAM,mCAAmC;AAC7E,MAAI,CAAC,OAAO;AACV,UAAM,IAAI,MAAM,wBAAwB,OAAO,kCAAkC;AAAA,EACnF;AAEA,QAAM,QAAQ,WAAW,MAAM,CAAC,CAAC;AACjC,QAAM,OAAO,MAAM,CAAC,KAAK;AAEzB,QAAM,cAAsC;AAAA,IAC1C,GAAG;AAAA,IACH,IAAI;AAAA,IACJ,IAAI,OAAO;AAAA,IACX,IAAI,OAAO,OAAO;AAAA,EACpB;AAEA,SAAO,KAAK,MAAM,QAAQ,YAAY,IAAI,CAAC;AAC7C;;;ACvjBA,SAAS,UAAU,OAA2B;AAC5C,MAAI,UAAU,QAAQ,UAAU,QAAW;AACzC,WAAO;AAAA,EACT;AAEA,QAAM,OAAO,OAAO;AAEpB,MAAI,SAAS,UAAU;AAErB,QAAI,aAAa,KAAe,GAAG;AACjC,aAAO;AAAA,IACT;AACA,WAAO;AAAA,EACT;AAEA,MAAI,SAAS,UAAU;AACrB,WAAO;AAAA,EACT;AAEA,MAAI,SAAS,WAAW;AACtB,WAAO;AAAA,EACT;AAGA,SAAO;AACT;AAKA,SAAS,aAAa,OAAwB;AAE5C,QAAM,aAAa;AACnB,MAAI,WAAW,KAAK,KAAK,GAAG;AAC1B,UAAM,OAAO,IAAI,KAAK,KAAK;AAC3B,WAAO,CAAC,MAAM,KAAK,QAAQ,CAAC;AAAA,EAC9B;AACA,SAAO;AACT;AAKA,SAAS,WAAW,OAAkB,OAA6B;AACjE,MAAI,UAAU,MAAO,QAAO;AAC5B,MAAI,UAAU,OAAQ,QAAO;AAC7B,MAAI,UAAU,OAAQ,QAAO;AAG7B,SAAO;AACT;AAKA,IAAM,sBAAN,MAA0B;AAAA,EAChB,SAAS,oBAAI,IAAY;AAAA,EACzB;AAAA,EACA;AAAA,EACA,YAAY;AAAA,EACZ,QAAQ;AAAA,EACR,OAAkB;AAAA,EAClB,eAAqD,CAAC;AAAA,EAE9D,IAAI,OAAsB;AACxB,SAAK;AAEL,QAAI,UAAU,QAAQ,UAAU,QAAW;AACzC,WAAK;AACL;AAAA,IACF;AAEA,UAAM,YAAY,UAAU,KAAK;AACjC,SAAK,OAAO,WAAW,KAAK,MAAM,SAAS;AAG3C,QAAI,KAAK,OAAO,OAAO,KAAO;AAC5B,WAAK,OAAO,IAAI,OAAO,KAAK,CAAC;AAAA,IAC/B;AAGA,QAAI,KAAK,aAAa,SAAS,KAAK,CAAC,KAAK,aAAa,SAAS,KAAyC,GAAG;AAC1G,WAAK,aAAa,KAAK,KAAyC;AAAA,IAClE;AAGA,QAAI,OAAO,UAAU,UAAU;AAC7B,UAAI,KAAK,QAAQ,UAAa,QAAS,KAAK,KAAgB;AAC1D,aAAK,MAAM;AAAA,MACb;AACA,UAAI,KAAK,QAAQ,UAAa,QAAS,KAAK,KAAgB;AAC1D,aAAK,MAAM;AAAA,MACb;AAAA,IACF,WAAW,OAAO,UAAU,aAAa,KAAK,SAAS,UAAU,KAAK,SAAS,WAAW;AACxF,UAAI,KAAK,QAAQ,UAAa,QAAQ,KAAK,KAAK;AAC9C,aAAK,MAAM;AAAA,MACb;AACA,UAAI,KAAK,QAAQ,UAAa,QAAQ,KAAK,KAAK;AAC9C,aAAK,MAAM;AAAA,MACb;AAAA,IACF;AAAA,EACF;AAAA,EAEA,WAAuB;AACrB,WAAO;AAAA,MACL,KAAK,KAAK;AAAA,MACV,KAAK,KAAK;AAAA,MACV,aAAa,KAAK,OAAO;AAAA,MACzB,WAAW,KAAK;AAAA,MAChB,cAAc,KAAK;AAAA,IACrB;AAAA,EACF;AAAA,EAEA,UAAqB;AACnB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,aAAsB;AACpB,WAAO,KAAK,YAAY;AAAA,EAC1B;AACF;AAKA,SAAS,YACP,MACA,MACA,aACA,cACA,OACS;AAET,MAAI,eAAe,GAAG;AACpB,WAAO;AAAA,EACT;AAGA,MAAI,cAAc,eAAe,KAAK;AACpC,WAAO;AAAA,EACT;AAGA,MAAI,cAAc,KAAM;AACtB,WAAO;AAAA,EACT;AAGA,QAAM,UAAU,MAAM,gBAAgB,CAAC;AACvC,aAAW,UAAU,SAAS;AAC5B,QAAI,WAAW,QAAQ,WAAW,OAAW;AAC7C,UAAM,MAAM,OAAO,MAAM;AAGzB,QAAI,IAAI,WAAW,UAAU,KAAK,QAAQ,mBAAmB;AAC3D,aAAO;AAAA,IACT;AAGA,QAAI,IAAI,SAAS,KAAK,GAAG;AACvB,aAAO;AAAA,IACT;AAGA,QAAI,IAAI,SAAS,KAAK;AACpB,aAAO;AAAA,IACT;AAAA,EACF;AAGA,QAAM,YAAY,KAAK,YAAY;AACnC,QAAM,eAAe;AAAA,IACnB;AAAA,IAAQ;AAAA,IAAQ;AAAA;AAAA,IAChB;AAAA,IAAO;AAAA,IAAO;AAAA,IAAQ;AAAA;AAAA,IACtB;AAAA,IAAQ;AAAA,IAAS;AAAA,IAAU;AAAA;AAAA,IAC3B;AAAA,IAAe;AAAA,IAAQ;AAAA,IAAW;AAAA;AAAA,EACpC;AAGA,QAAM,gBAAgB;AAAA,IACpB;AAAA,IAAY;AAAA,IAAQ;AAAA,IAAU;AAAA,IAAS;AAAA,IACvC;AAAA,IAAS;AAAA,IAAQ;AAAA,IAAU;AAAA,IAAO;AAAA,EACpC;AAEA,QAAM,iBAAiB,aAAa,KAAK,OAAK,UAAU,SAAS,CAAC,CAAC;AACnE,QAAM,kBAAkB,cAAc,KAAK,OAAK,UAAU,SAAS,CAAC,CAAC;AAErE,MAAI,kBAAkB,CAAC,iBAAiB;AACtC,WAAO;AAAA,EACT;AAGA,MAAI,SAAS,WAAW;AACtB,WAAO;AAAA,EACT;AAGA,SAAO,eAAe,KAAK,eAAe;AAC5C;AAKO,SAAS,YAAY,SAA+B;AACzD,MAAI,QAAQ,WAAW,GAAG;AACxB,WAAO,EAAE,QAAQ,CAAC,GAAG,cAAc,KAAK;AAAA,EAC1C;AAGA,QAAM,aAAa,oBAAI,IAAY;AACnC,aAAW,UAAU,SAAS;AAC5B,eAAW,OAAO,OAAO,KAAK,MAAM,GAAG;AACrC,iBAAW,IAAI,GAAG;AAAA,IACpB;AAAA,EACF;AAGA,QAAM,aAAa,oBAAI,IAAiC;AACxD,aAAW,QAAQ,YAAY;AAC7B,eAAW,IAAI,MAAM,IAAI,oBAAoB,CAAC;AAAA,EAChD;AAEA,aAAW,UAAU,SAAS;AAC5B,eAAW,QAAQ,YAAY;AAC7B,YAAM,YAAY,WAAW,IAAI,IAAI;AACrC,gBAAU,IAAI,OAAO,IAAI,CAAC;AAAA,IAC5B;AAAA,EACF;AAGA,QAAM,SAAwB,CAAC;AAC/B,MAAI,eAA8B;AAElC,aAAW,QAAQ,YAAY;AAC7B,UAAM,YAAY,WAAW,IAAI,IAAI;AACrC,UAAM,QAAQ,UAAU,SAAS;AACjC,UAAM,OAAO,UAAU,QAAQ;AAG/B,UAAM,cAAc,MAAM;AAC1B,UAAM,UAAU,YAAY,MAAM,MAAM,aAAa,QAAQ,QAAQ,KAAK;AAI1E,QACE,iBAAiB,QACjB,MAAM,gBAAgB,QAAQ,UAC9B,CAAC,UAAU,WAAW,MACrB,KAAK,YAAY,EAAE,SAAS,IAAI,KAAK,KAAK,YAAY,MAAM,QAC7D;AACA,qBAAe;AAAA,IACjB;AAEA,WAAO,KAAK;AAAA,MACV;AAAA,MACA;AAAA,MACA,UAAU,UAAU,WAAW;AAAA,MAC/B;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH;AAGA,SAAO,KAAK,CAAC,GAAG,MAAM;AACpB,QAAI,EAAE,SAAS,aAAc,QAAO;AACpC,QAAI,EAAE,SAAS,aAAc,QAAO;AACpC,WAAO,EAAE,KAAK,cAAc,EAAE,IAAI;AAAA,EACpC,CAAC;AAED,SAAO,EAAE,QAAQ,aAAa;AAChC;AAKO,SAAS,kBAAkB,QAAgB,SAAsC;AAEtF,MAAI,OAAO,cAAc;AACvB,WAAO,OAAO;AAAA,EAChB;AAMA,MAAI,YAA2B;AAC/B,MAAI,YAAY;AAEhB,aAAW,SAAS,OAAO,QAAQ;AACjC,QAAI,MAAM,SAAU;AAEpB,QAAI,QAAQ;AAGZ,QAAI,MAAM,SAAS,YAAY,MAAM,SAAS,QAAQ;AACpD,eAAS;AAAA,IACX;AAGA,UAAM,mBAAmB,MAAM,MAAM,cAAc,QAAQ;AAC3D,QAAI,mBAAmB,OAAO,oBAAoB,GAAG;AACnD,eAAS,mBAAmB;AAAA,IAC9B;AAGA,UAAM,YAAY,MAAM,KAAK,YAAY;AACzC,QAAI,UAAU,SAAS,IAAI,EAAG,UAAS;AACvC,QAAI,UAAU,SAAS,MAAM,KAAK,UAAU,SAAS,MAAM,EAAG,UAAS;AACvE,QAAI,UAAU,SAAS,SAAS,KAAK,UAAU,SAAS,SAAS,EAAG,UAAS;AAE7E,QAAI,QAAQ,WAAW;AACrB,kBAAY;AACZ,kBAAY,MAAM;AAAA,IACpB;AAAA,EACF;AAEA,SAAO;AACT;AAKO,SAAS,mBAAmB,QAA0B;AAC3D,SAAO,OAAO,OACX,OAAO,CAAC,MAAM,EAAE,OAAO,EACvB,IAAI,CAAC,MAAM,EAAE,IAAI;AACtB;;;AC3TA,SAAS,mBAAmB,QAA4B;AACtD,SAAO,KAAK,UAAU,MAAM,EAAE;AAChC;AAKA,SAAS,cAAc,GAAY,GAAoB;AACrD,MAAI,MAAM,EAAG,QAAO;AACpB,MAAI,MAAM,QAAQ,MAAM,OAAW,QAAO;AAC1C,MAAI,MAAM,QAAQ,MAAM,OAAW,QAAO;AAE1C,MAAI,OAAO,MAAM,YAAY,OAAO,MAAM,UAAU;AAClD,WAAO,IAAI;AAAA,EACb;AAEA,SAAO,OAAO,CAAC,EAAE,cAAc,OAAO,CAAC,CAAC;AAC1C;AAKO,SAAS,aACd,SACA,QACA,SACS;AACT,QAAM,EAAE,YAAY,QAAQ,IAAI;AAGhC,MAAI,gBAAgB;AACpB,MAAI,SAAS;AACX,oBAAgB,CAAC,GAAG,OAAO,EAAE;AAAA,MAAK,CAAC,GAAG,MACpC,cAAc,EAAE,OAAO,GAAG,EAAE,OAAO,CAAC;AAAA,IACtC;AAAA,EACF;AAEA,QAAM,SAAkB,CAAC;AACzB,MAAI,eAA6B,CAAC;AAClC,MAAI,cAAc;AAClB,MAAI,aAAa;AAGjB,QAAM,gBAAgB;AACtB,QAAM,gBAAgB;AAEtB,aAAW,UAAU,eAAe;AAClC,UAAM,aAAa,mBAAmB,MAAM,IAAI;AAIhD,QAAI,cAAc,aAAa,cAAc,aAAa,SAAS,GAAG;AACpE,aAAO,KAAK;AAAA,QACV,IAAI,OAAO,UAAU;AAAA,QACrB,SAAS;AAAA,QACT,UAAU,cAAc;AAAA,MAC1B,CAAC;AACD;AACA,qBAAe,CAAC;AAChB,oBAAc;AAAA,IAChB;AAEA,iBAAa,KAAK,MAAM;AACxB,mBAAe;AAAA,EACjB;AAGA,MAAI,aAAa,SAAS,GAAG;AAC3B,WAAO,KAAK;AAAA,MACV,IAAI,OAAO,UAAU;AAAA,MACrB,SAAS;AAAA,MACT,UAAU,cAAc;AAAA,IAC1B,CAAC;AAAA,EACH;AAEA,SAAO;AACT;AAKO,SAAS,qBACd,SACA,QAC0B;AAC1B,QAAM,SAAmC,CAAC;AAE1C,aAAW,SAAS,OAAO,QAAQ;AACjC,UAAM,SAAS,QACZ,IAAI,CAAC,MAAM,EAAE,MAAM,IAAI,CAAC,EACxB,OAAO,CAAC,MAAM,MAAM,QAAQ,MAAM,MAAS;AAE9C,QAAI,OAAO,WAAW,GAAG;AACvB;AAAA,IACF;AAGA,QAAI,MAAM,SAAS,YAAY,MAAM,SAAS,UAAU,MAAM,SAAS,UAAU;AAC/E,YAAM,SAAS,CAAC,GAAG,MAAM,EAAE,KAAK,aAAa;AAC7C,aAAO,MAAM,IAAI,IAAI;AAAA,QACnB,KAAK,OAAO,CAAC;AAAA,QACb,KAAK,OAAO,OAAO,SAAS,CAAC;AAAA,MAC/B;AAAA,IACF;AAAA,EACF;AAEA,SAAO;AACT;AAKO,SAAS,kBACd,OACA,QACA,UACW;AACX,SAAO;AAAA,IACL,IAAI,MAAM;AAAA,IACV,MAAM,GAAG,QAAQ,IAAI,MAAM,EAAE;AAAA,IAC7B,OAAO,MAAM,QAAQ;AAAA,IACrB,UAAU,MAAM;AAAA,IAChB,aAAa,qBAAqB,MAAM,SAAS,MAAM;AAAA,EACzD;AACF;AAKO,SAAS,eAAe,OAAsB;AACnD,SAAO,KAAK,UAAU,MAAM,OAAO;AACrC;;;ACvIO,SAAS,qBACd,QACA,QACA,eACe;AACf,QAAM,UAAyB,CAAC;AAGhC,QAAM,gBAAgB,gBAClB,OAAO,OAAO,OAAO,CAAC,MAAM,cAAc,SAAS,EAAE,IAAI,CAAC,IAC1D,OAAO,OAAO,OAAO,CAAC,MAAM,EAAE,OAAO;AAEzC,aAAW,SAAS,eAAe;AACjC,UAAM,aAAuC,CAAC;AAE9C,eAAW,SAAS,QAAQ;AAC1B,YAAM,gBAAgB,oBAAI,IAAY;AAEtC,iBAAW,UAAU,MAAM,SAAS;AAClC,cAAM,QAAQ,OAAO,MAAM,IAAI;AAC/B,YAAI,UAAU,QAAQ,UAAU,QAAW;AACzC,wBAAc,IAAI,OAAO,KAAK,CAAC;AAAA,QACjC;AAAA,MACF;AAGA,iBAAW,SAAS,eAAe;AACjC,YAAI,CAAC,WAAW,KAAK,GAAG;AACtB,qBAAW,KAAK,IAAI,CAAC;AAAA,QACvB;AACA,mBAAW,KAAK,EAAE,KAAK,MAAM,EAAE;AAAA,MACjC;AAAA,IACF;AAIA,UAAM,eAAe,OAAO,KAAK,UAAU,EAAE;AAC7C,QAAI,gBAAgB,KAAO;AACzB,cAAQ,MAAM,IAAI,IAAI;AAAA,IACxB;AAAA,EACF;AAEA,SAAO;AACT;;;AClDA,SAAS,cAAc,OAA4B;AACjD,MAAI;AAEJ,UAAQ,MAAM,MAAM;AAAA,IAClB,KAAK;AAAA,IACL,KAAK;AACH,eAAS;AACT;AAAA,IACF,KAAK;AACH,eAAS;AACT;AAAA,IACF,KAAK;AACH,eAAS;AACT;AAAA,IACF,KAAK;AACH,eAAS;AACT;AAAA,IACF;AACE,eAAS;AAAA,EACb;AAEA,MAAI,MAAM,UAAU;AAClB,cAAU;AAAA,EACZ;AAEA,SAAO;AACT;AAMA,SAAS,wBAAwB,QAAwB;AACvD,QAAM,SAAS,OAAO,OACnB,IAAI,CAAC,MAAM,KAAK,EAAE,IAAI,KAAK,cAAc,CAAC,CAAC,GAAG,EAC9C,KAAK,IAAI;AAEZ,SAAO;AAAA,EACP,MAAM;AAAA;AAER;AAKA,SAAS,uBAAuB,QAAwB;AACtD,QAAM,QAAQ,OAAO,OAAO,IAAI,CAAC,MAAM,IAAI,EAAE,IAAI,GAAG,EAAE,KAAK,KAAK;AAChE,SAAO,2BAA2B,KAAK;AACzC;AAKA,SAAS,mBAAmB,QAAwB;AAClD,QAAM,eAAe,OAAO,OACzB,OAAO,CAAC,MAAM,EAAE,SAAS,YAAY,EAAE,SAAS,MAAM,EACtD,IAAI,CAAC,MAAM,EAAE,IAAI;AAEpB,QAAM,gBAAgB,OAAO,OAC1B,OAAO,CAAC,MAAM,EAAE,SAAS,QAAQ,EACjC,IAAI,CAAC,MAAM,EAAE,IAAI;AAEpB,SAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAqBP,OAAO,OACN,IAAI,CAAC,MAAM;AACV,QAAI,EAAE,SAAS,UAAU;AACvB,aAAO,KAAK,EAAE,IAAI;AAAA,IACpB,WAAW,EAAE,SAAS,WAAW;AAC/B,aAAO,KAAK,EAAE,IAAI;AAAA,IACpB,OAAO;AACL,aAAO,KAAK,EAAE,IAAI;AAAA,IACpB;AAAA,EACF,CAAC,EACA,KAAK,IAAI,CAAC;AAAA;AAEb;AAKO,SAAS,eAAe,QAAgB,UAA4B;AACzE,QAAM,kBAAkB,wBAAwB,MAAM;AACtD,QAAM,iBAAiB,uBAAuB,MAAM;AACpD,QAAM,aAAa,mBAAmB,MAAM;AAE5C,QAAM,iBAAiB,OAAO,OAC3B,OAAO,CAAC,MAAM,EAAE,SAAS,YAAY,EAAE,SAAS,YAAY,EAAE,SAAS,MAAM,EAC7E,IAAI,CAAC,MAAM,IAAI,EAAE,IAAI,GAAG,EACxB,KAAK,KAAK;AAEb,SAAO;AAAA;AAAA,mBAEU,SAAS,WAAW;AAAA,oBACnB,SAAS,YAAY;AAAA,aAC5B,SAAS,OAAO,MAAM;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOjC,eAAe;AAAA;AAAA,EAEf,cAAc;AAAA;AAAA,EAEd,UAAU;AAAA;AAAA,8BAEkB,kBAAkB,QAAQ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AA8SxD;;;ALxaA,IAAM,UAAU;AAGhB,IAAMC,uBAAsB,MAAM,OAAO;AASzC,eAAsB,MACpB,WACA,SACsB;AACtB,QAAM,YAAY,KAAK,IAAI;AAG3B,MAAI,CAAI,eAAW,SAAS,GAAG;AAC7B,UAAM,IAAI,MAAM,yBAAyB,SAAS,EAAE;AAAA,EACtD;AAEA,QAAM,WAAW,YAAY,SAAS;AACtC,UAAQ,IAAI,UAAU,SAAS,KAAK,YAAY,QAAQ,CAAC,GAAG;AAG5D,MAAI,WAAWA,sBAAqB;AAClC,YAAQ,IAAI,wCAAwC;AACpD,WAAO,eAAe,WAAW,SAAS,SAAS;AAAA,EACrD;AAEA,SAAO,cAAc,WAAW,SAAS,SAAS;AACpD;AAKA,eAAe,cACb,WACA,SACA,WACsB;AACtB,UAAQ,IAAI,iBAAiB;AAG7B,QAAM,EAAE,SAAS,OAAO,IAAI,MAAM,UAAU,WAAW,QAAQ,MAAM;AACrE,UAAQ,IAAI,UAAU,QAAQ,OAAO,eAAe,CAAC,qBAAqB,MAAM,GAAG;AAEnF,MAAI,QAAQ,WAAW,GAAG;AACxB,UAAM,IAAI,MAAM,gCAAgC;AAAA,EAClD;AAGA,UAAQ,IAAI,qBAAqB;AACjC,QAAM,SAAS,YAAY,OAAO;AAClC,UAAQ,IAAI,SAAS,OAAO,OAAO,MAAM,SAAS;AAElD,MAAI,OAAO,cAAc;AACvB,YAAQ,IAAI,2BAA2B,OAAO,YAAY,EAAE;AAAA,EAC9D;AAGA,QAAM,UAAU,QAAQ,WAAW,kBAAkB,QAAQ,OAAO;AACpE,MAAI,SAAS;AACX,YAAQ,IAAI,sBAAsB,OAAO,EAAE;AAAA,EAC7C;AAGA,QAAM,kBAAkB,UAAU,QAAQ,SAAS;AACnD,UAAQ,IAAI,sBAAsB,YAAY,eAAe,CAAC,EAAE;AAGhE,QAAM,gBAAgB,QAAQ,QAC1B,QAAQ,MAAM,MAAM,GAAG,EAAE,IAAI,CAAC,MAAM,EAAE,KAAK,CAAC,IAC5C,OAAO,OAAO,OAAO,CAAC,MAAM,EAAE,OAAO,EAAE,IAAI,CAAC,MAAM,EAAE,IAAI;AAE5D,MAAI,cAAc,SAAS,GAAG;AAC5B,YAAQ,IAAI,oBAAoB,cAAc,KAAK,IAAI,CAAC,EAAE;AAAA,EAC5D;AAGA,aAAW,SAAS,OAAO,QAAQ;AACjC,UAAM,UAAU,cAAc,SAAS,MAAM,IAAI;AAAA,EACnD;AAGA,UAAQ,IAAI,kBAAkB;AAC9B,QAAM,SAAS,aAAa,SAAS,QAAQ;AAAA,IAC3C,YAAY;AAAA,IACZ;AAAA,EACF,CAAC;AACD,UAAQ,IAAI,WAAW,OAAO,MAAM,SAAS;AAG7C,UAAQ,IAAI,qBAAqB;AACjC,QAAM,UAAU,qBAAqB,QAAQ,QAAQ,aAAa;AAClE,UAAQ,IAAI,qBAAqB,OAAO,KAAK,OAAO,EAAE,MAAM,SAAS;AAGrE,QAAM,YAAiB,aAAQ,QAAQ,MAAM;AAC7C,QAAM,YAAiB,UAAK,WAAW,QAAQ;AAC/C,QAAS,aAAS,MAAM,WAAW,EAAE,WAAW,KAAK,CAAC;AAGtD,UAAQ,IAAI,mBAAmB;AAC/B,QAAM,aAAa,CAAC;AACpB,aAAW,SAAS,QAAQ;AAC1B,UAAM,YAAiB,UAAK,WAAW,GAAG,MAAM,EAAE,OAAO;AACzD,UAAS,aAAS,UAAU,WAAW,eAAe,KAAK,CAAC;AAC5D,eAAW,KAAK,kBAAkB,OAAO,QAAQ,QAAQ,CAAC;AAAA,EAC5D;AAGA,QAAM,SAAsB;AAAA,IAC1B,WAAW;AAAA,IACX,SAAS,WAAW;AAAA,IACpB;AAAA,EACF;AAGA,QAAM,WAAqB;AAAA,IACzB,SAAS;AAAA,IACT,cAAa,oBAAI,KAAK,GAAE,YAAY;AAAA,IACpC;AAAA,IACA,QAAQ;AAAA,IACR;AAAA,IACA,cAAc,QAAQ;AAAA,IACtB;AAAA,EACF;AAGA,QAAM,eAAoB,UAAK,WAAW,eAAe;AACzD,QAAS,aAAS,UAAU,cAAc,KAAK,UAAU,UAAU,MAAM,CAAC,CAAC;AAC3E,UAAQ,IAAI,qBAAqB,YAAY,EAAE;AAG/C,UAAQ,IAAI,sBAAsB;AAClC,QAAM,aAAa,eAAe,QAAQ,QAAQ;AAClD,QAAM,aAAkB,UAAK,WAAW,WAAW;AACnD,QAAS,aAAS,UAAU,YAAY,UAAU;AAClD,UAAQ,IAAI,mBAAmB,UAAU,EAAE;AAE3C,QAAM,YAAY,KAAK,IAAI,IAAI,aAAa,KAAM,QAAQ,CAAC;AAC3D,UAAQ,IAAI;AAAA,qBAAwB,OAAO,GAAG;AAC9C,UAAQ,IAAI,WAAW,SAAS,EAAE;AAClC,UAAQ,IAAI,OAAO,OAAO,MAAM,SAAS;AACzC,UAAQ,IAAI,OAAO,QAAQ,OAAO,eAAe,CAAC,gBAAgB;AAClE,UAAQ,IAAI,mBAAmB;AAC/B,UAAQ,IAAI,eAAe;AAE3B,SAAO;AAAA,IACL;AAAA,IACA;AAAA,IACA,YAAY,OAAO;AAAA,IACnB,cAAc,QAAQ;AAAA,EACxB;AACF;AAMA,eAAe,eACb,WACA,SACA,WACsB;AAEtB,QAAM,kBAAkB,UAAU,QAAQ,SAAS;AACnD,UAAQ,IAAI,sBAAsB,YAAY,eAAe,CAAC,EAAE;AAGhE,QAAM,YAAiB,aAAQ,QAAQ,MAAM;AAC7C,QAAM,YAAiB,UAAK,WAAW,QAAQ;AAC/C,QAAS,aAAS,MAAM,WAAW,EAAE,WAAW,KAAK,CAAC;AAGtD,MAAI,eAA6B,CAAC;AAClC,MAAI,mBAAmB;AACvB,MAAI,UAAU;AACd,QAAM,aAA0B,CAAC;AACjC,QAAM,UAAoD,CAAC;AAC3D,MAAI,SAAwB;AAC5B,MAAI,gBAA0B,CAAC;AAC/B,MAAI,UAAyB;AAG7B,QAAM,gBAAgB,CAAC,WAAuB;AAC5C,iBAAa,KAAK,MAAM;AACxB,wBAAoB,KAAK,UAAU,MAAM,EAAE;AAG3C,QAAI,oBAAoB,iBAAiB;AACvC,iBAAW;AAAA,IACb;AAAA,EACF;AAGA,QAAM,aAAa,MAAM;AACvB,QAAI,aAAa,WAAW,EAAG;AAG/B,QAAI,SAAS;AACX,mBAAa,KAAK,CAAC,GAAG,MAAM;AAC1B,cAAM,OAAO,EAAE,OAAO;AACtB,cAAM,OAAO,EAAE,OAAO;AACtB,YAAI,SAAS,KAAM,QAAO;AAC1B,YAAI,SAAS,QAAQ,SAAS,OAAW,QAAO;AAChD,YAAI,SAAS,QAAQ,SAAS,OAAW,QAAO;AAChD,eAAO,OAAO,OAAO,KAAK;AAAA,MAC5B,CAAC;AAAA,IACH;AAEA,UAAM,QAAe;AAAA,MACnB,IAAI,OAAO,OAAO;AAAA,MAClB,SAAS;AAAA,MACT,UAAU;AAAA,IACZ;AAGA,UAAM,YAAiB,UAAK,WAAW,GAAG,OAAO,OAAO;AACxD,IAAG,kBAAc,WAAW,KAAK,UAAU,YAAY,CAAC;AAGxD,UAAM,cAAc,qBAAqB,cAAc,MAAO;AAC9D,eAAW,KAAK;AAAA,MACd,IAAI,OAAO,OAAO;AAAA,MAClB,MAAM,UAAU,OAAO;AAAA,MACvB,OAAO,aAAa;AAAA,MACpB,UAAU;AAAA,MACV;AAAA,IACF,CAAC;AAGD,eAAW,aAAa,eAAe;AACrC,UAAI,CAAC,QAAQ,SAAS,GAAG;AACvB,gBAAQ,SAAS,IAAI,CAAC;AAAA,MACxB;AACA,iBAAW,UAAU,cAAc;AACjC,cAAM,QAAQ,OAAO,SAAS;AAC9B,YAAI,UAAU,QAAQ,UAAU,OAAW;AAC3C,cAAM,MAAM,OAAO,KAAK;AACxB,YAAI,CAAC,QAAQ,SAAS,EAAE,GAAG,GAAG;AAC5B,kBAAQ,SAAS,EAAE,GAAG,IAAI,CAAC;AAAA,QAC7B;AACA,YAAI,CAAC,QAAQ,SAAS,EAAE,GAAG,EAAE,SAAS,OAAO,OAAO,CAAC,GAAG;AACtD,kBAAQ,SAAS,EAAE,GAAG,EAAE,KAAK,OAAO,OAAO,CAAC;AAAA,QAC9C;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,IAAI,iBAAiB,OAAO,KAAK,aAAa,OAAO,eAAe,CAAC,aAAa,YAAY,gBAAgB,CAAC,GAAG;AAG1H;AACA,mBAAe,CAAC;AAChB,uBAAmB;AAAA,EACrB;AAGA,UAAQ,IAAI,0CAA0C;AACtD,QAAM,eAAe,MAAM;AAAA,IACzB;AAAA,IACA,QAAQ;AAAA,IACR,MAAM;AAAA,IAAC;AAAA;AAAA,IACP;AAAA,IACA,EAAE,YAAY,MAAM,YAAY,IAAK;AAAA,EACvC;AAEA,QAAM,SAAS,aAAa;AAC5B,QAAM,SAAS,aAAa;AAE5B,MAAI,OAAO,WAAW,GAAG;AACvB,UAAM,IAAI,MAAM,gCAAgC;AAAA,EAClD;AAGA,UAAQ,IAAI,iCAAiC;AAC7C,WAAS,YAAY,MAAM;AAC3B,UAAQ,IAAI,SAAS,OAAO,OAAO,MAAM,SAAS;AAElD,MAAI,OAAO,cAAc;AACvB,YAAQ,IAAI,2BAA2B,OAAO,YAAY,EAAE;AAAA,EAC9D;AAGA,YAAU,QAAQ,WAAW,kBAAkB,QAAQ,MAAM;AAC7D,MAAI,SAAS;AACX,YAAQ,IAAI,sBAAsB,OAAO,EAAE;AAAA,EAC7C;AAGA,kBAAgB,QAAQ,QACpB,QAAQ,MAAM,MAAM,GAAG,EAAE,IAAI,CAAC,MAAM,EAAE,KAAK,CAAC,IAC5C,OAAO,OAAO,OAAO,CAAC,MAAM,EAAE,OAAO,EAAE,IAAI,CAAC,MAAM,EAAE,IAAI;AAE5D,MAAI,cAAc,SAAS,GAAG;AAC5B,YAAQ,IAAI,oBAAoB,cAAc,KAAK,IAAI,CAAC,EAAE;AAAA,EAC5D;AAGA,aAAW,SAAS,OAAO,QAAQ;AACjC,UAAM,UAAU,cAAc,SAAS,MAAM,IAAI;AAAA,EACnD;AAGA,UAAQ,IAAI,2BAA2B;AACvC,QAAM,EAAE,MAAM,IAAI,MAAM;AAAA,IACtB;AAAA,IACA,QAAQ;AAAA,IACR;AAAA,IACA;AAAA,IACA,EAAE,cAAc,KAAK;AAAA,EACvB;AAGA,aAAW;AAEX,UAAQ,IAAI;AAAA,YAAe,MAAM,eAAe,CAAC,qBAAqB,MAAM,GAAG;AAG/E,QAAM,SAAsB;AAAA,IAC1B,WAAW;AAAA,IACX,SAAS,WAAW;AAAA,IACpB;AAAA,EACF;AAGA,QAAM,WAAqB;AAAA,IACzB,SAAS;AAAA,IACT,cAAa,oBAAI,KAAK,GAAE,YAAY;AAAA,IACpC;AAAA,IACA,QAAQ;AAAA,IACR;AAAA,IACA,cAAc;AAAA,IACd;AAAA,EACF;AAGA,QAAM,eAAoB,UAAK,WAAW,eAAe;AACzD,QAAS,aAAS,UAAU,cAAc,KAAK,UAAU,UAAU,MAAM,CAAC,CAAC;AAC3E,UAAQ,IAAI,qBAAqB,YAAY,EAAE;AAG/C,UAAQ,IAAI,sBAAsB;AAClC,QAAM,aAAa,eAAe,QAAS,QAAQ;AACnD,QAAM,aAAkB,UAAK,WAAW,WAAW;AACnD,QAAS,aAAS,UAAU,YAAY,UAAU;AAClD,UAAQ,IAAI,mBAAmB,UAAU,EAAE;AAE3C,QAAM,YAAY,KAAK,IAAI,IAAI,aAAa,KAAM,QAAQ,CAAC;AAC3D,UAAQ,IAAI;AAAA,qBAAwB,OAAO,GAAG;AAC9C,UAAQ,IAAI,WAAW,SAAS,EAAE;AAClC,UAAQ,IAAI,OAAO,WAAW,MAAM,SAAS;AAC7C,UAAQ,IAAI,OAAO,MAAM,eAAe,CAAC,gBAAgB;AACzD,UAAQ,IAAI,mBAAmB;AAC/B,UAAQ,IAAI,eAAe;AAE3B,SAAO;AAAA,IACL;AAAA,IACA;AAAA,IACA,YAAY,WAAW;AAAA,IACvB,cAAc;AAAA,EAChB;AACF;AAEA,SAAS,YAAY,OAAuB;AAC1C,MAAI,QAAQ,KAAM,QAAO,GAAG,KAAK;AACjC,MAAI,QAAQ,OAAO,KAAM,QAAO,IAAI,QAAQ,MAAM,QAAQ,CAAC,CAAC;AAC5D,MAAI,QAAQ,OAAO,OAAO,KAAM,QAAO,IAAI,SAAS,OAAO,OAAO,QAAQ,CAAC,CAAC;AAC5E,SAAO,IAAI,SAAS,OAAO,OAAO,OAAO,QAAQ,CAAC,CAAC;AACrD;;;AM7XA,YAAYC,SAAQ;AAMpB,IAAMC,uBAAsB,MAAM,OAAO;AAEzC,eAAsB,QACpB,WACA,SACe;AAEf,MAAI,CAAI,eAAW,SAAS,GAAG;AAC7B,UAAM,IAAI,MAAM,yBAAyB,SAAS,EAAE;AAAA,EACtD;AAEA,QAAM,WAAW,YAAY,SAAS;AACtC,UAAQ,IAAI;AAAA,QAAW,SAAS,EAAE;AAClC,UAAQ,IAAI,SAASC,aAAY,QAAQ,CAAC,EAAE;AAE5C,MAAI;AACJ,MAAI;AACJ,MAAI;AACJ,MAAI,cAAc;AAGlB,MAAI,WAAWD,sBAAqB;AAClC,YAAQ,IAAI,0CAA0C;AAEtD,UAAME,cAAa,QAAQ,UAAU;AAGrC,QAAI,QAAQ,MAAM;AAChB,cAAQ,IAAI,0DAA0D;AAEtE,YAAM,SAAS,MAAM;AAAA,QACnB;AAAA,QACA,QAAQ;AAAA,QACR,MAAM;AAAA,QAAC;AAAA;AAAA,QACPA;AAAA,QACA,EAAE,YAAY,MAAM,YAAAA,YAAW;AAAA,MACjC;AAEA,qBAAe,OAAO,kBAAkB,OAAO;AAC/C,sBAAgB,OAAO;AACvB,eAAS,OAAO;AAChB,oBAAc;AAEd,cAAQ,IAAI,WAAW,MAAM,EAAE;AAC/B,cAAQ,IAAI,uBAAuB,aAAa,eAAe,CAAC,EAAE;AAAA,IACpE,OAAO;AACL,cAAQ,IAAI,kEAAkE;AAE9E,YAAM,SAAS,MAAM;AAAA,QACnB;AAAA,QACA,QAAQ;AAAA,QACR,MAAM;AAAA,QAAC;AAAA;AAAA,QACPA;AAAA,QACA,EAAE,cAAc,KAAK;AAAA,MACvB;AAEA,qBAAe,OAAO;AACtB,sBAAgB,OAAO;AACvB,eAAS,OAAO;AAEhB,cAAQ,IAAI,WAAW,MAAM,EAAE;AAC/B,cAAQ,IAAI,kBAAkB,aAAa,eAAe,CAAC,EAAE;AAAA,IAC/D;AAAA,EACF,OAAO;AACL,YAAQ,IAAI,mBAAmB;AAG/B,UAAM,SAAS,MAAM,UAAU,WAAW,QAAQ,MAAM;AACxD,mBAAe,OAAO,QAAQ;AAC9B,aAAS,OAAO;AAEhB,YAAQ,IAAI,WAAW,MAAM,EAAE;AAC/B,YAAQ,IAAI,kBAAkB,aAAa,eAAe,CAAC,EAAE;AAE7D,QAAI,iBAAiB,GAAG;AACtB,cAAQ,IAAI,qBAAqB;AACjC;AAAA,IACF;AAGA,UAAMA,cAAa,KAAK,IAAI,QAAQ,UAAU,KAAM,YAAY;AAChE,oBAAgB,OAAO,QAAQ,MAAM,GAAGA,WAAU;AAAA,EACpD;AAEA,MAAI,cAAc,WAAW,GAAG;AAC9B,YAAQ,IAAI,qBAAqB;AACjC;AAAA,EACF;AAEA,QAAM,aAAa,cAAc;AACjC,UAAQ,IAAI;AAAA,YAAe,WAAW,eAAe,CAAC,aAAa;AAGnE,QAAM,SAAS,YAAY,aAAa;AAExC,UAAQ,IAAI,OAAO,IAAI,OAAO,EAAE,CAAC;AACjC,UAAQ,IAAI,QAAQ;AACpB,UAAQ,IAAI,IAAI,OAAO,EAAE,CAAC;AAE1B,UAAQ,IAAI;AAAA,UAAa,OAAO,OAAO,MAAM;AAAA,CAAM;AAEnD,aAAW,SAAS,OAAO,QAAQ;AACjC,UAAM,YAAY,MAAM,SAAS,OAAO,eAAe,eAAe;AACtE,UAAM,YAAY,MAAM,UAAU,eAAe;AACjD,UAAM,WAAW,MAAM,WAAW,gBAAgB;AAElD,YAAQ,IAAI,KAAK,MAAM,IAAI,KAAK,MAAM,IAAI,GAAG,QAAQ,GAAG,SAAS,GAAG,SAAS,EAAE;AAG/E,UAAM,QAAQ,MAAM;AACpB,UAAM,YAAY,CAAC;AAEnB,QAAI,MAAM,gBAAgB,QAAW;AACnC,YAAM,kBAAmB,MAAM,cAAc,eAAgB,KAAK,QAAQ,CAAC;AAC3E,gBAAU,KAAK,gBAAgB,MAAM,WAAW,KAAK,cAAc,IAAI;AAAA,IACzE;AAEA,QAAI,MAAM,QAAQ,UAAa,MAAM,QAAQ,QAAW;AACtD,gBAAU,KAAK,UAAU,YAAY,MAAM,GAAG,CAAC,MAAM,YAAY,MAAM,GAAG,CAAC,EAAE;AAAA,IAC/E;AAEA,QAAI,MAAM,YAAY,GAAG;AACvB,gBAAU,KAAK,UAAU,MAAM,SAAS,EAAE;AAAA,IAC5C;AAEA,QAAI,UAAU,SAAS,GAAG;AACxB,cAAQ,IAAI,OAAO,UAAU,KAAK,IAAI,CAAC,EAAE;AAAA,IAC3C;AAEA,QAAI,MAAM,gBAAgB,MAAM,aAAa,SAAS,GAAG;AACvD,YAAM,UAAU,MAAM,aAAa,MAAM,GAAG,CAAC,EAAE,IAAI,WAAW,EAAE,KAAK,IAAI;AACzE,cAAQ,IAAI,iBAAiB,OAAO,EAAE;AAAA,IACxC;AAAA,EACF;AAEA,UAAQ,IAAI,OAAO,IAAI,OAAO,EAAE,CAAC;AACjC,UAAQ,IAAI,iBAAiB;AAC7B,UAAQ,IAAI,IAAI,OAAO,EAAE,CAAC;AAG1B,QAAM,sBAAsB,kBAAkB,QAAQ,aAAa;AACnE,MAAI,qBAAqB;AACvB,YAAQ,IAAI;AAAA,0BAA6B,mBAAmB,EAAE;AAAA,EAChE,OAAO;AACL,YAAQ,IAAI,oEAAoE;AAAA,EAClF;AAGA,QAAM,kBAAkB,mBAAmB,MAAM;AACjD,MAAI,gBAAgB,SAAS,GAAG;AAC9B,YAAQ,IAAI,wBAAwB,gBAAgB,KAAK,GAAG,CAAC,EAAE;AAAA,EACjE,OAAO;AACL,YAAQ,IAAI,oCAAoC;AAAA,EAClD;AAGA,QAAM,gBAAgB,WAAW;AACjC,QAAM,kBAAkB,IAAI,OAAO;AACnC,QAAM,kBAAkB,KAAK,KAAK,WAAW,eAAe;AAE5D,UAAQ,IAAI,OAAO,IAAI,OAAO,EAAE,CAAC;AACjC,UAAQ,IAAI,gBAAgB;AAC5B,UAAQ,IAAI,IAAI,OAAO,EAAE,CAAC;AAE1B,UAAQ,IAAI;AAAA,uBAA0BD,aAAY,aAAa,CAAC,EAAE;AAClE,UAAQ,IAAI;AAAA,yBAA4B;AACxC,UAAQ,IAAI,uBAAuB,eAAe,EAAE;AACpD,UAAQ,IAAI,yBAAyB,KAAK,KAAK,eAAe,eAAe,EAAE,eAAe,CAAC,EAAE;AAGjG,UAAQ,IAAI,OAAO,IAAI,OAAO,EAAE,CAAC;AACjC,UAAQ,IAAI,iBAAiB;AAC7B,UAAQ,IAAI,IAAI,OAAO,EAAE,CAAC;AAE1B,MAAI,MAAM,0BAA0B,SAAS;AAC7C,MAAI,qBAAqB;AACvB,WAAO,eAAe,mBAAmB;AAAA,EAC3C;AACA,MAAI,gBAAgB,SAAS,GAAG;AAC9B,WAAO,aAAa,gBAAgB,KAAK,GAAG,CAAC;AAAA,EAC/C;AAEA,UAAQ,IAAI;AAAA,EAAK,GAAG;AAAA,CAAI;AAC1B;AAEA,SAASA,aAAY,OAAuB;AAC1C,MAAI,QAAQ,KAAM,QAAO,GAAG,KAAK;AACjC,MAAI,QAAQ,OAAO,KAAM,QAAO,IAAI,QAAQ,MAAM,QAAQ,CAAC,CAAC;AAC5D,MAAI,QAAQ,OAAO,OAAO,KAAM,QAAO,IAAI,SAAS,OAAO,OAAO,QAAQ,CAAC,CAAC;AAC5E,SAAO,IAAI,SAAS,OAAO,OAAO,OAAO,QAAQ,CAAC,CAAC;AACrD;AAEA,SAAS,YAAY,OAAwB;AAC3C,MAAI,UAAU,KAAM,QAAO;AAC3B,MAAI,UAAU,OAAW,QAAO;AAChC,MAAI,OAAO,UAAU,UAAU;AAC7B,QAAI,MAAM,SAAS,IAAI;AACrB,aAAO,IAAI,MAAM,MAAM,GAAG,EAAE,CAAC;AAAA,IAC/B;AACA,WAAO,IAAI,KAAK;AAAA,EAClB;AACA,SAAO,OAAO,KAAK;AACrB;;;AChNA,YAAYE,SAAQ;AACpB,SAAS,eAAe;AAIxB,IAAMC,uBAAsB,MAAM,OAAO;AAWzC,SAAS,aAAaC,QAAuB;AAC3C,SAAOA,OAEJ,QAAQ,mCAAmC,EAAE,EAE7C,QAAQ,cAAc,MAAM,EAE5B,QAAQ,oBAAoB,uBAAuB,EAEnD,QAAQ,uBAAuB,qBAAqB;AACzD;AAKA,SAASC,wBAAuB,SAA+B;AAC7D,QAAM,aAAa,oBAAI,IAAY;AACnC,aAAW,UAAU,SAAS;AAC5B,eAAW,OAAO,OAAO,KAAK,MAAM,GAAG;AACrC,iBAAW,IAAI,GAAG;AAAA,IACpB;AAAA,EACF;AACA,QAAM,QAAQ,MAAM,KAAK,UAAU,EAAE,KAAK,EAAE,IAAI,CAAC,MAAM,IAAI,CAAC,GAAG,EAAE,KAAK,KAAK;AAC3E,SAAO,2BAA2B,KAAK;AACzC;AAEA,eAAsB,MACpB,WACA,SACe;AAEf,MAAI,CAAI,eAAW,SAAS,GAAG;AAC7B,UAAM,IAAI,MAAM,yBAAyB,SAAS,EAAE;AAAA,EACtD;AAEA,QAAM,WAAW,YAAY,SAAS;AACtC,QAAM,aAAa,QAAQ,UAAU;AAErC,UAAQ,MAAM,cAAc,SAAS,EAAE;AACvC,UAAQ,MAAM,YAAY,UAAU,aAAa;AAEjD,MAAI;AACJ,MAAI;AAEJ,MAAI,WAAWF,sBAAqB;AAElC,UAAM,SAAS,MAAM;AAAA,MACnB;AAAA,MACA,QAAQ;AAAA,MACR,MAAM;AAAA,MAAC;AAAA,MACP;AAAA,MACA,EAAE,YAAY,MAAM,WAAW;AAAA,IACjC;AACA,cAAU,OAAO;AACjB,aAAS,OAAO;AAAA,EAClB,OAAO;AAEL,UAAM,cAAc,MAAM,UAAU,WAAW,QAAQ,MAAM;AAC7D,cAAU,YAAY,QAAQ,MAAM,GAAG,UAAU;AACjD,aAAS,YAAY;AAAA,EACvB;AAEA,UAAQ,MAAM,WAAW,MAAM,EAAE;AACjC,UAAQ,MAAM,WAAW,QAAQ,MAAM;AAAA,CAAY;AAGnD,QAAM,WAAW,QAAQ,KAAK,UAAU,OAAO,GAAG,EAAE,UAAU,OAAO,CAAC;AACtE,QAAMC,SAAQ,aAAa,QAAQ;AACnC,QAAM,aAAaC,wBAAuB,OAAO;AAGjD,QAAM,SAAS;AAAA;AAAA,aAEJ,SAAS;AAAA,iBACN,oBAAI,KAAK,GAAE,YAAY,CAAC;AAAA;AAAA;AAAA,EAGtCD,MAAK;AAAA;AAAA,EAEL,UAAU;AAAA;AAGV,MAAI,QAAQ,QAAQ;AAClB,UAAS,aAAS,UAAU,QAAQ,QAAQ,MAAM;AAClD,YAAQ,MAAM,qBAAqB,QAAQ,MAAM,EAAE;AAAA,EACrD,OAAO;AAEL,YAAQ,IAAI,MAAM;AAAA,EACpB;AACF;;;ARnGA,IAAM,UAAU,IAAI,QAAQ;AAE5B,QACG,KAAK,cAAc,EACnB,YAAY,uEAAuE,EACnF,QAAQ,OAAO;AAElB,QACG,QAAQ,OAAO,EACf,YAAY,0CAA0C,EACtD,SAAS,WAAW,wCAAwC,EAC5D,eAAe,sBAAsB,kBAAkB,EACvD,OAAO,2BAA2B,iCAAiC,KAAK,EACxE,OAAO,0BAA0B,4BAA4B,EAC7D,OAAO,wBAAwB,iCAAiC,EAChE,OAAO,yBAAyB,kCAAkC,EAClE,OAAO,OAAO,OAAO,YAAY;AAChC,MAAI;AACF,UAAM,MAAM,OAAO;AAAA,MACjB,QAAQ,QAAQ;AAAA,MAChB,WAAW,QAAQ;AAAA,MACnB,SAAS,QAAQ;AAAA,MACjB,OAAO,QAAQ;AAAA,MACf,QAAQ,QAAQ;AAAA,IAClB,CAAC;AAAA,EACH,SAAS,OAAO;AACd,YAAQ,MAAM,UAAW,MAAgB,OAAO;AAChD,YAAQ,KAAK,CAAC;AAAA,EAChB;AACF,CAAC;AAEH,QACG,QAAQ,SAAS,EACjB,YAAY,mDAAmD,EAC/D,SAAS,WAAW,iBAAiB,EACrC,OAAO,wBAAwB,+BAA+B,MAAM,EACpE,OAAO,yBAAyB,kCAAkC,EAClE,OAAO,UAAU,iEAAiE,EAClF,OAAO,OAAO,OAAO,YAAY;AAChC,MAAI;AACF,UAAM,QAAQ,OAAO;AAAA,MACnB,QAAQ,SAAS,QAAQ,QAAQ,EAAE;AAAA,MACnC,QAAQ,QAAQ;AAAA,MAChB,MAAM,QAAQ;AAAA,IAChB,CAAC;AAAA,EACH,SAAS,OAAO;AACd,YAAQ,MAAM,UAAW,MAAgB,OAAO;AAChD,YAAQ,KAAK,CAAC;AAAA,EAChB;AACF,CAAC;AAEH,QACG,QAAQ,OAAO,EACf,YAAY,4CAA4C,EACxD,SAAS,WAAW,wCAAwC,EAC5D,OAAO,wBAAwB,+BAA+B,MAAM,EACpE,OAAO,yBAAyB,kCAAkC,EAClE,OAAO,uBAAuB,+BAA+B,EAC7D,OAAO,OAAO,OAAO,YAAY;AAChC,MAAI;AACF,UAAM,MAAM,OAAO;AAAA,MACjB,QAAQ,SAAS,QAAQ,QAAQ,EAAE;AAAA,MACnC,QAAQ,QAAQ;AAAA,MAChB,QAAQ,QAAQ;AAAA,IAClB,CAAC;AAAA,EACH,SAAS,OAAO;AACd,YAAQ,MAAM,UAAW,MAAgB,OAAO;AAChD,YAAQ,KAAK,CAAC;AAAA,EAChB;AACF,CAAC;AAEH,QAAQ,MAAM;","names":["fs","resolve","STREAMING_THRESHOLD","fs","STREAMING_THRESHOLD","formatBytes","sampleSize","fs","STREAMING_THRESHOLD","types","generateFieldNamesType"]}